<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 5 Modelos Avançados de Vetorização | Vetorização de Texto com Python</title>
  <meta name="description" content="Capítulo 5 Modelos Avançados de Vetorização | Vetorização de Texto com Python" />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 5 Modelos Avançados de Vetorização | Vetorização de Texto com Python" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 5 Modelos Avançados de Vetorização | Vetorização de Texto com Python" />
  
  
  

<meta name="author" content="Giseldo Neo" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="vetorização-de-texto.html"/>
<link rel="next" href="aplicações-práticas-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Vetorização de Texto com Python</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Informações Adicionais</a></li>
<li class="chapter" data-level="" data-path="prefácio.html"><a href="prefácio.html"><i class="fa fa-check"></i>Prefácio</a></li>
<li class="chapter" data-level="1" data-path="introdução-à-vetorização-de-texto.html"><a href="introdução-à-vetorização-de-texto.html"><i class="fa fa-check"></i><b>1</b> Introdução à Vetorização de Texto</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introdução-à-vetorização-de-texto.html"><a href="introdução-à-vetorização-de-texto.html#conceito-de-vetorização-de-texto"><i class="fa fa-check"></i><b>1.1</b> Conceito de Vetorização de Texto</a></li>
<li class="chapter" data-level="1.2" data-path="introdução-à-vetorização-de-texto.html"><a href="introdução-à-vetorização-de-texto.html#importância-na-análise-de-dados-e-aprendizado-de-máquina"><i class="fa fa-check"></i><b>1.2</b> Importância na Análise de Dados e Aprendizado de Máquina</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introdução-à-vetorização-de-texto.html"><a href="introdução-à-vetorização-de-texto.html#exemplo-em-python"><i class="fa fa-check"></i><b>1.2.1</b> Exemplo em Python</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introdução-à-vetorização-de-texto.html"><a href="introdução-à-vetorização-de-texto.html#aplicações-práticas"><i class="fa fa-check"></i><b>1.3</b> Aplicações Práticas</a></li>
<li class="chapter" data-level="" data-path="introdução-à-vetorização-de-texto.html"><a href="introdução-à-vetorização-de-texto.html#exercícios"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html"><i class="fa fa-check"></i><b>2</b> Preparação do Ambiente</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#instalação-do-python"><i class="fa fa-check"></i><b>2.1</b> Instalação do Python</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#instalando-o-python"><i class="fa fa-check"></i><b>2.1.1</b> Instalando o Python</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#instalação-de-bibliotecas-necessárias"><i class="fa fa-check"></i><b>2.2</b> Instalação de Bibliotecas Necessárias</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#instalando-bibliotecas-com-pip"><i class="fa fa-check"></i><b>2.2.1</b> Instalando Bibliotecas com <code>pip</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#exemplo-em-python-verificando-instalações"><i class="fa fa-check"></i><b>2.2.2</b> Exemplo em Python: Verificando Instalações</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#introdução-ao-jupyter-notebook"><i class="fa fa-check"></i><b>2.3</b> Introdução ao Jupyter Notebook</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#instalando-o-jupyter-notebook"><i class="fa fa-check"></i><b>2.3.1</b> Instalando o Jupyter Notebook</a></li>
<li class="chapter" data-level="2.3.2" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#exemplo-em-python-primeiros-passos-no-jupyter"><i class="fa fa-check"></i><b>2.3.2</b> Exemplo em Python: Primeiros Passos no Jupyter</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preparação-do-ambiente.html"><a href="preparação-do-ambiente.html#exercícios-1"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html"><i class="fa fa-check"></i><b>3</b> Pré-processamento de Texto</a>
<ul>
<li class="chapter" data-level="3.1" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#limpeza-de-texto"><i class="fa fa-check"></i><b>3.1</b> Limpeza de Texto</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#exemplo-em-python-limpeza-de-texto"><i class="fa fa-check"></i><b>3.1.1</b> Exemplo em Python: Limpeza de Texto</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#tokenização"><i class="fa fa-check"></i><b>3.2</b> Tokenização</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#exemplo-em-python-tokenização"><i class="fa fa-check"></i><b>3.2.1</b> Exemplo em Python: Tokenização</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#lematização-e-stemming"><i class="fa fa-check"></i><b>3.3</b> Lematização e Stemming</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#exemplo-em-python-lematização-e-stemming"><i class="fa fa-check"></i><b>3.3.1</b> Exemplo em Python: Lematização e Stemming</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pré-processamento-de-texto.html"><a href="pré-processamento-de-texto.html#exercícios-2"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html"><i class="fa fa-check"></i><b>4</b> Vetorização de Texto</a>
<ul>
<li class="chapter" data-level="4.1" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#bag-of-words-bow"><i class="fa fa-check"></i><b>4.1</b> Bag of Words (BoW)</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#exemplo-em-python-bag-of-words"><i class="fa fa-check"></i><b>4.1.1</b> Exemplo em Python: Bag of Words</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#term-frequency-inverse-document-frequency-tf-idf"><i class="fa fa-check"></i><b>4.2</b> Term Frequency-Inverse Document Frequency (TF-IDF)</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#exemplo-em-python-tf-idf"><i class="fa fa-check"></i><b>4.2.1</b> Exemplo em Python: TF-IDF</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#word-embeddings"><i class="fa fa-check"></i><b>4.3</b> Word Embeddings</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#exemplo-em-python-word2vec"><i class="fa fa-check"></i><b>4.3.1</b> Exemplo em Python: Word2Vec</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="vetorização-de-texto.html"><a href="vetorização-de-texto.html#exercícios-3"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html"><i class="fa fa-check"></i><b>5</b> Modelos Avançados de Vetorização</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#embeddings-contextuais"><i class="fa fa-check"></i><b>5.1</b> Embeddings Contextuais</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#exemplo-em-python-usando-bert-para-vetorização"><i class="fa fa-check"></i><b>5.1.1</b> Exemplo em Python: Usando BERT para Vetorização</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#análise-de-sentimento-com-embeddings"><i class="fa fa-check"></i><b>5.2</b> Análise de Sentimento com Embeddings</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#exemplo-em-python-classificação-de-sentimento-com-bert"><i class="fa fa-check"></i><b>5.2.1</b> Exemplo em Python: Classificação de Sentimento com BERT</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#redução-de-dimensionalidade"><i class="fa fa-check"></i><b>5.3</b> Redução de Dimensionalidade</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#exemplo-em-python-redução-de-dimensionalidade-com-pca"><i class="fa fa-check"></i><b>5.3.1</b> Exemplo em Python: Redução de Dimensionalidade com PCA</a></li>
<li class="chapter" data-level="5.3.2" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#exemplo-em-python-redução-de-dimensionalidade-com-t-sne"><i class="fa fa-check"></i><b>5.3.2</b> Exemplo em Python: Redução de Dimensionalidade com t-SNE</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modelos-avançados-de-vetorização.html"><a href="modelos-avançados-de-vetorização.html#exercício"><i class="fa fa-check"></i>Exercício</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html"><i class="fa fa-check"></i><b>6</b> Aplicações Práticas</a>
<ul>
<li class="chapter" data-level="6.1" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#classificação-de-texto-análise-de-sentimento"><i class="fa fa-check"></i><b>6.1</b> Classificação de Texto: Análise de Sentimento</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#exemplo-em-python-classificação-de-texto-com-tf-idf-e-naive-bayes"><i class="fa fa-check"></i><b>6.1.1</b> Exemplo em Python: Classificação de Texto com TF-IDF e Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#agrupamento-de-documentos"><i class="fa fa-check"></i><b>6.2</b> Agrupamento de Documentos</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#exemplo-em-python-agrupamento-com-k-means"><i class="fa fa-check"></i><b>6.2.1</b> Exemplo em Python: Agrupamento com k-means</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#detecção-de-tópicos"><i class="fa fa-check"></i><b>6.3</b> Detecção de Tópicos</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#exemplo-em-python-detecção-de-tópicos-com-lda"><i class="fa fa-check"></i><b>6.3.1</b> Exemplo em Python: Detecção de Tópicos com LDA</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="aplicações-práticas-1.html"><a href="aplicações-práticas-1.html#exercícios-4"><i class="fa fa-check"></i>Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html"><i class="fa fa-check"></i><b>7</b> Estudo de Caso</a>
<ul>
<li class="chapter" data-level="7.1" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#análise-de-reviews-de-produtos"><i class="fa fa-check"></i><b>7.1</b> Análise de Reviews de Produtos</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#exemplo-em-python-análise-de-sentimentos-em-reviews-de-produtos"><i class="fa fa-check"></i><b>7.1.1</b> Exemplo em Python: Análise de Sentimentos em Reviews de Produtos</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#processamento-de-tweets"><i class="fa fa-check"></i><b>7.2</b> Processamento de Tweets</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#exemplo-em-python-análise-de-sentimentos-em-tweets"><i class="fa fa-check"></i><b>7.2.1</b> Exemplo em Python: Análise de Sentimentos em Tweets</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#análise-de-notícias"><i class="fa fa-check"></i><b>7.3</b> Análise de Notícias</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#exemplo-em-python-modelagem-de-tópicos-em-notícias"><i class="fa fa-check"></i><b>7.3.1</b> Exemplo em Python: Modelagem de Tópicos em Notícias</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="estudo-de-caso.html"><a href="estudo-de-caso.html#exercício-1"><i class="fa fa-check"></i>Exercício</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="próximos-passos.html"><a href="próximos-passos.html"><i class="fa fa-check"></i><b>8</b> Próximos Passos</a>
<ul>
<li class="chapter" data-level="8.1" data-path="próximos-passos.html"><a href="próximos-passos.html#futuras-direções-no-campo-de-vetorização-de-texto"><i class="fa fa-check"></i><b>8.1</b> Futuras Direções no Campo de Vetorização de Texto</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="próximos-passos.html"><a href="próximos-passos.html#modelos-de-linguagem-de-grande-escala"><i class="fa fa-check"></i><b>8.1.1</b> Modelos de Linguagem de Grande Escala</a></li>
<li class="chapter" data-level="8.1.2" data-path="próximos-passos.html"><a href="próximos-passos.html#multimodalidade"><i class="fa fa-check"></i><b>8.1.2</b> Multimodalidade</a></li>
<li class="chapter" data-level="8.1.3" data-path="próximos-passos.html"><a href="próximos-passos.html#vetorização-de-texto-em-tempo-real"><i class="fa fa-check"></i><b>8.1.3</b> Vetorização de Texto em Tempo Real</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="próximos-passos.html"><a href="próximos-passos.html#leitura-recomendada"><i class="fa fa-check"></i><b>8.2</b> Leitura Recomendada</a></li>
<li class="chapter" data-level="8.3" data-path="próximos-passos.html"><a href="próximos-passos.html#próximos-passos-1"><i class="fa fa-check"></i><b>8.3</b> Próximos Passos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="gabarito-das-questões.html"><a href="gabarito-das-questões.html"><i class="fa fa-check"></i>Gabarito das questões</a></li>
<li class="chapter" data-level="" data-path="sobre-os-autores.html"><a href="sobre-os-autores.html"><i class="fa fa-check"></i>Sobre os autores</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Vetorização de Texto com Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelos-avançados-de-vetorização" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Capítulo 5</span> Modelos Avançados de Vetorização<a href="modelos-avançados-de-vetorização.html#modelos-avançados-de-vetorização" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Neste capítulo, vamos explorar modelos avançados de vetorização de
texto, que vão além das técnicas tradicionais como Bag of Words e
TF-IDF. Vamos discutir embeddings contextuais, como o BERT e o GPT, e a
aplicação de redução de dimensionalidade usando técnicas como PCA e
t-SNE. A seguir, forneceremos exemplos práticos em Python para cada um
desses métodos.</p>
<div id="embeddings-contextuais" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Embeddings Contextuais<a href="modelos-avançados-de-vetorização.html#embeddings-contextuais" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Embeddings contextuais são vetores que capturam o significado de uma
palavra com base no contexto em que ela aparece. Diferente de embeddings
como Word2Vec e GloVe, que geram uma única representação para cada
palavra, modelos como BERT (Bidirectional Encoder Representations from
Transformers) e GPT (Generative Pretrained Transformer) produzem
diferentes embeddings para a mesma palavra, dependendo do seu contexto.</p>
<div id="exemplo-em-python-usando-bert-para-vetorização" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Exemplo em Python: Usando BERT para Vetorização<a href="modelos-avançados-de-vetorização.html#exemplo-em-python-usando-bert-para-vetorização" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb23" language="Python" caption="Código Python"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="modelos-avançados-de-vetorização.html#cb23-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb23-2"><a href="modelos-avançados-de-vetorização.html#cb23-2" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb23-3"><a href="modelos-avançados-de-vetorização.html#cb23-3" tabindex="-1"></a></span>
<span id="cb23-4"><a href="modelos-avançados-de-vetorização.html#cb23-4" tabindex="-1"></a><span class="co"># Carregando o tokenizer e o modelo BERT</span></span>
<span id="cb23-5"><a href="modelos-avançados-de-vetorização.html#cb23-5" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span>
<span id="cb23-6"><a href="modelos-avançados-de-vetorização.html#cb23-6" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span>
<span id="cb23-7"><a href="modelos-avançados-de-vetorização.html#cb23-7" tabindex="-1"></a></span>
<span id="cb23-8"><a href="modelos-avançados-de-vetorização.html#cb23-8" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb23-9"><a href="modelos-avançados-de-vetorização.html#cb23-9" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;O gato está sentado no tapete.&quot;</span></span>
<span id="cb23-10"><a href="modelos-avançados-de-vetorização.html#cb23-10" tabindex="-1"></a></span>
<span id="cb23-11"><a href="modelos-avançados-de-vetorização.html#cb23-11" tabindex="-1"></a><span class="co"># Tokenização do texto e conversão para tensores</span></span>
<span id="cb23-12"><a href="modelos-avançados-de-vetorização.html#cb23-12" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(texto, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb23-13"><a href="modelos-avançados-de-vetorização.html#cb23-13" tabindex="-1"></a></span>
<span id="cb23-14"><a href="modelos-avançados-de-vetorização.html#cb23-14" tabindex="-1"></a><span class="co"># Obtenção dos embeddings a partir do modelo BERT</span></span>
<span id="cb23-15"><a href="modelos-avançados-de-vetorização.html#cb23-15" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb23-16"><a href="modelos-avançados-de-vetorização.html#cb23-16" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb23-17"><a href="modelos-avançados-de-vetorização.html#cb23-17" tabindex="-1"></a></span>
<span id="cb23-18"><a href="modelos-avançados-de-vetorização.html#cb23-18" tabindex="-1"></a><span class="co"># Extraindo os embeddings da última camada oculta</span></span>
<span id="cb23-19"><a href="modelos-avançados-de-vetorização.html#cb23-19" tabindex="-1"></a>embeddings <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb23-20"><a href="modelos-avançados-de-vetorização.html#cb23-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Embeddings para cada token:&quot;</span>, embeddings)</span></code></pre></div>
<pre caption="Saída do Console"><code>Embeddings para cada token: tensor([[[-8.4815e-01, -3.2018e-01, -6.1327e-02,  ..., -3.2507e-01,
7.6626e-02,  9.1921e-01],
[-1.1552e+00, -8.0612e-02, -3.9960e-01,  ..., -1.1649e-01,
9.8960e-02,  1.0148e+00],
[ 4.3945e-01, -8.0826e-01,  2.9775e-01,  ...,  3.0753e-01,
1.6638e-01,  1.4990e+00],
...,
[-5.2439e-01, -4.2932e-01, -6.0938e-01,  ..., -4.7871e-01,
-3.9740e-01,  6.2908e-01],
[-4.5428e-02, -6.9424e-01,  1.2010e-02,  ...,  5.4892e-01,
-9.5592e-04, -1.7835e-01],
[ 6.6603e-01, -7.9354e-02,  1.1458e-02,  ..., -4.0039e-02,
-7.0081e-01,  2.2797e-02]]])</code></pre>
<p>Este exemplo mostra como usar o BERT para gerar embeddings contextuais.
O modelo BERT é capaz de gerar um vetor de embeddings para cada token no
texto, considerando o contexto de toda a frase.</p>
</div>
</div>
<div id="análise-de-sentimento-com-embeddings" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Análise de Sentimento com Embeddings<a href="modelos-avançados-de-vetorização.html#análise-de-sentimento-com-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Modelos de embeddings contextuais, como o BERT, podem ser utilizados em
tarefas de análise de sentimento, fornecendo representações mais ricas e
precisas das palavras. A seguir, aplicamos BERT em uma tarefa de
classificação de sentimento.</p>
<div id="exemplo-em-python-classificação-de-sentimento-com-bert" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Exemplo em Python: Classificação de Sentimento com BERT<a href="modelos-avançados-de-vetorização.html#exemplo-em-python-classificação-de-sentimento-com-bert" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb25" language="Python" caption="Código Python"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="modelos-avançados-de-vetorização.html#cb25-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification</span>
<span id="cb25-2"><a href="modelos-avançados-de-vetorização.html#cb25-2" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> softmax</span>
<span id="cb25-3"><a href="modelos-avançados-de-vetorização.html#cb25-3" tabindex="-1"></a></span>
<span id="cb25-4"><a href="modelos-avançados-de-vetorização.html#cb25-4" tabindex="-1"></a><span class="co"># Carregando o tokenizer e o modelo BERT para classificação de sequência</span></span>
<span id="cb25-5"><a href="modelos-avançados-de-vetorização.html#cb25-5" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb25-6"><a href="modelos-avançados-de-vetorização.html#cb25-6" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained(<span class="st">&#39;bert-base-uncased&#39;</span>, num_labels<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb25-7"><a href="modelos-avançados-de-vetorização.html#cb25-7" tabindex="-1"></a></span>
<span id="cb25-8"><a href="modelos-avançados-de-vetorização.html#cb25-8" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb25-9"><a href="modelos-avançados-de-vetorização.html#cb25-9" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Eu adoro gatos, eles são fofos&quot;</span></span>
<span id="cb25-10"><a href="modelos-avançados-de-vetorização.html#cb25-10" tabindex="-1"></a></span>
<span id="cb25-11"><a href="modelos-avançados-de-vetorização.html#cb25-11" tabindex="-1"></a><span class="co"># Tokenização do texto e conversão para tensores</span></span>
<span id="cb25-12"><a href="modelos-avançados-de-vetorização.html#cb25-12" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(texto, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb25-13"><a href="modelos-avançados-de-vetorização.html#cb25-13" tabindex="-1"></a></span>
<span id="cb25-14"><a href="modelos-avançados-de-vetorização.html#cb25-14" tabindex="-1"></a><span class="co"># Fazendo a previsão</span></span>
<span id="cb25-15"><a href="modelos-avançados-de-vetorização.html#cb25-15" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-16"><a href="modelos-avançados-de-vetorização.html#cb25-16" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb25-17"><a href="modelos-avançados-de-vetorização.html#cb25-17" tabindex="-1"></a></span>
<span id="cb25-18"><a href="modelos-avançados-de-vetorização.html#cb25-18" tabindex="-1"></a><span class="co"># Aplicando softmax para obter as probabilidades das classes</span></span>
<span id="cb25-19"><a href="modelos-avançados-de-vetorização.html#cb25-19" tabindex="-1"></a>probabilidades <span class="op">=</span> softmax(outputs.logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-20"><a href="modelos-avançados-de-vetorização.html#cb25-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probabilidade de sentimento positivo:&quot;</span>, probabilidades[<span class="dv">0</span>][<span class="dv">1</span>].item())</span></code></pre></div>
<pre caption="Saída do Console"><code>Probabilidade de sentimento positivo: 0.6633508801460266</code></pre>
<p>Este código ilustra como utilizar o BERT para classificar o sentimento
de um texto. Aqui, o modelo é capaz de prever a probabilidade de um
sentimento positivo ou negativo para a frase fornecida.</p>
</div>
</div>
<div id="redução-de-dimensionalidade" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Redução de Dimensionalidade<a href="modelos-avançados-de-vetorização.html#redução-de-dimensionalidade" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Vetores de alta dimensionalidade podem ser difíceis de manipular e
visualizar. Técnicas como Análise de Componentes Principais (PCA) e
t-SNE (t-Distributed Stochastic Neighbor Embedding) são comumente usadas
para reduzir a dimensionalidade dos dados de forma a manter as
informações mais importantes.</p>
<div id="exemplo-em-python-redução-de-dimensionalidade-com-pca" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Exemplo em Python: Redução de Dimensionalidade com PCA<a href="modelos-avançados-de-vetorização.html#exemplo-em-python-redução-de-dimensionalidade-com-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb27" language="Python" caption="Código Python"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="modelos-avançados-de-vetorização.html#cb27-1" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb27-2"><a href="modelos-avançados-de-vetorização.html#cb27-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-3"><a href="modelos-avançados-de-vetorização.html#cb27-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb27-4"><a href="modelos-avançados-de-vetorização.html#cb27-4" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb27-5"><a href="modelos-avançados-de-vetorização.html#cb27-5" tabindex="-1"></a></span>
<span id="cb27-6"><a href="modelos-avançados-de-vetorização.html#cb27-6" tabindex="-1"></a><span class="co"># Carregando o tokenizer e o modelo BERT</span></span>
<span id="cb27-7"><a href="modelos-avançados-de-vetorização.html#cb27-7" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span>
<span id="cb27-8"><a href="modelos-avançados-de-vetorização.html#cb27-8" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span>
<span id="cb27-9"><a href="modelos-avançados-de-vetorização.html#cb27-9" tabindex="-1"></a></span>
<span id="cb27-10"><a href="modelos-avançados-de-vetorização.html#cb27-10" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb27-11"><a href="modelos-avançados-de-vetorização.html#cb27-11" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Eu adoro gatos, eles são fofos&quot;</span></span>
<span id="cb27-12"><a href="modelos-avançados-de-vetorização.html#cb27-12" tabindex="-1"></a></span>
<span id="cb27-13"><a href="modelos-avançados-de-vetorização.html#cb27-13" tabindex="-1"></a><span class="co"># Tokenização do texto e conversão para tensores</span></span>
<span id="cb27-14"><a href="modelos-avançados-de-vetorização.html#cb27-14" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(texto, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb27-15"><a href="modelos-avançados-de-vetorização.html#cb27-15" tabindex="-1"></a></span>
<span id="cb27-16"><a href="modelos-avançados-de-vetorização.html#cb27-16" tabindex="-1"></a><span class="co"># Obtenção dos embeddings a partir do modelo BERT</span></span>
<span id="cb27-17"><a href="modelos-avançados-de-vetorização.html#cb27-17" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb27-18"><a href="modelos-avançados-de-vetorização.html#cb27-18" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb27-19"><a href="modelos-avançados-de-vetorização.html#cb27-19" tabindex="-1"></a></span>
<span id="cb27-20"><a href="modelos-avançados-de-vetorização.html#cb27-20" tabindex="-1"></a><span class="co"># Extraindo os embeddings da última camada oculta</span></span>
<span id="cb27-21"><a href="modelos-avançados-de-vetorização.html#cb27-21" tabindex="-1"></a>embeddings <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb27-22"><a href="modelos-avançados-de-vetorização.html#cb27-22" tabindex="-1"></a></span>
<span id="cb27-23"><a href="modelos-avançados-de-vetorização.html#cb27-23" tabindex="-1"></a><span class="co"># Exemplo de vetores de alta dimensionalidade (usaremos os embeddings do BERT)</span></span>
<span id="cb27-24"><a href="modelos-avançados-de-vetorização.html#cb27-24" tabindex="-1"></a>vetores <span class="op">=</span> embeddings.squeeze().numpy()  <span class="co"># Convertendo de tensor para numpy array</span></span>
<span id="cb27-25"><a href="modelos-avançados-de-vetorização.html#cb27-25" tabindex="-1"></a></span>
<span id="cb27-26"><a href="modelos-avançados-de-vetorização.html#cb27-26" tabindex="-1"></a><span class="co"># Aplicando PCA para reduzir para 2 dimensões</span></span>
<span id="cb27-27"><a href="modelos-avançados-de-vetorização.html#cb27-27" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb27-28"><a href="modelos-avançados-de-vetorização.html#cb27-28" tabindex="-1"></a>vetores_reduzidos <span class="op">=</span> pca.fit_transform(vetores)</span>
<span id="cb27-29"><a href="modelos-avançados-de-vetorização.html#cb27-29" tabindex="-1"></a></span>
<span id="cb27-30"><a href="modelos-avançados-de-vetorização.html#cb27-30" tabindex="-1"></a><span class="co"># Plotando os vetores em 2D</span></span>
<span id="cb27-31"><a href="modelos-avançados-de-vetorização.html#cb27-31" tabindex="-1"></a>plt.scatter(vetores_reduzidos[:, <span class="dv">0</span>], vetores_reduzidos[:, <span class="dv">1</span>])</span>
<span id="cb27-32"><a href="modelos-avançados-de-vetorização.html#cb27-32" tabindex="-1"></a>plt.title(<span class="st">&quot;Vetores de palavras reduzidos para 2D usando PCA&quot;</span>)</span>
<span id="cb27-33"><a href="modelos-avançados-de-vetorização.html#cb27-33" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="float">
<img src="figuras/pca.png" style="width:70.0%" alt="image" />
<div class="figcaption">image</div>
</div>
<p>Neste exemplo, utilizamos PCA para reduzir os embeddings gerados pelo
BERT para duas dimensões, facilitando a visualização.</p>
</div>
<div id="exemplo-em-python-redução-de-dimensionalidade-com-t-sne" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Exemplo em Python: Redução de Dimensionalidade com t-SNE<a href="modelos-avançados-de-vetorização.html#exemplo-em-python-redução-de-dimensionalidade-com-t-sne" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb28" language="Python" caption="Código Python"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="modelos-avançados-de-vetorização.html#cb28-1" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb28-2"><a href="modelos-avançados-de-vetorização.html#cb28-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-3"><a href="modelos-avançados-de-vetorização.html#cb28-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb28-4"><a href="modelos-avançados-de-vetorização.html#cb28-4" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb28-5"><a href="modelos-avançados-de-vetorização.html#cb28-5" tabindex="-1"></a></span>
<span id="cb28-6"><a href="modelos-avançados-de-vetorização.html#cb28-6" tabindex="-1"></a><span class="co"># Carregando o tokenizer e o modelo BERT</span></span>
<span id="cb28-7"><a href="modelos-avançados-de-vetorização.html#cb28-7" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span>
<span id="cb28-8"><a href="modelos-avançados-de-vetorização.html#cb28-8" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">&quot;bert-base-uncased&quot;</span>)</span>
<span id="cb28-9"><a href="modelos-avançados-de-vetorização.html#cb28-9" tabindex="-1"></a></span>
<span id="cb28-10"><a href="modelos-avançados-de-vetorização.html#cb28-10" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb28-11"><a href="modelos-avançados-de-vetorização.html#cb28-11" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Eu adoro gatos, eles são fofos&quot;</span></span>
<span id="cb28-12"><a href="modelos-avançados-de-vetorização.html#cb28-12" tabindex="-1"></a></span>
<span id="cb28-13"><a href="modelos-avançados-de-vetorização.html#cb28-13" tabindex="-1"></a><span class="co"># Tokenização do texto e conversão para tensores</span></span>
<span id="cb28-14"><a href="modelos-avançados-de-vetorização.html#cb28-14" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(texto, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb28-15"><a href="modelos-avançados-de-vetorização.html#cb28-15" tabindex="-1"></a></span>
<span id="cb28-16"><a href="modelos-avançados-de-vetorização.html#cb28-16" tabindex="-1"></a><span class="co"># Obtenção dos embeddings a partir do modelo BERT</span></span>
<span id="cb28-17"><a href="modelos-avançados-de-vetorização.html#cb28-17" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb28-18"><a href="modelos-avançados-de-vetorização.html#cb28-18" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb28-19"><a href="modelos-avançados-de-vetorização.html#cb28-19" tabindex="-1"></a></span>
<span id="cb28-20"><a href="modelos-avançados-de-vetorização.html#cb28-20" tabindex="-1"></a><span class="co"># Extraindo os embeddings da última camada oculta</span></span>
<span id="cb28-21"><a href="modelos-avançados-de-vetorização.html#cb28-21" tabindex="-1"></a>embeddings <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb28-22"><a href="modelos-avançados-de-vetorização.html#cb28-22" tabindex="-1"></a></span>
<span id="cb28-23"><a href="modelos-avançados-de-vetorização.html#cb28-23" tabindex="-1"></a>vetores <span class="op">=</span> embeddings.squeeze().numpy()  <span class="co"># Convertendo de tensor para numpy array</span></span>
<span id="cb28-24"><a href="modelos-avançados-de-vetorização.html#cb28-24" tabindex="-1"></a></span>
<span id="cb28-25"><a href="modelos-avançados-de-vetorização.html#cb28-25" tabindex="-1"></a><span class="co"># Aplicando t-SNE para reduzir para 2 dimensões</span></span>
<span id="cb28-26"><a href="modelos-avançados-de-vetorização.html#cb28-26" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb28-27"><a href="modelos-avançados-de-vetorização.html#cb28-27" tabindex="-1"></a>vetores_reduzidos_tsne <span class="op">=</span> tsne.fit_transform(vetores)</span>
<span id="cb28-28"><a href="modelos-avançados-de-vetorização.html#cb28-28" tabindex="-1"></a></span>
<span id="cb28-29"><a href="modelos-avançados-de-vetorização.html#cb28-29" tabindex="-1"></a><span class="co"># Plotando os vetores em 2D</span></span>
<span id="cb28-30"><a href="modelos-avançados-de-vetorização.html#cb28-30" tabindex="-1"></a>plt.scatter(vetores_reduzidos_tsne[:, <span class="dv">0</span>], vetores_reduzidos_tsne[:, <span class="dv">1</span>])</span>
<span id="cb28-31"><a href="modelos-avançados-de-vetorização.html#cb28-31" tabindex="-1"></a>plt.title(<span class="st">&quot;Vetores de palavras reduzidos para 2D usando t-SNE&quot;</span>)</span>
<span id="cb28-32"><a href="modelos-avançados-de-vetorização.html#cb28-32" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="float">
<img src="figuras/vet.png" style="width:70.0%" alt="image" />
<div class="figcaption">image</div>
</div>
<p>Aqui, usamos t-SNE para reduzir os embeddings para duas dimensões. O
t-SNE é especialmente útil para a visualização de dados em espaços de
alta dimensionalidade.</p>
<p>Em resumo, exploramos modelos avançados de vetorização de texto,
incluindo embeddings contextuais com BERT e GPT, e técnicas de redução
de dimensionalidade como PCA e t-SNE. Esses métodos fornecem ferramentas
poderosas para capturar e visualizar informações complexas em dados
textuais.</p>
</div>
</div>
<div id="exercício" class="section level2 unnumbered hasAnchor">
<h2>Exercício<a href="modelos-avançados-de-vetorização.html#exercício" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Versão on-line destes exercícios</p>
<p><a href="https://forms.gle/hh4BgCZVhJLMyFiy6" class="uri">https://forms.gle/hh4BgCZVhJLMyFiy6</a></p>
<ol style="list-style-type: decimal">
<li><p><strong>Qual é a principal vantagem dos embeddings contextuais, como BERT,
em comparação com embeddings tradicionais como Word2Vec?</strong></p>
<ol style="list-style-type: decimal">
<li><p>Embeddings contextuais capturam o significado das palavras com
base em seu contexto específico.</p></li>
<li><p>Embeddings contextuais são sempre mais rápidos de treinar.</p></li>
<li><p>Embeddings contextuais geram representações esparsas das
palavras.</p></li>
<li><p>Embeddings contextuais são menos precisos do que os embeddings
tradicionais.</p></li>
</ol></li>
<li><p><strong>O que a técnica de PCA (Análise de Componentes Principais) realiza
em dados de alta dimensionalidade?</strong></p>
<ol style="list-style-type: decimal">
<li><p>Aumenta o número de dimensões nos dados.</p></li>
<li><p>Reduz a dimensionalidade dos dados mantendo a maior
variabilidade possível.</p></li>
<li><p>Gera novas características que não são correlacionadas.</p></li>
<li><p>Elimina completamente a variabilidade dos dados.</p></li>
</ol></li>
<li><p><strong>Qual é a função principal da técnica t-SNE?</strong></p>
<ol style="list-style-type: decimal">
<li><p>Agrupar dados de alta dimensionalidade.</p></li>
<li><p>Visualizar dados de alta dimensionalidade em espaços de menor
dimensão.</p></li>
<li><p>Normalizar dados textuais.</p></li>
<li><p>Criar novas features a partir dos dados originais.</p></li>
</ol></li>
<li><p><strong>Por que técnicas de redução de dimensionalidade, como PCA, são
úteis em vetorização de texto?</strong></p>
<ol style="list-style-type: decimal">
<li><p>Elas aumentam a precisão dos modelos de aprendizado de máquina.</p></li>
<li><p>Elas eliminam a necessidade de embeddings contextuais.</p></li>
<li><p>Elas reduzem a quantidade de dados de entrada para tornar o
processamento mais eficiente e visualizável.</p></li>
<li><p>Elas criam novas features para melhorar o desempenho de modelos
de deep learning.</p></li>
</ol></li>
<li><p><strong>Em qual cenário a redução de dimensionalidade é particularmente
útil?</strong></p>
<ol style="list-style-type: decimal">
<li><p>Quando se deseja aumentar a complexidade do modelo.</p></li>
<li><p>Quando os dados têm poucas features e baixa variabilidade.</p></li>
<li><p>Quando os dados têm muitas dimensões e se deseja melhorar a
visualização ou performance do modelo.</p></li>
<li><p>Quando se deseja eliminar o ruído dos dados, independentemente
da dimensionalidade.</p></li>
</ol></li>
</ol>
</div>
</div>
<script async defer src="https://hypothes.is/embed.js"></script>
<link href='https://fonts.googleapis.com/css?family=Arvo' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Arvo' rel='stylesheet' type='text/css'>
            </section>

          </div>
        </div>
      </div>
<a href="vetorização-de-texto.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="aplicações-práticas-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
