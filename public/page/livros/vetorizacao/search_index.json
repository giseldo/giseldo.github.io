[["index.html", "Vetorização de Texto com Python Informações Adicionais", " Vetorização de Texto com Python Giseldo Neo Informações Adicionais O repositório com o código-fonte dos exemplos do livro está disponível na internet no endereço: https://github.com/giseldo/livrovetorizacao.git Se você encontrou algum erro, deseja enviar alguma sugestão ou está com alguma dúvida, envie um e-mail para giseldo@gmail.com. "],["prefácio.html", "Prefácio", " Prefácio O processamento de linguagem natural (PLN) tem se tornado cada vez mais relevante na era da informação, onde a habilidade de transformar dados textuais em insights acionáveis é crucial. Este livro oferece uma introdução concisa, porém abrangente, ao processo de vetorização de texto, uma técnica fundamental em PLN. Espero que este livro seja um ponto de partida útil para suas futuras explorações no mundo do processamento de linguagem natural. Boa leitura! Giseldo Neo "],["introdução-à-vetorização-de-texto.html", "Capítulo 1 Introdução à Vetorização de Texto 1.1 Conceito de Vetorização de Texto 1.2 Importância na Análise de Dados e Aprendizado de Máquina 1.3 Aplicações Práticas Exercícios", " Capítulo 1 Introdução à Vetorização de Texto 1.1 Conceito de Vetorização de Texto O processamento de linguagem natural (PLN) é um campo interdisciplinar que se concentra em permitir que computadores compreendam, interpretem e gerem linguagem humana. Uma etapa fundamental no PLN é a vetorização de texto, que consiste em transformar texto, um dado textual, em uma representação numérica que possa ser processada por algoritmos de machine learning. Algoritmos de machine learning, como redes neurais e algoritmos de clustering, operam em espaços vetoriais. Ao representar palavras e documentos como vetores numéricos, podemos aplicar essas ferramentas para realizar tarefas como classificação, clustering e geração de texto. Podemos pensar na vetorização de texto como uma forma de mapear palavras e documentos para um espaço vetorial multidimensional. Cada dimensão desse espaço representa uma característica particular do texto, como a frequência de uma palavra, o contexto em que ela aparece ou sua relação semântica com outras palavras. Por exemplo, a palavra “cachorro” poderia ser representada por um vetor numérico de 70 dimensões, onde cada dimensão corresponde a uma característica específica, como a presença da palavra em um contexto relacionado a animais, a sua similaridade com a palavra \"cão\", etc. Ao transformar texto em vetores, podemos aplicar algoritmos de machine learning para extrair informações valiosas e realizar tarefas complexas. A vetorização de texto é o processo de transformar texto em uma representação numérica que pode ser utilizada por algoritmos de aprendizado de máquina. 1.2 Importância na Análise de Dados e Aprendizado de Máquina Na era dos dados digitais, o texto é uma das formas mais abundantes de dados não estruturados. Analisar e extrair informações úteis de texto requer sua conversão para uma forma numérica, permitindo a aplicação de métodos estatísticos e de aprendizado de máquina. Uma das principais razões para a vetorização é que a maioria dos modelos matemáticos e algoritmos de machine learning requerem números como entrada, em vez de texto bruto. Ela é essencial para tarefas como classificação de texto, análise de sentimentos, detecção de tópicos e muitos outros. 1.2.1 Exemplo em Python Para ilustrar a importância da vetorização, vamos começar com um exemplo simples de contagem de palavras em um conjunto de documentos. Este exemplo utiliza a biblioteca CountVectorizer do Scikit-learn. pip install scikit-learn from sklearn.feature_extraction.text import CountVectorizer # Exemplo de documentos documentos = [ &quot;Texto de exemplo para vetorização&quot;, &quot;Outro exemplo de texto&quot;, &quot;Vetorização de texto é essencial&quot; ] # Criação do vetor de contagem vectorizer = CountVectorizer() X = vectorizer.fit_transform(documentos) # Exibição do vetor de características print(&quot;Vetor de características:\\n&quot;, vectorizer.get_feature_names_out()) print(&quot;Matriz BoW:\\n&quot;, X.toarray()) Vetor de características: [&#39;de&#39; &#39;essencial&#39; &#39;exemplo&#39; &#39;outro&#39; &#39;para&#39; &#39;texto&#39; &#39;vetorização&#39;] Matriz BoW: [[1 0 1 0 1 1 1] [1 0 1 1 0 1 0] [1 1 0 0 0 1 1]] Neste exemplo, as frases são transformadas em uma matriz de contagem de palavras (Bag of Words), onde cada linha representa um documento e cada coluna representa uma palavra do vocabulário. 1.3 Aplicações Práticas A vetorização de texto encontra aplicação em diversas áreas, abrangendo múltiplos domínios do conhecimento. No campo da mineração de dados, por exemplo, a vetorização de texto é utilizada para transformar grandes volumes de dados textuais em formas estruturadas que permitem a análise quantitativa. Nos motores de busca, essa técnica possibilita que as palavras-chaves nas pesquisas dos usuários sejam comparadas eficientemente com o conteúdo indexado, proporcionando resultados mais relevantes. Na aprendizagem de máquina, a vetorização de texto é essencial para alimentar algoritmos com entradas numéricas derivadas de textos, permitindo o treinamento de modelos para tarefas como classificação de sentimentos, detecção de spam e tradução automática. Além disso, em áreas como a análise de redes sociais, a vetorização de texto facilita a identificação de tendências e padrões ao converter postagens e comentários em matrizes numéricas, possibilitando a análise estatística. A versatilidade da vetorização de texto abre caminhos para sua aplicação em variados contextos, auxiliando na extração de insights valiosos a partir de dados textuais.Classificação de texto: Determinar a categoria de um documento (ex.: spam ou não-spam). Classificação de texto: Determinar a categoria de um documento (ex.: spam ou não-spam). Análise de sentimentos: Identificar a polaridade de opiniões em textos (ex.: positiva, negativa ou neutra). Por exemplo, a detecção de tendências e sentimentos em plataformas como Twitter e Facebook (Pak and Paroubek 2010) Sistemas de recomendação: Sugerir produtos ou conteúdos com base em descrições textuais. Busca e recuperação de informação: Melhorar a relevância dos resultados de busca analisando o conteúdo dos documentos. Motores de busca: Utilizam vetorização de texto para indexar páginas web e retornar resultados relevantes para as consultas dos usuários (Manning, Raghavan, and Schütze 2008) Assistentes virtuais: Ferramentas como Alexa e Siri utilizam técnicas de vetorização para entender e responder a comandos de voz (Jurafsky and Martin 2000). Em resumo, a vetorização de texto é um passo essencial para qualquer tarefa de processamento de linguagem natural (PLN). Compreender as diferentes técnicas de vetorização e sua aplicação prática permite a construção de modelos mais precisos e eficientes para análise de dados textuais. Exercícios Versão on-line destes exercícios https://forms.gle/fz6gmAz5GYE2jHJF6 O que é vetorização de texto? Processo de transformar números em texto. Processo de transformar texto em uma representação numérica. Técnica de compressão de arquivos de texto. Método para gerar texto automaticamente. Qual das seguintes opções é uma aplicação da vetorização de texto? Envio de textos pela internet. Compressão de arquivos de texto. Classificação de texto. Edição de documentos de texto. Por que a vetorização de texto é importante no aprendizado de máquina? Porque algoritmos de aprendizado de máquina funcionam diretamente com texto bruto. Porque algoritmos de aprendizado de máquina requerem dados numéricos para processamento. Porque a vetorização é a única maneira de analisar grandes volumes de texto. Porque a vetorização simplifica a criação de modelos visuais. Qual é uma das principais razões para utilizar a vetorização de texto em projetos de aprendizado de máquina? Para melhorar a legibilidade dos textos. Para permitir que algoritmos de aprendizado de máquina processem dados textuais, que precisam ser convertidos em números. Para aumentar o tamanho do corpus de texto. Para corrigir erros ortográficos automaticamente. Qual das seguintes etapas não faz parte do pré-processamento de texto? Limpeza de texto. Tokenização. Treinamento de modelo. Remoção de stopwords. References Jurafsky, Daniel, and James H Martin. 2000. Speech and Language Processing. Prentice Hall. Manning, Christopher D, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press. Pak, Alexander, and Patrick Paroubek. 2010. “Twitter as a Corpus for Sentiment Analysis and Opinion Mining.” In Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10), 1320–26. European Language Resources Association (ELRA). "],["preparação-do-ambiente.html", "Capítulo 2 Preparação do Ambiente 2.1 Instalação do Python 2.2 Instalação de Bibliotecas Necessárias 2.3 Introdução ao Jupyter Notebook Exercícios", " Capítulo 2 Preparação do Ambiente Neste capítulo, abordaremos como preparar o ambiente necessário para trabalhar com vetorização de texto em Python. Isso inclui a instalação do Python e das bibliotecas necessárias, além de uma breve introdução ao uso do Jupyter Notebook. 2.1 Instalação do Python Para começar a trabalhar com vetorização de texto, é essencial ter o Python instalado. Python é uma linguagem de programação amplamente utilizada para análise de dados e aprendizado de máquina devido à sua simplicidade e a vasta gama de bibliotecas disponíveis. 2.1.1 Instalando o Python Para instalar o Python, siga as instruções abaixo: No Windows, baixe o instalador do site oficial do Python (https://www.python.org/) e siga as instruções do instalador. No macOS, você pode usar o Homebrew para instalar o Python executando o comando brew install python. No Linux, o Python geralmente já está instalado, mas você pode atualizá-lo usando o gerenciador de pacotes da sua distribuição (apt-get, yum, etc.). 2.2 Instalação de Bibliotecas Necessárias Uma vez que o Python esteja instalado, precisamos instalar algumas bibliotecas que são fundamentais para a vetorização de texto. Entre as principais estão NumPy, Pandas, Scikit-learn, NLTK e SpaCy. 2.2.1 Instalando Bibliotecas com pip O pip é o gerenciador de pacotes padrão do Python. Você pode instalar as bibliotecas necessárias usando o seguinte comando: pip install numpy pandas scikit-learn nltk spacy 2.2.2 Exemplo em Python: Verificando Instalações Após instalar as bibliotecas, é importante verificar se elas foram instaladas corretamente: import numpy as np import pandas as pd import sklearn import nltk import spacy print(&quot;NumPy version:&quot;, np.__version__) print(&quot;Pandas version:&quot;, pd.__version__) print(&quot;Scikit-learn version:&quot;, sklearn.__version__) print(&quot;NLTK version:&quot;, nltk.__version__) print(&quot;SpaCy version:&quot;, spacy.__version__) NumPy version: 1.26.4 Pandas version: 2.2.2 Scikit-learn version: 1.5.1 NLTK version: 3.9.1 SpaCy version: 3.7.6 Este código importará as bibliotecas e exibirá suas versões, garantindo que todas estejam corretamente instaladas. 2.3 Introdução ao Jupyter Notebook O Jupyter Notebook é uma ferramenta poderosa para o desenvolvimento de scripts em Python, permitindo a combinação de código, texto, visualizações e resultados em um único documento. 2.3.1 Instalando o Jupyter Notebook Você pode instalar o Jupyter Notebook usando o pip: pip install jupyterlab Para iniciar o Jupyter Notebook, execute o seguinte comando no terminal: jupyter notebook Isso abrirá o Jupyter Notebook no seu navegador padrão, permitindo que você comece a escrever e executar código Python de maneira interativa. 2.3.2 Exemplo em Python: Primeiros Passos no Jupyter Um exemplo simples de uso do Jupyter Notebook seria a criação de uma célula de código para calcular a soma de dois números: a = 10 b = 20 print(&quot;A soma de a e b é:&quot;, a + b) Este exemplo demonstra a simplicidade e interatividade que o Jupyter Notebook oferece, permitindo que você execute código Python célula por célula e veja os resultados imediatamente. Em resumo, configuramos o ambiente necessário para trabalhar com vetorização de texto em Python. Com Python e as principais bibliotecas instaladas, além da configuração do Jupyter Notebook, você está pronto para explorar e aplicar técnicas de vetorização de texto. Exercícios Versão on-line destes exercícios https://forms.gle/wXiSykwH1QRdLovx5 Qual é o objetivo principal da preparação do ambiente para vetorização de texto em Python? Instalar editores de texto avançados. Configurar o ambiente de desenvolvimento com Python e bibliotecas necessárias. Criar um ambiente gráfico para visualização de dados. Desenvolver interfaces de usuário. Qual é o objetivo da biblioteca Scikit-learn? Visualizar gráficos e imagens. Manipular dados em planilhas. Fornecer ferramentas para aprendizado de máquina, incluindo vetorização de texto. Realizar cálculos matemáticos avançados. Para que serve a biblioteca NLTK em um projeto de vetorização de texto? Para visualização de gráficos. Para manipulação de grandes volumes de dados numéricos. Para processamento e análise de texto natural. Para criação de modelos de aprendizado profundo. Qual ferramenta permite a criação de documentos interativos combinando código, texto e visualizações? Ganymed. Jupyter Notebook. Ms Paint. SQL Server. Qual comando é utilizado para instalar a biblioteca Scikit-learn usando pip? pip install numpy pip install pandas pip install scikit-learn pip install matplotlib "],["pré-processamento-de-texto.html", "Capítulo 3 Pré-processamento de Texto 3.1 Limpeza de Texto 3.2 Tokenização 3.3 Lematização e Stemming Exercícios", " Capítulo 3 Pré-processamento de Texto O pré-processamento de texto é uma etapa crucial na análise de dados textuais e no aprendizado de máquina. Antes de aplicar técnicas de vetorização, é essencial transformar e limpar os dados de texto para que possam ser processados eficientemente pelos algoritmos. Neste capítulo, abordaremos as principais técnicas de pré-processamento de texto, incluindo limpeza, tokenização, lematização e stemming. 3.1 Limpeza de Texto A limpeza de texto envolve a remoção de elementos indesejados, como stopwords, pontuação, números e caracteres especiais, que não contribuem para a análise. Abaixo está um exemplo em Python de como realizar a limpeza básica de texto usando a biblioteca re para expressões regulares e NLTK para remoção de stopwords. 3.1.1 Exemplo em Python: Limpeza de Texto O código a seguir remove as stopwords e pontuação de determinado texto. import re import nltk from nltk.corpus import stopwords # Certifique-se de baixar as stopwords nltk.download(&quot;stopwords&quot;) # Exemplo de texto texto = &quot;O processo de vetorização de texto envolve várias etapas, como a limpeza do texto!&quot; # Convertendo para minúsculas texto = texto.lower() # Removendo pontuação e caracteres especiais texto = re.sub(r&quot;[^\\w\\s]&quot;, &quot;&quot;, texto) print(&quot;Texto sem pontuação:&quot;, texto) # Removendo stopwords stop_words = set(stopwords.words(&quot;portuguese&quot;)) texto_limpo = &quot; &quot;.join([palavra for palavra in texto.split() if palavra not in stop_words]) print(&quot;Texto limpo:&quot;, texto_limpo) Texto sem pontuação: o processo de vetorização de texto envolve várias etapas como a limpeza do texto Texto limpo: processo vetorização texto envolve várias etapas limpeza texto Este código converte o texto para minúsculas, remove pontuação e stopwords, resultando em uma versão limpa do texto pronta para vetorização. 3.2 Tokenização A tokenização é o processo de dividir o texto em unidades menores, como palavras ou frases. Essas unidades são chamadas de tokens. A tokenização pode ser feita de várias formas, dependendo da granularidade desejada. A seguir, mostramos como tokenizar um texto usando a biblioteca NLTK. 3.2.1 Exemplo em Python: Tokenização import nltk # Certifique-se de baixar o tokenizer nltk.download(&quot;punkt&quot;) from nltk.tokenize import word_tokenize # Exemplo de texto texto = &quot;Tokenização é o processo de dividir o texto em palavras ou frases.&quot; # Tokenizando o texto tokens = word_tokenize(texto) print(&quot;Tokens:&quot;, tokens) Tokens: [&#39;Tokenização&#39;, &#39;é&#39;, &#39;o&#39;, &#39;processo&#39;, &#39;de&#39;, &#39;dividir&#39;, &#39;o&#39;, &#39;texto&#39;, &#39;em&#39;, &#39;palavras&#39;, &#39;ou&#39;, &#39;frases&#39;, &#39;.&#39;] Neste exemplo, o texto é dividido em tokens individuais, que podem ser utilizados para análise posterior, como vetorização. 3.3 Lematização e Stemming Lematização e stemming são técnicas para reduzir as palavras às suas formas base ou raízes. A lematização considera o contexto e reduz as palavras ao seu lema, enquanto o stemming simplesmente corta os sufixos para encontrar a raiz. 3.3.1 Exemplo em Python: Lematização e Stemming from nltk.stem import PorterStemmer, WordNetLemmatizer # Certifique-se de baixar o WordNet nltk.download(&#39;wordnet&#39;) nltk.download(&#39;omw-1.4&#39;) # Exemplo de texto palavras = [&quot;running&quot;, &quot;jumps&quot;, &quot;easily&quot;, &quot;fairly&quot;] # Inicializando Stemmer e Lemmatizer stemmer = PorterStemmer() lemmatizer = WordNetLemmatizer() # Aplicando Stemming e Lematização stems = [stemmer.stem(palavra) for palavra in palavras] lemmas = [lemmatizer.lemmatize(palavra) for palavra in palavras] print(&quot;Stemming:&quot;, stems) print(&quot;Lematização:&quot;, lemmas) Stemming: [&#39;run&#39;, &#39;jump&#39;, &#39;easili&#39;, &#39;fairli&#39;] Lematização: [&#39;running&#39;, &#39;jump&#39;, &#39;easily&#39;, &#39;fairly&#39;] Este código demonstra como aplicar stemming e lematização em um conjunto de palavras, resultando em suas formas raiz e lema, respectivamente. Em resumo, o pré-processamento de texto é uma etapa essencial para garantir que os dados textuais estejam em uma forma adequada para a vetorização e para o aprendizado de máquina. Técnicas como limpeza, tokenização, lematização e stemming são fundamentais para preparar o texto para análise. Exercícios Versão on-line destes exercícios https://forms.gle/FFGEvmi1zmrQ5Npk9 Qual é o objetivo principal do pré-processamento de texto? Melhorar a legibilidade do texto. Transformar o texto em uma forma que possa ser processada por algoritmos de aprendizado de máquina. Corrigir erros ortográficos no texto. Aumentar o tamanho do corpus de dados. Qual das seguintes etapas faz parte do pré-processamento de texto? Tokenização. Treinamento do modelo. Avaliação do desempenho do modelo. Geração de dados sintéticos. O que é tokenização no contexto do pré-processamento de texto? O processo de corrigir erros ortográficos em um texto. O processo de dividir um texto em palavras, frases ou outros elementos significativos. O processo de remover palavras irrelevantes de um texto. O processo de transformar texto em vetores numéricos. Qual técnica de pré-processamento reduz palavras às suas formas base ou raízes? Tokenização. Lematização e Stemming. Vetorização. Filtragem de stopwords. Qual biblioteca Python é comumente usada para realizar a tokenização e outras tarefas de pré-processamento de texto? NumPy. Pandas. NLTK. Matplotlib. "],["vetorização-de-texto.html", "Capítulo 4 Vetorização de Texto 4.1 Bag of Words (BoW) 4.2 Term Frequency-Inverse Document Frequency (TF-IDF) 4.3 Word Embeddings Exercícios", " Capítulo 4 Vetorização de Texto Neste capítulo, exploraremos as diferentes técnicas de vetorização de texto, que são essenciais para transformar dados textuais em uma forma numérica que pode ser utilizada por algoritmos de aprendizado de máquina. Abordaremos o Bag of Words (BoW), TF-IDF e Word Embeddings, com exemplos práticos em Python. 4.1 Bag of Words (BoW) O Bag of Words (BoW) é uma das técnicas mais simples de vetorização de texto. Ele representa um texto como um conjunto de palavras, desconsiderando a ordem das palavras, mas mantendo a multiplicidade. Ele simplifica a representação textual para que possa ser utilizada em tarefas de aprendizado de máquina. Imagine um texto como um saco onde jogamos todas as palavras, desconsiderando a ordem em que elas aparecem e contando apenas a frequência com que cada uma delas ocorre. Como funciona: Tokenização: O texto é dividido em unidades individuais, geralmente palavras. Criação do Vocabulário: É criado um conjunto de todas as palavras únicas presentes em um corpus de textos. Representação Vetorial: Cada documento é representado como um vetor numérico, onde cada posição corresponde a uma palavra do vocabulário e o valor numérico indica a frequência dessa palavra no documento. Por exemplo, considere dois documentos: Documento 1: \"O gato subiu na árvore.\" Documento 2: \"O cachorro latindo para o gato.\" O vocabulário seria: o, gato, subiu, na, árvore, cachorro, latindo, para. A representação vetorial dos documentos poderia ser: Documento 1: \\[1, 2, 1, 1, 1, 0, 0, 0\\] Documento 2: \\[1, 1, 0, 0, 0, 1, 1, 1\\] O Bag-of-Words é fácil de implementar e entender; permite a aplicação de algoritmos de aprendizado de máquina em grandes volumes de texto; além disso, pode ser utilizado em diversas tarefas de PLN, como classificação de textos, clustering e recuperação de informação. Algumas Limitações é que ele ignora a ordem das palavras e a estrutura gramatical, o que pode levar à perda de significado; além disso existe uma dificuldade em lidar com sinônimos e palavras ambíguas, pois, palavras com significados semelhantes podem ser tratadas como diferentes e palavras com múltiplos significados podem causar ambiguidade. Algumas aplicações são: a classificação de textos para identificar a categoria de um texto (por exemplo, spam ou não spam); a clusterização, ou seja, agrupar documentos semelhantes; outra é a recuperação de informação para encontrar documentos relevantes em resposta a uma consulta; além disso, a análise de sentimentos para determinar a polaridade de um texto (positivo, negativo ou neutro). 4.1.1 Exemplo em Python: Bag of Words from sklearn.feature_extraction.text import CountVectorizer # Exemplo de documentos documentos = [ &quot;existe gato laranja&quot;, &quot;existe gato preto&quot;, &quot;também existe gato branco&quot; ] # Criação do modelo BoW vectorizer = CountVectorizer() X = vectorizer.fit_transform(documentos) # Exibição da matriz BoW print(&quot;Vetor de características:\\n&quot;, vectorizer. get_feature_names_out()) print(&quot;Matriz BoW:\\n&quot;, X.toarray()) Vetor de características: [&#39;branco&#39; &#39;existe&#39; &#39;gato&#39; &#39;laranja&#39; &#39;preto&#39; &#39;também&#39;] Matriz BoW: [[0 1 1 1 0 0] [0 1 1 0 1 0] [1 1 1 0 0 1]] Este código gera uma matriz BoW, onde cada linha representa um documento e cada coluna representa uma palavra única do vocabulário. 4.2 Term Frequency-Inverse Document Frequency (TF-IDF) O TF-IDF é uma técnica de vetorização que leva em consideração a frequência das palavras em um documento em relação a um corpus de documentos. Ele pondera as palavras de acordo com sua importância relativa, diminuindo o peso das palavras comuns e aumentando o peso das palavras menos frequentes. Em termos simples, o TF-IDF nos ajuda a entender quais palavras são mais relevantes e distintivas em um texto específico, comparando-o com outros textos. Utilizando essa técnica, é possível extrair uma métrica estatística para avaliar a importância de uma palavra em um documento em relação a um conjunto de documentos. permitindo assim analisar e entender o conteúdo de grandes volumes de texto. TF (Term Frequency): Mede a frequência com que uma palavra aparece em um documento. Quanto mais vezes uma palavra aparece, maior é seu TF. IDF (Inverse Document Frequency): Mede a raridade de uma palavra em um conjunto de documentos. Palavras que aparecem em muitos documentos têm um IDF baixo, enquanto palavras que aparecem em poucos documentos têm um IDF alto. O valor TF-IDF é o produto do TF e do IDF: TF-IDF(t, d) = TF(t, d) * IDF(t) Onde: ‘t’ é um termo (palavra) ‘d’ é um documento Por exemplo, imagine um conjunto de documentos sobre carros. A palavra \"carro\" provavelmente terá um TF alto em todos os documentos, mas um IDF baixo, pois é muito comum. Já a palavra \"supercarro\" pode ter um TF baixo em muitos documentos, mas um IDF alto, pois é menos comum. O TF-IDF da palavra \"supercarro\" será maior do que o da palavra \"carro\", indicando que \"supercarro\" é mais distintiva e relevante para identificar documentos sobre carros esportivos. Alguns exemplos do uso de TF-IDF: recuperação de informação, para ranquear documentos em resposta a uma consulta de pesquisa; mineração de texto, para identificar tópicos e padrões em grandes conjuntos de documentos. E sistemas de recomendação, para sugerir itens relevantes aos usuários. Ao combinar informações sobre a frequência de palavras em um documento e sua raridade em um conjunto de documentos, o TF-IDF nos permite identificar as palavras mais importantes e relevantes para um determinado contexto. 4.2.1 Exemplo em Python: TF-IDF from sklearn.feature_extraction.text import TfidfVectorizer # Exemplo de documentos documentos = [ &quot;existe gato laranja&quot;, &quot;existe gato preto&quot;, &quot;também existe gato branco&quot; ] # Criação do modelo TF-IDF tfidf_vectorizer = TfidfVectorizer() X_tfidf = tfidf_vectorizer.fit_transform(documentos) # Exibição da matriz TF-IDF print(&quot;Vetor de características:\\n&quot;, tfidf_vectorizer.get_feature_names_out()) print(&quot;Matriz TF-IDF:\\n&quot;, X_tfidf.toarray()) Vetor de características: [&#39;branco&#39; &#39;existe&#39; &#39;gato&#39; &#39;laranja&#39; &#39;preto&#39; &#39;também&#39;] Matriz TF-IDF: [[0. 0.45329466 0.45329466 0.76749457 0. 0. ] [0. 0.45329466 0.45329466 0. 0.76749457 0. ] [0.6088451 0.35959372 0.35959372 0. 0. 0.6088451 ]] Este código gera uma matriz TF-IDF, onde cada valor representa a importância de uma palavra em um documento em relação ao corpus. 4.3 Word Embeddings Os Word Embeddings são representações densas de palavras em um espaço vetorial, que capturam as relações semânticas entre as palavras. Técnicas como Word2Vec, GloVe e FastText são amplamente utilizadas para gerar embeddings que refletem o contexto em que as palavras aparecem. 4.3.1 Exemplo em Python: Word2Vec from gensim.models import Word2Vec # Exemplo de corpus corpus = [ [&quot;existe&quot;, &quot;gato&quot;, &quot;laranja&quot;], [&quot;existe&quot;, &quot;gato&quot;, &quot;preto&quot;], [&quot;também&quot;, &quot;existe&quot;, &quot;gato&quot;, &quot;branco&quot;] ] # Treinamento do modelo Word2Vec model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4) # Obtenção do embedding para uma palavra print(&quot;Embedding para &#39;gato&#39;:\\n&quot;, model.wv[&quot;gato&quot;]) # Similaridade entre palavras print(&quot;Similaridade entre &#39;gato&#39; e &#39;laranja&#39;:&quot;, model.wv. similarity(&quot;gato&quot;, &quot;laranja&quot;)) Embedding para &#39;gato&#39;: [-5.3622725e-04 2.3643136e-04 5.1033497e-03 9.0092728e-03 -9.3029495e-03 -7.1168090e-03 6.4588725e-03 8.9729885e-03 -5.0154282e-03 -3.7633716e-03 7.3805046e-03 -1.5334714e-03 -4.5366134e-03 6.5540518e-03 -4.8601604e-03 -1.8160177e-03 2.8765798e-03 9.9187379e-04 -8.2852151e-03 -9.4488179e-03 7.3117660e-03 5.0702621e-03 6.7576934e-03 7.6286553e-04 6.3508903e-03 -3.4053659e-03 -9.4640139e-04 5.7685734e-03 -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04 9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03 8.0774371e-03 -5.9308959e-03 4.5162440e-05 -4.7537340e-03 -9.6035507e-03 5.0072931e-03 -8.7595852e-03 -4.3918253e-03 -3.5099984e-05 -2.9618145e-04 -7.6612402e-03 9.6147433e-03 4.9820580e-03 9.2331432e-03 -8.1579173e-03 4.4957981e-03 -4.1370760e-03 8.2453608e-04 8.4986202e-03 -4.4621765e-03 4.5175003e-03 -6.7869602e-03 -3.5484887e-03 9.3985079e-03 -1.5776526e-03 3.2137157e-04 -4.1406299e-03 -7.6826881e-03 -1.5080082e-03 2.4697948e-03 -8.8802696e-04 5.5336617e-03 -2.7429771e-03 2.2600652e-03 5.4557943e-03 8.3459532e-03 -1.4537406e-03 -9.2081428e-03 4.3705525e-03 5.7178497e-04 7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03 -8.5655687e-04 2.8265631e-03 5.4014288e-03 7.0526563e-03 -5.7031214e-03 1.8588197e-03 6.0888636e-03 -4.7980510e-03 -3.1072604e-03 6.7976294e-03 1.6314756e-03 1.8991709e-04 3.4736372e-03 2.1777749e-04 9.6188262e-03 5.0606038e-03 -8.9173904e-03 -7.0415605e-03 9.0145587e-04 6.3925339e-03] Similaridade entre &#39;gato&#39; e &#39;laranja&#39;: -0.05987629 Este exemplo treina um modelo Word2Vec e calcula a similaridade entre os vetores de duas palavras. Em resumo, a vetorização de texto é um passo crucial na preparação de dados para modelos de aprendizado de máquina. Técnicas como BoW, TF-IDF e Word Embeddings fornecem maneiras eficazes de transformar texto em uma forma que possa ser utilizada para diversas tarefas de processamento de linguagem natural. Exercícios Versão on-line destes exercícios https://forms.gle/euESDhq7hYWoCSGSA Qual é a principal característica do modelo Bag of Words (BoW)? Considera a ordem das palavras no texto. Ignora a ordem das palavras, mas mantém a contagem de frequência das palavras. Gera representações vetoriais densas das palavras. Utiliza embeddings contextuais para representar palavras. O que o termo TF-IDF representa? Transform Frequency-Inverse Document Frequency. Term Frequency-Inverse Document Frequency. Term Frequency-Indexed Document Frequency. Transform Frequency-Indexed Document Frequency. Qual é uma vantagem dos Word Embeddings em relação ao modelo Bag of Words? Word Embeddings capturam a ordem das palavras no texto. Word Embeddings geram vetores esparsos de alta dimensionalidade. Word Embeddings capturam relações semânticas entre palavras. Word Embeddings ignoram a frequência das palavras no texto. Qual biblioteca Python tem uma implementação do TF-IDF? NumPy. Pandas. Scikit-learn. RE. Qual técnica é utilizada para criar representações vetoriais densas que capturam o significado semântico das palavras? Bag of Words. TF-IDF. Word Embeddings. N-grams. "],["modelos-avançados-de-vetorização.html", "Capítulo 5 Modelos Avançados de Vetorização 5.1 Embeddings Contextuais 5.2 Análise de Sentimento com Embeddings 5.3 Redução de Dimensionalidade Exercício", " Capítulo 5 Modelos Avançados de Vetorização Neste capítulo, vamos explorar modelos avançados de vetorização de texto, que vão além das técnicas tradicionais como Bag of Words e TF-IDF. Vamos discutir embeddings contextuais, como o BERT e o GPT, e a aplicação de redução de dimensionalidade usando técnicas como PCA e t-SNE. A seguir, forneceremos exemplos práticos em Python para cada um desses métodos. 5.1 Embeddings Contextuais Embeddings contextuais são vetores que capturam o significado de uma palavra com base no contexto em que ela aparece. Diferente de embeddings como Word2Vec e GloVe, que geram uma única representação para cada palavra, modelos como BERT (Bidirectional Encoder Representations from Transformers) e GPT (Generative Pretrained Transformer) produzem diferentes embeddings para a mesma palavra, dependendo do seu contexto. 5.1.1 Exemplo em Python: Usando BERT para Vetorização from transformers import BertTokenizer, BertModel import torch # Carregando o tokenizer e o modelo BERT tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;) # Exemplo de texto texto = &quot;O gato está sentado no tapete.&quot; # Tokenização do texto e conversão para tensores inputs = tokenizer(texto, return_tensors=&quot;pt&quot;) # Obtenção dos embeddings a partir do modelo BERT with torch.no_grad(): outputs = model(**inputs) # Extraindo os embeddings da última camada oculta embeddings = outputs.last_hidden_state print(&quot;Embeddings para cada token:&quot;, embeddings) Embeddings para cada token: tensor([[[-8.4815e-01, -3.2018e-01, -6.1327e-02, ..., -3.2507e-01, 7.6626e-02, 9.1921e-01], [-1.1552e+00, -8.0612e-02, -3.9960e-01, ..., -1.1649e-01, 9.8960e-02, 1.0148e+00], [ 4.3945e-01, -8.0826e-01, 2.9775e-01, ..., 3.0753e-01, 1.6638e-01, 1.4990e+00], ..., [-5.2439e-01, -4.2932e-01, -6.0938e-01, ..., -4.7871e-01, -3.9740e-01, 6.2908e-01], [-4.5428e-02, -6.9424e-01, 1.2010e-02, ..., 5.4892e-01, -9.5592e-04, -1.7835e-01], [ 6.6603e-01, -7.9354e-02, 1.1458e-02, ..., -4.0039e-02, -7.0081e-01, 2.2797e-02]]]) Este exemplo mostra como usar o BERT para gerar embeddings contextuais. O modelo BERT é capaz de gerar um vetor de embeddings para cada token no texto, considerando o contexto de toda a frase. 5.2 Análise de Sentimento com Embeddings Modelos de embeddings contextuais, como o BERT, podem ser utilizados em tarefas de análise de sentimento, fornecendo representações mais ricas e precisas das palavras. A seguir, aplicamos BERT em uma tarefa de classificação de sentimento. 5.2.1 Exemplo em Python: Classificação de Sentimento com BERT from transformers import BertTokenizer, BertForSequenceClassification from torch.nn.functional import softmax # Carregando o tokenizer e o modelo BERT para classificação de sequência tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) model = BertForSequenceClassification.from_pretrained(&#39;bert-base-uncased&#39;, num_labels=2) # Exemplo de texto texto = &quot;Eu adoro gatos, eles são fofos&quot; # Tokenização do texto e conversão para tensores inputs = tokenizer(texto, return_tensors=&#39;pt&#39;) # Fazendo a previsão with torch.no_grad(): outputs = model(**inputs) # Aplicando softmax para obter as probabilidades das classes probabilidades = softmax(outputs.logits, dim=1) print(&quot;Probabilidade de sentimento positivo:&quot;, probabilidades[0][1].item()) Probabilidade de sentimento positivo: 0.6633508801460266 Este código ilustra como utilizar o BERT para classificar o sentimento de um texto. Aqui, o modelo é capaz de prever a probabilidade de um sentimento positivo ou negativo para a frase fornecida. 5.3 Redução de Dimensionalidade Vetores de alta dimensionalidade podem ser difíceis de manipular e visualizar. Técnicas como Análise de Componentes Principais (PCA) e t-SNE (t-Distributed Stochastic Neighbor Embedding) são comumente usadas para reduzir a dimensionalidade dos dados de forma a manter as informações mais importantes. 5.3.1 Exemplo em Python: Redução de Dimensionalidade com PCA from sklearn.decomposition import PCA import matplotlib.pyplot as plt from transformers import BertTokenizer, BertModel import torch # Carregando o tokenizer e o modelo BERT tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;) # Exemplo de texto texto = &quot;Eu adoro gatos, eles são fofos&quot; # Tokenização do texto e conversão para tensores inputs = tokenizer(texto, return_tensors=&quot;pt&quot;) # Obtenção dos embeddings a partir do modelo BERT with torch.no_grad(): outputs = model(**inputs) # Extraindo os embeddings da última camada oculta embeddings = outputs.last_hidden_state # Exemplo de vetores de alta dimensionalidade (usaremos os embeddings do BERT) vetores = embeddings.squeeze().numpy() # Convertendo de tensor para numpy array # Aplicando PCA para reduzir para 2 dimensões pca = PCA(n_components=2) vetores_reduzidos = pca.fit_transform(vetores) # Plotando os vetores em 2D plt.scatter(vetores_reduzidos[:, 0], vetores_reduzidos[:, 1]) plt.title(&quot;Vetores de palavras reduzidos para 2D usando PCA&quot;) plt.show() image Neste exemplo, utilizamos PCA para reduzir os embeddings gerados pelo BERT para duas dimensões, facilitando a visualização. 5.3.2 Exemplo em Python: Redução de Dimensionalidade com t-SNE from sklearn.manifold import TSNE import matplotlib.pyplot as plt from transformers import BertTokenizer, BertModel import torch # Carregando o tokenizer e o modelo BERT tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;) model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;) # Exemplo de texto texto = &quot;Eu adoro gatos, eles são fofos&quot; # Tokenização do texto e conversão para tensores inputs = tokenizer(texto, return_tensors=&quot;pt&quot;) # Obtenção dos embeddings a partir do modelo BERT with torch.no_grad(): outputs = model(**inputs) # Extraindo os embeddings da última camada oculta embeddings = outputs.last_hidden_state vetores = embeddings.squeeze().numpy() # Convertendo de tensor para numpy array # Aplicando t-SNE para reduzir para 2 dimensões tsne = TSNE(n_components=2, perplexity=2) vetores_reduzidos_tsne = tsne.fit_transform(vetores) # Plotando os vetores em 2D plt.scatter(vetores_reduzidos_tsne[:, 0], vetores_reduzidos_tsne[:, 1]) plt.title(&quot;Vetores de palavras reduzidos para 2D usando t-SNE&quot;) plt.show() image Aqui, usamos t-SNE para reduzir os embeddings para duas dimensões. O t-SNE é especialmente útil para a visualização de dados em espaços de alta dimensionalidade. Em resumo, exploramos modelos avançados de vetorização de texto, incluindo embeddings contextuais com BERT e GPT, e técnicas de redução de dimensionalidade como PCA e t-SNE. Esses métodos fornecem ferramentas poderosas para capturar e visualizar informações complexas em dados textuais. Exercício Versão on-line destes exercícios https://forms.gle/hh4BgCZVhJLMyFiy6 Qual é a principal vantagem dos embeddings contextuais, como BERT, em comparação com embeddings tradicionais como Word2Vec? Embeddings contextuais capturam o significado das palavras com base em seu contexto específico. Embeddings contextuais são sempre mais rápidos de treinar. Embeddings contextuais geram representações esparsas das palavras. Embeddings contextuais são menos precisos do que os embeddings tradicionais. O que a técnica de PCA (Análise de Componentes Principais) realiza em dados de alta dimensionalidade? Aumenta o número de dimensões nos dados. Reduz a dimensionalidade dos dados mantendo a maior variabilidade possível. Gera novas características que não são correlacionadas. Elimina completamente a variabilidade dos dados. Qual é a função principal da técnica t-SNE? Agrupar dados de alta dimensionalidade. Visualizar dados de alta dimensionalidade em espaços de menor dimensão. Normalizar dados textuais. Criar novas features a partir dos dados originais. Por que técnicas de redução de dimensionalidade, como PCA, são úteis em vetorização de texto? Elas aumentam a precisão dos modelos de aprendizado de máquina. Elas eliminam a necessidade de embeddings contextuais. Elas reduzem a quantidade de dados de entrada para tornar o processamento mais eficiente e visualizável. Elas criam novas features para melhorar o desempenho de modelos de deep learning. Em qual cenário a redução de dimensionalidade é particularmente útil? Quando se deseja aumentar a complexidade do modelo. Quando os dados têm poucas features e baixa variabilidade. Quando os dados têm muitas dimensões e se deseja melhorar a visualização ou performance do modelo. Quando se deseja eliminar o ruído dos dados, independentemente da dimensionalidade. "],["aplicações-práticas-1.html", "Capítulo 6 Aplicações Práticas 6.1 Classificação de Texto: Análise de Sentimento 6.2 Agrupamento de Documentos 6.3 Detecção de Tópicos Exercícios", " Capítulo 6 Aplicações Práticas Neste capítulo, exploraremos aplicações práticas das técnicas de vetorização de texto que discutimos nos capítulos anteriores. Vamos focar em três áreas principais: classificação de texto, agrupamento de documentos e detecção de tópicos. Cada seção incluirá exemplos práticos em Python. 6.1 Classificação de Texto: Análise de Sentimento A classificação de texto é uma das aplicações mais comuns da vetorização de texto. Milhares de pessoas visitam as mídias sociais para expressar seus sentimentos, principalmente no antigo Twitter, hoje X, (Recuero and Zago 2016). Essas mídias reúnem características que viabilizam a mineração de texto tais como: mensagens textuais, de perfil público e coleta automatizada (Hootsuite 2018). A enorme quantidade de dados gerados por essas mídias e as opiniões e sentimentos expressos podem ser analisadas por empresas para diversos fins, sendo possível capturar os sentimentos, opiniões e críticas sobre os produtos em tempo real, proporcionando à empresa uma nova forma de entendimento sobre o comportamento do consumidor (Nascimento Junior 2012). Algumas questões podem ser respondidas através da análise dos milhares de comentários e respostas expressos nessas mídias. Existem evidências que essas postagens são utilizadas na tomada de decisão e que podem influenciar eventos acontecendo em tempo real (Baldykowski et al. 2018; Shamma, Kennedy, and Churchill 2009). As mídias sociais também vêm sendo utilizadas para inferências e predições em vários setores, por exemplo: já foram encontradas relações entre o sentimento impresso nas postagens dessas mídias e os movimentos do preço do Bitcoin (Santos 2019; Nakamoto 2008); já foram utilizadas para prever quais seriam as palavras-chave (hashtags) mais comentadas no futuro (Das et al. 2015); para inferir a quantidade de pessoas que irão assistir ao filme (Teixeira and Azevedo 2011); até para correlacionar com os indicados ao Oscar (Tannús Corrêa 2017); para influenciar a visão do profissional no ambiente de trabalho, pois, quando os profissionais utilizam as mídias sociais nas relações profissionais e adotam um tipo de comportamentos de gerenciamento de público e de conteúdo, dados apontam uma tendência do profissional ser mais respeitados por seus colegas de trabalho (Ferretti and Araujo 2017). As mídias sociais também fornecem aos usuários da Internet uma maneira fácil e barata de se envolver em discussões políticas e promover pontos de vista e interesses. Essas postagens também podem auxiliar no processo de decisão de instituições públicas (Georgiadou, Angelopoulos, and Drake 2020). Alguns candidatos utilizam as mídias sociais para fazer propaganda e para aumentar a intenção de voto dos seus eleitores, além disso, foram encontradas evidências de que essa intenção é influenciada pela opinião do eleitor, confiança e imagem do candidato (Almeida and Oliveira 2018). O uso da mídia social também é utilizado para divulgar notícias falsas, e existem evidências de que a exposição a essas notícias pode influenciar os resultados das eleições. Nas eleições presidenciais dos EUA em 2016, notícias falsas postadas em mídias sociais foram cruciais na eleição do presidente Trump (Allcott and Gentzkow 2017). Métodos automáticos de extração de sentimentos, também chamado de mineração de opinião, já foram utilizados para avaliar o desempenho das ações na bolsa (Chen and David Zimbra 2010). O apoio a decisão baseado em sentimentos expresso em mídias sociais tem potencial para beneficiar e diminuir os custos organizacionais, mas ainda é preciso entender como utilizá-la e qual o seu impacto (Saxton and Guo 2020). Usaremos a técnica TF-IDF para vetorizar documentos e, em seguida, aplicaremos um classificador de Naive Bayes para categorizar o texto e analisar o sentimento de algumas postagens, como exemplo. 6.1.1 Exemplo em Python: Classificação de Texto com TF-IDF e Naive Bayes from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split from sklearn import metrics # Exemplo de corpus e rótulos documentos = [ &quot;Este é um ótimo produto&quot;, &quot;Muito ruim, não gostei&quot;, &quot;Fantástico, recomendo!&quot;, &quot;Não vale a pena, péssimo&quot;, &quot;Excelente qualidade, compraria de novo&quot;, &quot;Horrível, joguei meu dinheiro fora&quot; ] rótulos = [&quot;positivo&quot;, &quot;negativo&quot;, &quot;positivo&quot;, &quot;negativo&quot;, &quot;positivo&quot;, &quot;negativo&quot;] # Dividindo os dados em treino e teste X_train, X_test, y_train, y_test = train_test_split(documentos, rótulos, test_size=0.33, random_state=42) # Criando o pipeline modelo = make_pipeline(TfidfVectorizer(), MultinomialNB()) # Treinando o modelo modelo.fit(X_train, y_train) # Fazendo previsões predições = modelo.predict(X_test) # Avaliando o modelo print(metrics.classification_report(y_test, predições)) precision recall f1-score support negativo 0.50 1.00 0.67 1 positivo 0.00 0.00 0.00 1 accuracy 0.50 2 macro avg 0.25 0.50 0.33 2 weighted avg 0.25 0.50 0.33 2 Este exemplo demonstra como classificar textos usando TF-IDF e Naive Bayes. Após treinar o modelo, as previsões são feitas no conjunto de teste e uma avaliação do desempenho é realizada. 6.2 Agrupamento de Documentos O agrupamento de documentos é outra aplicação importante da vetorização de texto, permitindo que documentos similares sejam automaticamente agrupados. Nesta seção, utilizaremos a técnica de k-means clustering. 6.2.1 Exemplo em Python: Agrupamento com k-means from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans # Exemplo de documentos documentos = [ &quot;O gato gosta de peixe&quot;, &quot;O cachorro gosta de osso&quot;, &quot;O peixe não gosta de gato&quot;, &quot;O gato e o cachorro são amigos&quot;, &quot;O peixe vive na água&quot;, &quot;Os cães gostam de ossos&quot; ] # Vetorizando os documentos com TF-IDF vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(documentos) # Aplicando k-means clustering kmeans = KMeans(n_clusters=2, random_state=42) kmeans.fit(X) # Mostrando os rótulos dos clusters print(&quot;Rótulos dos Clusters:&quot;, kmeans.labels_) Rótulos dos Clusters: [0 0 0 0 0 1] Este código realiza o agrupamento dos documentos em dois clusters usando k-means. Cada documento é atribuído a um cluster com base na similaridade de seu conteúdo. 6.3 Detecção de Tópicos A detecção de tópicos permite identificar os principais assuntos discutidos em um conjunto de documentos. Uma técnica popular para isso é a Latent Dirichlet Allocation (LDA), que vamos explorar nesta seção. 6.3.1 Exemplo em Python: Detecção de Tópicos com LDA from sklearn.decomposition import LatentDirichletAllocation from sklearn.feature_extraction.text import CountVectorizer # Exemplo de documentos documentos = [ &quot;O gato gosta de peixe&quot;, &quot;O cachorro gosta de osso&quot;, &quot;O peixe não gosta de gato&quot;, &quot;O gato e o cachorro são amigos&quot;, &quot;O peixe vive na água&quot;, &quot;Os cães gostam de ossos&quot; ] # Vetorizando os documentos com CountVectorizer vectorizer = CountVectorizer() X = vectorizer.fit_transform(documentos) # Aplicando LDA lda = LatentDirichletAllocation(n_components=2, random_state=42) lda.fit(X) # Mostrando os tópicos tópicos = lda.components_ nomes_palavras = vectorizer.get_feature_names_out() for idx, tópico in enumerate(tópicos): print(f&quot;Tópico {idx + 1}:&quot;) palavras = [nomes_palavras[i] for i in tópico.argsort()[:-5 - 1:-1]] print(&quot; &quot;.join(palavras)) Tópico 1: gato peixe gosta de cachorro Tópico 2: de ossos os gostam cães Este exemplo demonstra como usar LDA para identificar tópicos em um conjunto de documentos. As palavras mais representativas para cada tópico são exibidas. Emresumo, as técnicas de vetorização de texto discutidas anteriormente têm uma ampla gama de aplicações práticas, desde a classificação e agrupamento de documentos até a detecção de tópicos. Essas aplicações ilustram a importância de transformar texto em representações numéricas para a análise eficiente de dados textuais. Exercícios Versão on-line destes exercícios https://forms.gle/FopDZjLXRVixmGW46 Qual é o principal objetivo da classificação de texto? Gerar texto automaticamente. Classificar textos em categorias predefinidas. Traduzir textos entre diferentes idiomas. Aumentar o tamanho do corpus de dados textuais. Qual técnica de vetorização é comumente usada em combinação para classificação de texto? Dinâmica de Sistemas. TF-IDF ou BoW. Redes de Petri. Automação. O que o algoritmo de k-means clustering faz em um conjunto de documentos? Agrupa documentos semelhantes em clusters. Classifica documentos em categorias predefinidas. Reduz a dimensionalidade dos vetores de documentos. Gera novos documentos a partir de um conjunto de treinamento. Qual técnica é utilizada para identificar os principais tópicos em um conjunto de documentos? Classificação de texto. Redução de dimensionalidade. Modelagem de tópicos com LDA. Tokenização. Em um modelo de classificação de sentimentos, qual é o propósito de utilizar TF-IDF na vetorização de texto? Melhorar a visualização dos dados. Reduzir o número de palavras no texto. Ponderar a importância das palavras com base na frequência no documento e no corpus. Gerar novos tópicos a partir dos documentos. References Allcott, Hunt, and Matthew Gentzkow. 2017. “Social media and fake news in the 2016 election.” Journal of Economic Perspectives 31 (2): 211–36. https://doi.org/10.1257/jep.31.2.211. Almeida, Cláudio Márcio, and Marcelo de Oliveira. 2018. O Impacto da Mídia Social na Intenção de Voto do Eleitor. 1st ed. Vitória: Independently Published. https://www.amazon.com.br/Impacto-M-CL-Almeida/dp/1720231613. Baldykowski, Ariane Lao Saulo Alves de Brito, Sandro A. Miczevski, and Thiago H. Silva. 2018. “Cheers to untappd! preferences for beer reflect cultural differences around the world.” Americas Conference on Information Systems 2018: Digital Disruption, AMCIS 2018, no. July. Chen, Hsinchun, and David Zimbra. 2010. “AI and opinion mining.” IEEE Intelligent Systems 25 (4): 72–79. https://doi.org/10.1109/MIS.2010.94. Das, Anubrata, Moumita Roy, Soumi Dutta, Saptarshi Ghosh, and Asit Kumar Das. 2015. “Predicting Trends in the Twitter Social Network: A Machine Learning Approach.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8947: 432–44. https://doi.org/10.1007/978-3-319-20294-5. Ferretti, Amanda Soares Zambelli, and Bruno Felix Von Borell de Araujo. 2017. “Comportamentos nas Redes Sociais Online e seus Impactos nas Relações Profissionais.” Revista Administração Em Diálogo - RAD 19 (2): 91. https://doi.org/10.20946/rad.v19i2.31143. Georgiadou, Elena, Spyros Angelopoulos, and Helen Drake. 2020. “Big data analytics and international negotiations: Sentiment analysis of Brexit negotiating outcomes.” International Journal of Information Management 51 (October 2019): 102048. https://doi.org/10.1016/j.ijinfomgt.2019.102048. Hootsuite. 2018. “Most popular social networks worldwide as of January 2018, ranked by number of active users (in millions).” Nakamoto, Satoshi. 2008. “Bitcoin: A Peer-to-Peer Electronic Cash System.” Nascimento Junior, Aldomar. 2012. “Com a boca no twitter: a cocriação e a colaboração impactando no sucesso do marketing boca a boca on-line.” PhD thesis. Recuero, Raquel, and Gabriela Zago. 2016. “Em busca das redes que importam: redes sociais e capital social no Twitter,” 81–94. Santos, Weuler Borges Santos. 2019. “Prevendo o preço do Bitcoin com Redes Neurais usando dados do Twitter e de Mercado.” Saxton, Gregory D., and Chao Guo. 2020. “Social media capital: Conceptualizing the nature, acquisition, and expenditure of social media-based organizational resources.” International Journal of Accounting Information Systems 36: 100443. https://doi.org/10.1016/j.accinf.2019.100443. Shamma, David A., Lyndon Kennedy, and Elizabeth F. Churchill. 2009. “Tweet the debates: Understanding community annotation of uncollected sources.” 1st ACM SIGMM International Workshop on Social Media, WSM’09, Co-Located with the 2009 ACM International Conference on Multimedia, MM’09, 3–10. https://doi.org/10.1145/1631144.1631148. Tannús Corrêa, Igor. 2017. “Análise dos sentimentos expressos na rede social Twitter em relação aos filmes indicados ao Oscar 2017.” https://repositorio.ufu.br/bitstream/123456789/20133/1/AnaliseSentimentosExpressos.pdf. Teixeira, Diogo, and Isabel Azevedo. 2011. “Análise de opiniões expressas nas redes sociais.” RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao, no. 8: 53–65. https://doi.org/10.4304/risti.8.53-65. "],["estudo-de-caso.html", "Capítulo 7 Estudo de Caso 7.1 Análise de Reviews de Produtos 7.2 Processamento de Tweets 7.3 Análise de Notícias Exercício", " Capítulo 7 Estudo de Caso Neste capítulo, exploraremos alguns estudos de caso que demonstram a aplicação prática das técnicas de vetorização de texto discutidas anteriormente. Cada caso de estudo será acompanhado por exemplos em Python para ilustrar como essas técnicas podem ser implementadas em situações do mundo real. 7.1 Análise de Reviews de Produtos Uma aplicação comum da vetorização de texto é a análise de opiniões e reviews de produtos. Este caso de estudo examina como podemos utilizar TF-IDF e modelos de classificação para analisar reviews de produtos e determinar a polaridade dos sentimentos expressos. 7.1.1 Exemplo em Python: Análise de Sentimentos em Reviews de Produtos from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split from sklearn import metrics # Exemplo de reviews de produtos e rótulos reviews = [ &quot;Este produto é excelente, superou minhas expectativas!&quot;, &quot;Horrível, não funcionou como esperado&quot;, &quot;Muito bom, compraria novamente&quot;, &quot;Péssimo, nunca mais compro deste vendedor&quot;, &quot;Produto de ótima qualidade&quot;, &quot;Não gostei, material de baixa qualidade&quot; ] rótulos = [&#39;positivo&#39;, &#39;negativo&#39;, &#39;positivo&#39;, &#39;negativo&#39;, &#39;positivo&#39;, &#39;negativo&#39;] # Dividindo os dados em treino e teste X_train, X_test, y_train, y_test = train_test_split(reviews, rótulos, test_size=0.33, random_state=42) # Criando o pipeline TF-IDF + Naive Bayes modelo = make_pipeline(TfidfVectorizer(), MultinomialNB()) # Treinando o modelo modelo.fit(X_train, y_train) # Fazendo previsões predições = modelo.predict(X_test) # Avaliando o modelo print(metrics.classification_report(y_test, predições)) precision recall f1-score support negativo 1.00 1.00 1.00 1 positivo 1.00 1.00 1.00 1 accuracy 1.00 2 macro avg 1.00 1.00 1.00 2 weighted avg 1.00 1.00 1.00 2 Este exemplo mostra como aplicar TF-IDF e um classificador de Naive Bayes para analisar reviews de produtos, determinando se o sentimento expresso é positivo ou negativo. 7.2 Processamento de Tweets Analisar e processar tweets é uma tarefa desafiadora devido à natureza informal e limitada dos textos. Este caso de estudo se concentra na análise de sentimentos e na classificação de tweets, utilizando técnicas de vetorização e aprendizado de máquina. 7.2.1 Exemplo em Python: Análise de Sentimentos em Tweets import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import metrics # Exemplo de tweets e rótulos tweets = [ &quot;Adoro usar o novo recurso, muito útil!&quot;, &quot;Não gosto desta atualização, ficou pior&quot;, &quot;Essa nova versão está incrível&quot;, &quot;Detestei o que fizeram no app&quot;, &quot;Excelente trabalho, equipe!&quot;, &quot;Uma das piores atualizações até agora&quot; ] rótulos = [&#39;positivo&#39;, &#39;negativo&#39;, &#39;positivo&#39;, &#39;negativo&#39;, &#39;positivo&#39;, &#39;negativo&#39;] # Dividindo os dados em treino e teste X_train, X_test, y_train, y_test = train_test_split(tweets, rótulos, test_size=0.33, random_state=42) # Criando o pipeline TF-IDF + Regressão Logística modelo = make_pipeline(TfidfVectorizer(), LogisticRegression()) # Treinando o modelo modelo.fit(X_train, y_train) # Fazendo previsões predições = modelo.predict(X_test) # Avaliando o modelo print(metrics.classification_report(y_test, predições)) precision recall f1-score support negativo 0.00 0.00 0.00 1 positivo 0.50 1.00 0.67 1 accuracy 0.50 2 macro avg 0.25 0.50 0.33 2 weighted avg 0.25 0.50 0.33 2 Este exemplo ilustra como podemos utilizar TF-IDF e Regressão Logística para analisar tweets, classificando-os como positivos ou negativos com base no conteúdo. 7.3 Análise de Notícias A análise de notícias é fundamental para entender as tendências atuais e os tópicos de interesse público. Neste caso de estudo, utilizaremos técnicas de vetorização e modelagem de tópicos para identificar os principais temas abordados em um conjunto de artigos de notícias. 7.3.1 Exemplo em Python: Modelagem de Tópicos em Notícias from sklearn.decomposition import LatentDirichletAllocation from sklearn.feature_extraction.text import CountVectorizer # Exemplo de notícias noticias = [ &quot;O governo aprova novas reformas econômicas&quot;, &quot;A tecnologia 5G está mudando o cenário global&quot;, &quot;Novas descobertas na área da saúde são promissoras&quot;, &quot;A economia global enfrenta novos desafios&quot;, &quot;Tecnologias emergentes como IA estão em alta&quot;, &quot;Novas políticas ambientais são implementadas&quot; ] # Vetorizando as notícias com CountVectorizer vectorizer = CountVectorizer() X = vectorizer.fit_transform(noticias) # Aplicando LDA para modelagem de tópicos lda = LatentDirichletAllocation(n_components=2, random_state=42) lda.fit(X) # Mostrando os tópicos tópicos = lda.components_ nomes_palavras = vectorizer.get_feature_names_out() for idx, tópico in enumerate(tópicos): print(f&quot;Tópico {idx + 1}:&quot;) palavras = [nomes_palavras[i] for i in tópico.argsort()[:-5 - 1:-1]] print(&quot; &quot;.join(palavras)) Tópico 1: global estão alta como em Tópico 2: novas são promissoras área descobertas Neste exemplo, aplicamos LDA para identificar tópicos em um conjunto de artigos de notícias, mostrando as palavras mais representativas para cada tópico. Em resumo, os casos de estudo apresentados neste capítulo demonstram como as técnicas de vetorização de texto podem ser aplicadas a problemas reais, como análise de sentimentos em reviews e tweets, e modelagem de tópicos em notícias. Essas técnicas são ferramentas poderosas para extrair informações valiosas de grandes volumes de dados textuais. Exercício Versão on-line destes exercícios https://forms.gle/uuFPqkeQFtPREuQy5 Qual é o principal objetivo da análise de sentimentos? Aumentar o número de visualizações. Identificar e classificar sentimentos expressos no texto. Melhorar a qualidade dos produtos. Reduzir o tempo de leitura dos reviews. Qual é o principal desafio ao realizar análise de sentimentos? A falta de técnicas de vetorização adequadas. O número limitado de palavras únicas nos reviews. A ambiguidade no sentimento expresso em algumas frases. A necessidade de grandes volumes de dados de treinamento. Qual é o principal desafio ao processar tweets em comparação com outros tipos de textos? Tweets são sempre escritos de forma gramaticalmente correta. Tweets têm comprimento fixo e linguagem informal, o que pode dificultar a análise. Tweets não contêm informações úteis para análise de sentimentos. Tweets são muito longos e detalhados, o que dificulta a análise. Qual técnica é utilizada para identificar os principais temas em um conjunto de artigos de notícias? Classificação de Sentimentos. Análise de Regressão. Modelagem de Tópicos com LDA. Clustering com k-means. Qual é uma aplicação comum da modelagem de tópicos em notícias? Traduzir notícias para diferentes idiomas. Identificar e agrupar artigos com temas semelhantes. Prever o futuro de tendências de notícias. Criar resumos automáticos de notícias. "],["próximos-passos.html", "Capítulo 8 Próximos Passos 8.1 Futuras Direções no Campo de Vetorização de Texto 8.2 Leitura Recomendada 8.3 Próximos Passos", " Capítulo 8 Próximos Passos 8.1 Futuras Direções no Campo de Vetorização de Texto O campo da vetorização de texto e PLN está em constante evolução. A seguir, algumas direções futuras e tendências que merecem atenção: 8.1.1 Modelos de Linguagem de Grande Escala Modelos de linguagem de grande escala, como GPT-3 e modelos de nova geração, estão redefinindo o que é possível em PLN. Eles não só geram texto de alta qualidade, como também podem ser usados para uma variedade de tarefas de NLP, incluindo tradução automática, resumo de textos, e geração de código. 8.1.2 Multimodalidade A integração de dados textuais com outros tipos de dados, como imagens e áudio, está ganhando força. Modelos multimodais, como CLIP (Contrastive Language-Image Pretraining), permitem o entendimento e a geração de conteúdo que combina múltiplas formas de mídia. 8.1.3 Vetorização de Texto em Tempo Real Com a necessidade crescente de processamento em tempo real, técnicas para vetorização de texto que possam ser aplicadas instantaneamente a fluxos de dados contínuos estão se tornando cada vez mais relevantes. Aplicações como chatbots, assistentes virtuais e sistemas de recomendação em tempo real se beneficiam dessas abordagens. 8.2 Leitura Recomendada Para aqueles que desejam se aprofundar ainda mais no assunto, aqui estão algumas leituras recomendadas que cobrem uma gama de tópicos desde os fundamentos até as últimas inovações em vetorização de texto e PLN. Speech and Language Processing por Daniel Jurafsky e James H. Martin (Jurafsky and Martin 2009). Deep Learning por Ian Goodfellow, Yoshua Bengio e Aaron Courville (Goodfellow, Bengio, and Courville 2016). Introduction to Information Retrieval por Christopher D. Manning, Prabhakar Raghavan e Hinrich Schütze (Manning, Raghavan, and Schütze 2008). Natural Language Processing with Python por Steven Bird, Ewan Klein e Edward Loper (Bird, Klein, and Loper 2009). 8.3 Próximos Passos Com o conhecimento adquirido neste livro, você está agora equipado para aplicar técnicas de vetorização de texto em projetos de NLP. Alguns próximos passos sugeridos incluem: Desenvolver Aplicações Reais: Aplique as técnicas aprendidas para resolver problemas reais, como análise de sentimentos em redes sociais ou categorização automática de e-mails. Explorar Novos Modelos: Experimente modelos de linguagem mais recentes, como GPT-3 ou T5, para ver como eles se comparam aos métodos abordados aqui. Participar de Comunidades: Engaje-se com a comunidade de PLN e machine learning através de fóruns online, workshops, e competições de codificação, como as organizadas pelo Kaggle. Esperamos que este livro tenha servido como uma base para seu crescimento contínuo no campo de processamento de linguagem natural e vetorização de texto. References Bird, Steven, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc. Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. ———. 2009. Speech and Language Processing. Prentice Hall. Manning, Christopher D, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press. "],["gabarito-das-questões.html", "Gabarito das questões", " Gabarito das questões Respostas: 2 Questão Resposta 1 \\(B\\) 2 \\(C\\) 3 \\(B\\) 4 \\(B\\) 5 \\(C\\) Cap 1 Questão Resposta 1 \\(B\\) 2 \\(C\\) 3 \\(C\\) 4 \\(B\\) 5 \\(C\\) Cap 2 Questão Resposta 1 \\(B\\) 2 \\(A\\) 3 \\(B\\) 4 \\(B\\) 5 \\(C\\) Cap 3 Questão Resposta 1 \\(B\\) 2 \\(B\\) 3 \\(C\\) 4 \\(C\\) 5 \\(C\\) Cap 4 Questão Resposta 1 \\(A\\) 2 \\(B\\) 3 \\(B\\) 4 \\(C\\) 5 \\(C\\) Cap 5 Questão Resposta 1 \\(B\\) 2 \\(B\\) 3 \\(A\\) 4 \\(C\\) 5 \\(C\\) Cap 6 Questão Resposta 1 \\(B\\) 2 \\(C\\) 3 \\(B\\) 4 \\(C\\) 5 \\(B\\) Cap 7 "],["sobre-os-autores.html", "Sobre os autores", " Sobre os autores GISELDO NEO é Professor de Informática no Instituto Federal de Alagoas (IFAL) campus Viçosa, Doutorando em Ciência da Computação na Universidade Federal de Campina Grande, Mestre em Modelagem Computacional do Conhecimento na Universidade Federal de Alagoas, Mestre em Contabilidade (FUCAPE). Possui MBA em Gestão e Estratégia Empresarial (ESTÁCIO), Especialização em Arquitetura e Engenharia de Software (ESTÁCIO), MBA em Gestão de Projetos (UNINTER). Graduação em Análise e Desenvolvimento de Sistemas (ESTÁCIO), Graduação em Processos Gerenciais (UNINTER) e Técnico de Informática (Instituto Federal de Sergipe - IFS). ALANA NEO é Professora de Informática no Instituto Federal do Mato Grosso do Sul e atualmente desenvolve pesquisas na área de Informática na Educação. Doutoranda em Ciência da Computação na Universidade Federal de Campina Grande, Mestra em Modelagem Computacional do Conhecimento na Universidade Federal de Alagoas, Especialista em Estratégias Didáticas para a Educação Básica com Uso de TIC na Universidade Federal de Alagoas, Especialista em Desenvolvimento de Software, Especialista em Segurança da Informação, Graduada em Análise e Desenvolvimento de Sistemas e Bacharel em Sistemas de Informação pela Universidade Estácio de Sá e Licenciatura em Computação pelo Claretiano Centro Universitário. Allcott, Hunt, and Matthew Gentzkow. 2017. “Social media and fake news in the 2016 election.” Journal of Economic Perspectives 31 (2): 211–36. https://doi.org/10.1257/jep.31.2.211. Almeida, Cláudio Márcio, and Marcelo de Oliveira. 2018. O Impacto da Mídia Social na Intenção de Voto do Eleitor. 1st ed. Vitória: Independently Published. https://www.amazon.com.br/Impacto-M-CL-Almeida/dp/1720231613. Baldykowski, Ariane Lao Saulo Alves de Brito, Sandro A. Miczevski, and Thiago H. Silva. 2018. “Cheers to untappd! preferences for beer reflect cultural differences around the world.” Americas Conference on Information Systems 2018: Digital Disruption, AMCIS 2018, no. July. Bird, Steven, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc. Chen, Hsinchun, and David Zimbra. 2010. “AI and opinion mining.” IEEE Intelligent Systems 25 (4): 72–79. https://doi.org/10.1109/MIS.2010.94. Das, Anubrata, Moumita Roy, Soumi Dutta, Saptarshi Ghosh, and Asit Kumar Das. 2015. “Predicting Trends in the Twitter Social Network: A Machine Learning Approach.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8947: 432–44. https://doi.org/10.1007/978-3-319-20294-5. Ferretti, Amanda Soares Zambelli, and Bruno Felix Von Borell de Araujo. 2017. “Comportamentos nas Redes Sociais Online e seus Impactos nas Relações Profissionais.” Revista Administração Em Diálogo - RAD 19 (2): 91. https://doi.org/10.20946/rad.v19i2.31143. Georgiadou, Elena, Spyros Angelopoulos, and Helen Drake. 2020. “Big data analytics and international negotiations: Sentiment analysis of Brexit negotiating outcomes.” International Journal of Information Management 51 (October 2019): 102048. https://doi.org/10.1016/j.ijinfomgt.2019.102048. Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. Hootsuite. 2018. “Most popular social networks worldwide as of January 2018, ranked by number of active users (in millions).” Jurafsky, Daniel, and James H Martin. 2000. Speech and Language Processing. Prentice Hall. ———. 2009. Speech and Language Processing. Prentice Hall. Manning, Christopher D, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press. Nakamoto, Satoshi. 2008. “Bitcoin: A Peer-to-Peer Electronic Cash System.” Nascimento Junior, Aldomar. 2012. “Com a boca no twitter: a cocriação e a colaboração impactando no sucesso do marketing boca a boca on-line.” PhD thesis. Pak, Alexander, and Patrick Paroubek. 2010. “Twitter as a Corpus for Sentiment Analysis and Opinion Mining.” In Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10), 1320–26. European Language Resources Association (ELRA). Recuero, Raquel, and Gabriela Zago. 2016. “Em busca das redes que importam: redes sociais e capital social no Twitter,” 81–94. Santos, Weuler Borges Santos. 2019. “Prevendo o preço do Bitcoin com Redes Neurais usando dados do Twitter e de Mercado.” Saxton, Gregory D., and Chao Guo. 2020. “Social media capital: Conceptualizing the nature, acquisition, and expenditure of social media-based organizational resources.” International Journal of Accounting Information Systems 36: 100443. https://doi.org/10.1016/j.accinf.2019.100443. Shamma, David A., Lyndon Kennedy, and Elizabeth F. Churchill. 2009. “Tweet the debates: Understanding community annotation of uncollected sources.” 1st ACM SIGMM International Workshop on Social Media, WSM’09, Co-Located with the 2009 ACM International Conference on Multimedia, MM’09, 3–10. https://doi.org/10.1145/1631144.1631148. Tannús Corrêa, Igor. 2017. “Análise dos sentimentos expressos na rede social Twitter em relação aos filmes indicados ao Oscar 2017.” https://repositorio.ufu.br/bitstream/123456789/20133/1/AnaliseSentimentosExpressos.pdf. Teixeira, Diogo, and Isabel Azevedo. 2011. “Análise de opiniões expressas nas redes sociais.” RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao, no. 8: 53–65. https://doi.org/10.4304/risti.8.53-65. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
