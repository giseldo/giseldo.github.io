[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Construindo Chatbots",
    "section": "",
    "text": "Bem vindo",
    "crumbs": [
      "Bem vindo"
    ]
  },
  {
    "objectID": "sections/1_pref-cio-pref-cio-unnumbered.html",
    "href": "sections/1_pref-cio-pref-cio-unnumbered.html",
    "title": "Prefácio",
    "section": "",
    "text": "Vivemos um momento histórico em que a inteligência artificial (IA) deixou de ser apenas tema de ficção científica para se tornar parte do nosso cotidiano. Os chatbots, os avanços em Processamento de Linguagem Natural (PLN) e os Modelos de Linguagem Grande (LLMs) são hoje protagonistas dessa transformação, moldando a forma como nos comunicamos, aprendemos, trabalhamos e interagimos com a tecnologia.\nO que antes era limitado a sistemas rígidos, que apenas repetiam respostas pré-programadas, evoluiu para assistentes virtuais capazes de compreender contexto, interpretar nuances da linguagem e até gerar conteúdo inédito. Essa trajetória revela não apenas conquistas tecnológicas, porém também desafios éticos, sociais e econômicos.\nEste livro é um convite acessível e profundo para quem deseja compreender como nascem, evoluem e funcionam os chatbots — desde os primórdios com ELIZA até os modernos modelos de linguagem como o ChatGPT. Ele nasce com o propósito de explicar, refletir e inspirar. Explicar como o PLN e os LLMs funcionam e por que se tornaram tão relevantes. Refletir sobre os impactos dessa revolução: o futuro do trabalho, a responsabilidade no uso da IA, a relação entre humano e máquina. Inspirar pesquisadores, profissionais, estudantes e curiosos a explorar esse campo em rápida expansão, contribuindo para um uso responsável e criativo dessas ferramentas tecnológicas. Ele também é um convite para explorar esse universo fascinante. Você encontrará explicações sobre como essas tecnologias funcionam, exemplos e reflexões sobre seus impactos na sociedade. Mais do que compreender as máquinas que conversam, trata-se de entender o futuro da linguagem e da interação entre humanos e sistemas inteligentes.\nSeja você estudante, pesquisador ou profissional do mercado, este livro pode inspirar novas ideias e mostrar como a inteligência artificial conversacional já está transformando o presente — e continuará a transformar o futuro.\n\nBoa leitura!",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "sections/2_informa-es-informa-es-unnumbered.html",
    "href": "sections/2_informa-es-informa-es-unnumbered.html",
    "title": "Informações",
    "section": "",
    "text": "O código-fonte - com os mesmos códigos em python utilizados neste livro - está disponível no endereço:\n\nhttps://github.com/giseldo/chatbotLivroCodigoFonte.\n\nUma versão atualizada do livro pode ser encontrada no site\n\nhttps://giseldo.github.io/.\n\nCitação Formato ABNT:\n\nNeo, Giseldo da Silva & Neo Alana Viana Borges da Silva. Construindo Chatbots: do Artificial Inteligence Markup Language e PLN até os Modernos Large Language Models. 2026. Edição do kindle.\n\nCitação Bibtex:\n    @book{neoneo2026,\n        author = {Giseldo da Silva Neo & Alana da Silva Neo},\n        title = {Construindo Chatbots: do Artificial Inteligence Markup Language e PLN até os Modernos Large Language Models},\n        year = 2026,\n        publisher = {Edição do kindle}\n    }",
    "crumbs": [
      "Informações"
    ]
  },
  {
    "objectID": "sections/3_resumo-resumo-unnumbered.html",
    "href": "sections/3_resumo-resumo-unnumbered.html",
    "title": "Resumo",
    "section": "",
    "text": "Se você já se perguntou como funcionam os chatbots que conversam como humanos, este livro é a sua porta de entrada para um dos campos mais fascinantes da inteligência artificial. Ele é mais do que um guia técnico — é uma jornada envolvente pela evolução, construção e futuro dos agentes conversacionais, unindo clareza didática com profundidade técnica. A obra começa explorando o conceito de chatbot, diferenciando suas principais categorias — conversacionais e orientados a tarefas — e contextualizando o leitor com um panorama histórico que vai do lendário ELIZA, criado nos anos 1960, até os impressionantes modelos atuais como ChatGPT. Essa viagem no tempo é pontuada por explicações acessíveis e exemplos práticos em Python que mostram, passo a passo, como esses sistemas funcionam por trás das cortinas. O livro se destaca por traduzir conceitos complexos em linguagem clara. Você vai entender o que é o pattern matching usado por chatbots clássicos, como funciona o AIML, e por que expressões regulares ainda são ferramentas utilizadas no desenvolvimento de bots. Em capítulos dedicados ao Processamento de Linguagem Natural (PLN), técnicas como tokenização, lematização e análise sintática ganham vida por meio de exemplos aplicáveis, preparando o leitor para dar seus próprios passos no mundo da IA conversacional. Mas ele não se limita ao passado. Ele mergulha fundo nas tecnologias que revolucionaram o campo, como Transformers, Word2Vec e os Modelos de Linguagem Grande (LLMs), incluindo BERT, GPT e LLaMA. O leitor descobre como essas arquiteturas funcionam, como treiná-las e como aplicá-las usando frameworks modernos como Hugging Face e LangChain. Além de guiar a construção técnica de um chatbot — do código ao deploy — o livro também traz reflexões sobre ética, privacidade e o impacto social desses agentes. Tópicos como personalização, explicabilidade e segurança ampliam o olhar do leitor para além da implementação. Combinando teoria, prática e visão de futuro, ele é um convite para programadores, educadores, empreendedores e curiosos que desejam dominar a arte de construir diálogos entre humanos e máquinas. É uma obra que ensina, inspira e instiga. Se você quer entender como os chatbots realmente pensam — ou pelo menos como fingem tão bem — este livro é para você.",
    "crumbs": [
      "Resumo"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html",
    "href": "sections/4_introdu-o.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Contexto\nPara uma parcela da população um chatbot, estilo ChatGPT, é ferramenta essencial para executar tarefas, redigir e-mails e traduzir textos. O que um dia causou espanto e admiração hoje se converteu em dependência. Neste contexto, um chatbot é um programa de computador que simula uma conversa humana, via texto ou áudio, oferecendo respostas diretas a perguntas e apoiando diversas atividades, desde conversas gerais até ações específicas, como abrir uma conta bancária ou agendar um voo para o show do seu cantor preferido.\nPara entender como chegamos a esse ponto, vale recordar a trajetória do próprio conceito. Embora o programa ELIZA, criado por @Weizenbaum1996, seja frequentemente citado como um dos primeiros exemplos de software conversacional, o termo “chatbot” ainda não era empregado à época. Sua origem remete a “chatterbot” — sinônimo de “chatbot” —, popularizado por Michael Mauldin em 1994 ao descrever seu programa JULIA [@Mauldin1994]. Anos depois, outras publicações acadêmicas, como os anais da Virtual Worlds and Simulation Conference de 1998 [@Jacobstein1998], contribuíram para consolidar os termos entre os pesquisadores.\nO chatbot ELIZA representou um experimento marcante na interação entre humano e computador [@Weizenbaum1996]. Seu roteiro (ou script) mais famoso, o DOCTOR, imitava rudimentarmente um psicoterapeuta, utilizando correspondência de padrões simples. Por exemplo, quando um usuário inseria a frase “Estou triste” no ELIZA, o programa respondia “Por que você está triste hoje?”, reformulando a entrada do usuário como uma pergunta. O funcionamento básico do sistema baseava-se em um conjunto restrito de regras e substituições, o que lhe permitia apenas uma compreensão superficial e limitada da linguagem humana.\nO roteiro DOCTOR do ELIZA adequou-se bem a um tipo de diálogo mais simples, pois suas respostas dependiam de pouco conhecimento sobre o ambiente externo. As regras no roteiro permitiam que o programa respondesse ao usuário com outras perguntas ou simplesmente refletisse a afirmação original.\nOutro chatbot famoso é o ChatGPT da OpenAI. Ele é um programa capaz de gerar texto muito semelhante ao criado por humanos. Ele utiliza redes neurais, com aprendizagem profunda, para gerar sentenças e parágrafos com base nas entradas e informações fornecidas. Entre suas capacidades, ele pode traduzir e resumir textos, responder a perguntas e explicar conceitos. Contudo, o ChatGPT não possui consciência nem a capacidade de compreender contexto ou emoções.\nO chatGPT é um exemplo de Modelo de Linguagem Grande (em inglês Large Language Model - LLM), baseado na arquitetura Transformers, introduzida em 2017 [@vaswani2017attention]. Modelos deste tipo são treinados com terabytes de texto, utilizando mecanismos de autoatenção que avaliam a relevância de cada palavra em uma frase. Ao contrário das regras manuais do ELIZA, os LLMs extraem padrões linguísticos a partir da vasta quantidade de dados com que a rede neural foi treinada.\nEsses dois chatbots, ELIZA e ChatGPT, são bons representantes do tipo de chatbot conversacional. Apesar de terem surgido com décadas de diferença — ELIZA em 1966 e ChatGPT em 2022 — e de diferirem bastante na forma como geram suas respostas, ambos compartilham semelhanças em seu objetivo: conversar sobre determinado assunto ou responder perguntas, mantendo o usuário em um diálogo fluido quando necessário. Chatbots com essas características podem ser agrupados, de acordo com o objetivo, como chatbots conversacionais e são utilizados para interagir sobre assuntos gerais.\nOutro tipo de chatbot classificado em relação ao objetivo é o chatbot orientado a tarefas. Os chatbots orientados a tarefas executam ações específicas, como abrir uma conta bancária ou pedir uma pizza. Geralmente, as empresas disponibilizam chatbots orientados a tarefas para seus usuários, com regras de negócio embutidas na conversação e com fluxos bem definidos. Normalmente, não se espera pedir uma pizza e, no mesmo chatbot, discutir os estudos sobre Ética do filósofo Immanuel Kant (embora talvez haja quem queira).\nEssas duas classificações, “conversacional” e “orientado a tarefas”, ainda não são suficientes para uma completa classificação. Existem outras classificações que serão discutidas em seções posteriores. Além disso, uma abordagem híbrida, unindo funções de chatbots do tipo “conversacional” e “orientado a tarefas” vem sendo utilizada para atender às necessidades dos usuários.\nA popularidade dos chatbots tem crescido significativamente em diversos domínios de aplicação [@B2020; @Klopfenstein2017; @Sharma2020]. Essa tendência é corroborada pelo aumento do interesse de busca pelo termo “chatbots”, conforme análise de dados do Google Trends no período entre 2020 e 2025 (Figura 1.1). Nesta figura, os valores representam o interesse relativo de busca ao longo do tempo, onde 100 indica o pico de popularidade no período analisado e 0 (ou a ausência de dados) indica interesse mínimo ou dados insuficientes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#contexto",
    "href": "sections/4_introdu-o.html#contexto",
    "title": "1  Introdução",
    "section": "",
    "text": "NOTA\nUma descrição detalhada do funcionamento do ELIZA e o roteiro DOCTOR, com exemplos em Python, será apresentada no Capítulo 2.\n\n\n\n\n\n\n\n\n\n \n\n\nFigure 1: Evolução do interesse de busca pelo termo “chatbot” (Google Trends, 2020-2025). Fonte: Google Trends acesso em 05/04/2025",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#agentes",
    "href": "sections/4_introdu-o.html#agentes",
    "title": "1  Introdução",
    "section": "1.2 Agentes",
    "text": "1.2 Agentes\nOs termos “chatbots” e “agentes” são usadas indiscriminadamente, o que pode causar confusão. Vamos a uma definição mais precisa. Um chatbot é um programa computacional projetado para interagir com usuários por meio de linguagem natural. Por outro lado, o conceito de agente possui uma definição mais ampla. Um agente trata-se de uma entidade computacional que percebe seu ambiente por meio de sensores e atua sobre esse ambiente por meio de atuadores.\nNesse contexto, um chatbot (Figura 1.3) pode ser considerado uma instanciação específica de um agente - veja na Figura 1.2 a arquitetura conceitual de alto nível para um agente - cujo propósito primário é a interação conversacional em linguagem natural.\n\n\n \n\n\nFigure 2: Arquitetura conceitual de um agente. Fonte: Adaptado de (Russel and Norving 2013)\n\n\n\n\n \n\n\nFigure 3: Representação esquemática de um chatbot.\n\n\nCom o advento de modelos de linguagem, como os baseados na arquitetura Generative Pretrained Transformer (GPT), a exemplo do ChatGPT, observou-se uma recontextualização do termo “agente” no domínio dos sistemas conversacionais. Nessa abordagem mais recente, um sistema focado predominantemente na geração de texto conversacional tende a ser denominado “chatbot”. Em contraste, o termo “agente” é frequentemente reservado para sistemas que, além da capacidade conversacional, integram e utilizam ferramentas externas (por exemplo, acesso à Internet, execução de código e interação com APIs) para realizar tarefas complexas e interagir proativamente com o ambiente digital. Um sistema capaz de realizar uma compra online, processar um pagamento e confirmar um endereço de entrega por meio do navegador do usuário seria, portanto, classificado como um agente, diferentemente de chatbots mais simples como ELIZA, ou mesmo versões mais simples do chatGPT (GPT-2), cujo foco era estritamente o diálogo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#fluxo-conversacional",
    "href": "sections/4_introdu-o.html#fluxo-conversacional",
    "title": "1  Introdução",
    "section": "1.3 Fluxo Conversacional",
    "text": "1.3 Fluxo Conversacional\nUm chatbot responde a uma entrada do usuário. Porém, essa interação textual mediada por chatbots não se constitui em uma mera justaposição aleatória de turnos de conversação ou pares isolados de estímulo-resposta. Pelo contrário, espera-se que a conversação exiba coerência e mantenha relações lógicas e semânticas entre os turnos consecutivos. O estudo da estrutura e organização da conversa humana é abordado por disciplinas como a Análise da Conversação.\nNo contexto da análise da conversação em língua portuguesa, os trabalhos de Marcuschi [@Marchuschi1986] são relevantes ao investigar a organização dessa conversação. Marcuschi analisou a estrutura conversacional em termos de unidades coesas, como o “tópico conversacional”, que agrupa turnos relacionados a um mesmo assunto ou propósito interacional.\nConceitos oriundos da Análise da Conversação, como a gestão de tópicos, têm sido aplicados no desenvolvimento de chatbots para aprimorar sua capacidade de manter diálogos coerentes e contextualmente relevantes com usuários humanos [@Neves2005].\nNa prática de desenvolvimento de sistemas conversacionais, a estrutura lógica e sequencial da interação é frequentemente modelada e referida como “fluxo de conversação” ou “fluxo de diálogo”. Contudo, é importante ressaltar que a implementação explícita de modelos sofisticados de gerenciamento de diálogo, inspirados na Análise da Conversação, não é uma característica universal de todos os chatbots, variando conforme a complexidade e o propósito do sistema.\nUm exemplo esquemático de um fluxo conversacional é apresentado na Figura 1.4. Nesta figura, o fluxo de conversação inicia quando o usuário entra com o texto: I WANT PIZZA, o chatbot responde com uma pergunta: HI I AM THE PIZZA BOT. CAN I CONFIRM THIS DELIVERY TO YOUR HOUSE? O usuário então pode responder: YES, e o chatbot finaliza a conversa com: IT’S ON THE WAY. THANK YOU FOR CHOOSE OUR PRODUCT. Caso o usuário responda: NO, o chatbot responde com a pergunta original: HI I AM THE PIZZA BOT. CAN I CONFIRM THIS DELIVERY TO YOUR HOUSE? O fluxo de conversação continua até que o usuário responda com um “YES” para a pergunta inicial. Essa estrutura de perguntas e respostas é comum em chatbots orientados a tarefas, onde o objetivo é guiar o usuário por um processo específico, tal como fazer um pedido de pizza.\n\n\n \n\n\nFigure 4: Exemplo esquemático de um fluxo conversacional em um chatbot.\n\n\nUm outro tipo de fluxo para um chatbot que vende roupas online está representado na Figura 1.5.\n\n\n \n\n\nFigure 5: Representação de uma árvore de decisão para vender roupas online. Retirado de (Raj 2019).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#histórico",
    "href": "sections/4_introdu-o.html#histórico",
    "title": "1  Introdução",
    "section": "1.4 Histórico",
    "text": "1.4 Histórico\nUm marco significativo na evolução dos chatbots depois do ELIZA foi o ALICE, que introduziu a Artificial Intelligence Markup Language (AIML), uma linguagem de marcação baseada em XML [@Wallace2000]. A AIML estabeleceu um paradigma para a construção de agentes conversacionais ao empregar algoritmos de correspondência de padrões. Essa abordagem utiliza modelos pré-definidos para mapear as entradas do usuário a respostas correspondentes, permitindo a definição modular de blocos de conhecimento [@Wallace2000].\nNo contexto brasileiro, um dos primeiros chatbots documentados capaz de interagir em português, inspirado no modelo ELIZA, foi o Cybele [@primo2001chatterbot]. Posteriormente, foi desenvolvido o Elecktra, também em língua portuguesa, com aplicação voltada para a educação a distância [@Leonhardt2003]. Em 2019, o processo de inscrição para o Exame Nacional do Ensino Médio (ENEM) foi disponibilizado por meio de uma interface conversacional baseada em chatbot (Figura 1.6).\n\n\n \n\n\nFigure 6: Interface de chatbot para inscrição no ENEM 2019.\n\n\nO desenvolvimento de chatbots tem atraído investimentos de grandes corporações. Notavelmente, a IBM desenvolveu um sistema de resposta a perguntas em domínio aberto utilizando sua plataforma Watson [@Ferrucci2012]. Esse tipo de tarefa representa um desafio computacional e de inteligência artificial (IA) considerável. Em 2011, o sistema baseado em Watson demonstrou sua capacidade ao competir e vencer competidores humanos no programa de perguntas e respostas JEOPARDY! [@Ferrucci2012].\nDiversos outros chatbots foram desenvolvidos para atender a demandas específicas em variados domínios. Exemplos incluem: BUTI, um companheiro virtual com computação afetiva para auxiliar na manutenção da saúde cardiovascular [@Junior2008]; EduBot, um agente conversacional projetado para a criação e desenvolvimento de ontologias com lógica de descrição [@Lima2017]; PMKLE, um ambiente inteligente de aprendizado focado na educação em gerenciamento de projetos [@Torreao2005]; RENAN, um sistema de diálogo inteligente fundamentado em lógica de descrição [@AZEVEDO2015]; e MOrFEu, voltado para a mediação de atividades cooperativas em ambientes inteligentes na Web [@Bada2012].\nEntre os chatbots baseados em LLMs de destaque atualmente estão o Qwen https://chat.qwen.ai/, desenvolvido pela Alibaba, que se destaca por sua eficiência e suporte multilíngue; o DeepSeek https://chat.deepseek.com/, de código aberto voltado para pesquisa e aplicações empresariais com foco em precisão e escalabilidade; o Maritaca https://chat.maritaca.ai/, modelo brasileiro otimizado para o português; o Gemini https://gemini.google.com/, da Google, que integra capacidades multimodais e forte desempenho em tarefas diversas; o Mistralhttps://chat.mistral.ai/, da Mistral AI, que utiliza arquitetura de mistura de especialistas para maior eficiência; o Llama https://www.meta.ai/, da Meta, reconhecido por ser código aberto e ampla adoção na comunidade; o Claude https://claude.ai/, da Anthropic, projetado com ênfase em segurança e alinhamento ético, que vem ganhando adeptos para tarefas e codificação; e o Nemotron https://build.nvidia.com/models, da NVIDIA, que oferece modelos de linguagem otimizados para execução em GPUs e aplicações empresariais de alto desempenho. E o Groq https://grok.com/ da xAI.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#abordagens",
    "href": "sections/4_introdu-o.html#abordagens",
    "title": "1  Introdução",
    "section": "1.5 Abordagens",
    "text": "1.5 Abordagens\nDesde o pioneirismo do ELIZA, múltiplas abordagens e técnicas foram exploradas para o desenvolvimento de chatbots. Entre as mais relevantes, destacam-se: AIML com correspondência de padrões (pattern matching), análise sintática (Parsing), modelos de cadeia de Markov (Markov Chain Models), uso de ontologias, redes neurais recorrentes (RNNs), redes de memória de longo prazo (LSTMs), modelos neurais sequência-a-sequência (Sequence-to-Sequence), aprendizado adversarial para geração de diálogo, além de abordagens baseadas em recuperação (Retrieval-Based) e generativas (Generative-Based) [@borah2018survey; @Ramesh2019; @Shaikh2016; @Abdul-Kader2015; @Li2018], entre outras.\nA seguir, uma lista resumida das tecnologias e marcos da criação dos chatbots:\n\nELIZA: o primeiro chatbot, que utilizava correspondência de padrões simples para simular um psicoterapeuta. Foi um marco na história dos chatbots e influenciou o desenvolvimento de sistemas conversacionais subsequentes [@Weizenbaum1996].\nAIML: Artificial Intelligence Markup Language, uma linguagem de marcação baseada em XML que deu origem ao ALICE [@Wallace2000]. Essa linguagem de marcação permite a definição de regras de correspondência de padrões (pattern matching) para mapear entradas do usuário a respostas predefinidas.\nTransformers: arquitetura de rede neural baseada em atenção, que revolucionou o processamento de linguagem natural (NLP) [@vaswani2017attention]. Modelos como BERT e GPT são exemplos de arquiteturas baseadas em Transformer que têm sido amplamente utilizadas em chatbots modernos.\n\nAlém disso, diversos frameworks têm sido desenvolvidos para facilitar a criação desses agentes complexos, como CrewAI [@crewai2025] e Mangaba.AI [@mangabaAI2025] e plataformas diversas como Hugging Face (e.g., Transformers Agents), que fornecem abstrações e ferramentas em Python para orquestrar múltiplos componentes e o uso de ferramentas externas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#chatbot-pensa",
    "href": "sections/4_introdu-o.html#chatbot-pensa",
    "title": "1  Introdução",
    "section": "1.6 Chatbot pensa?",
    "text": "1.6 Chatbot pensa?\nApesar do progresso recente de chatbots, o mecanismo fundamental da inteligência em nível humano, frequentemente refletido na comunicação, ainda não está totalmente esclarecido [@Shum2018]. Para avançar na solução desses desafios, serão necessários progressos em diversas áreas da IA cognitiva, tais como: modelagem empática de conversas, modelagem de conhecimento e memória, inteligência de máquina interpretável e controlável, e calibração de recompensas emocionais [@Shum2018].\nUma das dificuldades na construção de chatbots do tipo orientado a tarefas - a exemplo do Artificial Intelligence Markup Language (AIML) usado no ALICE - reside em gerenciar a complexidade das estruturas condicionais (“se-então”) que definem o fluxo do diálogo [@Raj2019]. Quanto maior o número de decisões a serem tomadas, mais complexas tendem a ser essas estruturas condicionais. Contudo, elas são essenciais para codificar fluxos de conversação complexos. Se a tarefa que o chatbot visa simular é inerentemente complexa e envolve múltiplas condições, o código precisará refletir essa complexidade. Para facilitar a visualização desses fluxos, uma solução eficaz é a utilização de fluxogramas. Embora simples de criar e entender, os fluxogramas constituem uma ferramenta visual de representação para este problema.\n\nNOTA\nUma explicação detalhada do AIML será apresentada no Capítulo 2.\n\nOs chatbots baseados em AIML apresentam desvantagens específicas. Por exemplo, o conhecimento é representado como instâncias de arquivos AIML. Se esse conhecimento for criado com base em dados coletados da Internet, ele não será atualizado automaticamente, exigindo atualizações periódicas manuais [@Madhumitha2015]. No entanto, já existem abordagens para mitigar essa limitação, permitindo carregar conteúdo AIML a partir de fontes como arquivos XML [@Macedo2014], um corpus textual [@DeGasperis2013] ou dados do Twitter [@Yamaguchi2018]. Além de abordagens no-code que geram o AIML a partir de fluxogramas [@neo2023BPMN].\nOutra desvantagem do AIML, a exemplo do Eliza, reside na relativa complexidade de seus padrões de correspondência (patterns). Além disso, a manutenção do sistema pode ser árdua, pois, embora a inserção de conteúdo (categorias) seja conceitualmente simples, grandes volumes de informação frequentemente precisam ser adicionados manualmente [@Madhumitha2015].\nEspecificamente no caso do AIML, a construção e a visualização de fluxos de diálogo complexos enfrentam dificuldades adicionais. Devido ao seu formato baseado em texto, muitas vezes é difícil perceber claramente como as diferentes categorias (unidades de conhecimento e resposta) se interligam para formar a estrutura da conversação.\nO interesse pelos chatbots continua crescendo. No entanto, eles podem ser complicados para se construir e os usuários nem sempre têm experiência suficiente para configurá-los. Alguns usuários não têm necessariamente habilidades de programação ou de TI avançadas. Para que eles possam criar e personalizar os chatbots, é importante que a autoria seja fácil de usar e intuitiva. Ela não deve exigir conhecimento de linguagens de computador que sejam difíceis de entender para o público em geral. Para resolver esses problemas, várias ferramentas de autoria já foram propostas e podem ser utilizadas por usuários sem o uso de código, ferramentas no-code [@Neo2023Chatcreator].\nMesmo um chatbot baseado em LLM também não “pensa” no sentido cognitivo humano, mas opera a partir de um mecanismo estatístico de predição de tokens condicionado a um contexto. Sua forma de funcionamento é sustentada por arquiteturas neurais profundas, treinadas sobre grandes volumes de dados textuais, que capturam regularidades linguísticas, semânticas e pragmáticas. Assim, a resposta gerada resulta da estimativa da sequência de palavras mais provável dado um histórico de entrada, e não de um processo deliberativo consciente.\nA aparente coerência e racionalidade observadas nas respostas decorrem da capacidade do modelo em representar padrões complexos da linguagem natural em espaços vetoriais de alta dimensão. Esses modelos internalizam relações sintáticas, semânticas e discursivas, permitindo generalização para contextos não vistos durante o treinamento. Dessa forma, o chatbot consegue simular comportamentos como argumentação, explicação e síntese, ainda que tais comportamentos sejam emergentes do ajuste estatístico e não de compreensão genuína.\nA justificativa para esse modo de “pensar” reside na eficácia prática do paradigma. Ao invés de regras explícitas ou sistemas simbólicos rígidos, os LLMs oferecem flexibilidade, escalabilidade e adaptação a múltiplos domínios, o que os torna adequados para tarefas complexas de interação homem-máquina. A combinação de aprendizado profundo, atenção contextual e grandes corpora textuais permite ao chatbot responder de forma contextualizada, mantendo consistência local e global no discurso.\nPor fim, é fundamental reconhecer as limitações inerentes a esse modelo de funcionamento. A ausência de intencionalidade, consciência e compreensão semântica profunda implica que o chatbot pode produzir respostas plausíveis, porém factualmente incorretas ou logicamente inconsistentes. Assim, a forma de “pensar” de um chatbot baseado em LLM deve ser entendida como um mecanismo probabilístico avançado de geração de linguagem, útil e poderoso, mas distinto do raciocínio humano.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/4_introdu-o.html#exercícios",
    "href": "sections/4_introdu-o.html#exercícios",
    "title": "1  Introdução",
    "section": "1.7 Exercícios",
    "text": "1.7 Exercícios\n\nQual é o objetivo principal de um chatbot?\n\nSubstituir completamente os seres humanos no atendimento ao cliente.\nSimular uma conversa humana para resolver problemas ou fornecer informações.\nGerar textos literários complexos.\nArmazenar grandes quantidades de dados em tempo real.\n\nQual das seguintes opções descreve corretamente um benefício dos chatbots?\n\nEles nunca precisam ser atualizados.\nEles podem operar 24 horas por dia, 7 dias por semana, sem intervenção humana.\nEles sempre tomam decisões melhores do que humanos.\nEles substituem completamente a necessidade de suporte técnico.\n\nQual das opções a seguir é uma técnica comum usada por chatbots para entender o que o usuário está perguntando?\n\nMineração de Dados\nTokenização\nCompressão de Dados\nBalanceamento de Carga\n\nQual é o papel dos embeddings de palavras em chatbots?\n\nConverter palavras em vetores numéricos que capturam o significado semântico.\nArmazenar grandes quantidades de dados de conversação.\nExecutar algoritmos de compressão de texto.\nFacilitar a tradução de texto entre diferentes idiomas.\n\nQual é a principal limitação dos chatbots baseados em regras?\n\nEles não conseguem operar em tempo real.\nEles exigem grandes quantidades de dados para funcionar.\nEles só podem responder a consultas específicas para as quais foram\nprogramados.\nEles são incapazes de realizar tarefas repetitivas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html",
    "title": "2  ELIZA e AIML",
    "section": "",
    "text": "2.1 Introdução\nO chatbot ELIZA foi um marco histórico e um dos primeiros programas de processamento de linguagem natural e foi apresentado em 1966 por Joseph Weizenbaum no MIT [@Weizenbaum1996].\nO conjunto de padrões e respostas predefinidas constitui o que Weizenbaum chamou de “roteiro” (ou script) de conversa. O mecanismo do ELIZA separa o motor de processamento dos dados, sendo o algoritmo de busca de palavras-chave e aplicação de regras, do script em si. Isso significa que ELIZA podia, em teoria, simular diferentes personalidades ou tópicos apenas carregando um script diferente, sem alterar o código do programa (veja na Figura 2.1). Também foi codificado um editor de texto para as alterações do próprio script.\nUm destes roteiros, o script DOCTOR, deixou o ELIZA famoso. Com este script carregado, o ELIZA simulava precariamente um psicoterapeuta (do estilo Rogeriano), refletindo as palavras do usuário de volta em forma de pergunta.\nNa lista abaixo é apresentado um exemplo de diálogo (retirado do artigo original) entre um usuário humano e ELIZA, mostrando as respostas em letras maiúsculas. Nesse exemplo, o usuário faz afirmações pessoais (“Men are all alike.”, “Well, my boyfriend made me come here.”) e ELIZA responde com perguntas ou reformulações (“IN WHAT WAY”, “YOUR BOYFRIEND MADE YOU COME HERE?”) reutilizando termos do próprio usuário. Esse estilo reflexivo encoraja o usuário a continuar falando sem fornecer respostas diretas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#introdução",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#introdução",
    "title": "2  ELIZA e AIML",
    "section": "",
    "text": "Figure 7: ELIZA: separação entre o código fonte, o script e o editor de texto.\n\n\n\n\n\nMen are all alike. \nIN WHAT WAY \nThey're always bugging us about something or other. \nCAN YOU THINK OF A SPECIFIC EXAMPLE \nWell, my boyfriend made me come here. \nYOUR BOYFRIEND MADE YOU COME HERE \nHe says I'm depressed much of the time. \nI AM SORRY TO HEAR YOU ARE DEPRESSED \nIt's true. I am unhappy. \nDO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY \nI need some help, that much seems certain. \nWHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP \nPerhaps I could learn to get along with my mother. \nTELL ME MORE ABOUT YOUR FAMILY \nMy mother takes care of me. \nWHO ELSE IN YOUR FAMILY TAKES CARE OF YOU \nMy father. \nYOUR FATHER \nYou are like my father in some ways. \nWHAT RESEMBLANCE DO YOU SEE \nYou are not very aggressive but I think you don't want me to \nnotice that. \nWHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE \nYou don't argue with me. \nWHY DO YOU THINK I DON'T ARGUE WITH YOU \nYou are afraid of me. \nDOES IT PLEASE YOU TO BELIEVE I AM AFRAID OF YOU \nMy father is afraid of everybody. \nWHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER \nBullies. \nDOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YOU COME HERE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#processamento",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#processamento",
    "title": "2  ELIZA e AIML",
    "section": "2.2 Processamento",
    "text": "2.2 Processamento\nO funcionamento do ELIZA baseia-se em correspondência de padrões por palavras-chave [@mittmann2021ElizaOZ]. Técnica também chamada “casamento de padrão”, em inglês pattner matching, seu paradigma também se encaixa na técnica baseada em regras (ou rule-based). A cada turno do diálogo, o texto de entrada do usuário é analisado em busca de palavras-chave predefinidas. O ELIZA faz uma varredura da sentença da esquerda para a direita, identificando se alguma palavra correspondia a uma palavra-chave do script. Caso encontre uma palavra-chave no script, ELIZA empilha essa palavra até o final da frase. Depois, ele seleciona a palavra-chave mais “importante” até encontrar uma pontuação. Para isso, ele mantém uma lista de pesos associados a cada palavra-chave.\nPor exemplo, o script DOCTOR definia palavras-chave como “ALIKE” ou “SAME” com alta prioridade; assim, na frase “Men are all alike.” o programa detectava a palavra “ALIKE” e disparava uma resposta associada a ela (no caso: “In what way?”). Se múltiplas palavras-chave aparecessem, ELIZA escolhia aquela de maior peso para formular a resposta.\nPrimeiro o texto de entrada digitado pelo usuário era separado em palavras, em uma técnica que hoje chamamos de tokenização de palavras, mas que ainda não existia na época. A palavra-chave era identificada, comparando-a sequencialmente até o fim das palavras existentes, ou até ser encontrada uma pontuação. Caso fosse encontrada uma pontuação (ponto final ou vírgula), o texto após a pontuação era ignorado se já tivesse sido identificada uma palavra-chave. Assim, cada processamento da resposta utiliza uma única frase do usuário. Se várias palavras-chave fossem encontradas antes da pontuação, a de maior peso era selecionada.\nPor exemplo, o usuário entra com o texto: “I am sick. but, today is raining”. Se houvesse uma palavra-chave no script ranqueando a palavra “SICK” com alta prioridade, a entrada processada seria somente “I am sick”, o restante depois da pontuação (neste caso, o ponto) seria ignorado pelo programa.\nSe nenhuma palavra-chave fosse encontrada na entrada, ELIZA recorria a frases genéricas programadas, chamadas de respostas vazias ou sem conteúdo. Nesses casos, o chatbot emitia mensagens do tipo “I see.” ou “Please, go on.”. Esse mecanismo evitava silêncio quando o usuário dizia algo fora do escopo do script.\nAlém disso, a implementação original incluía uma estrutura de memória: algumas declarações recentes do usuário eram armazenadas e, se uma entrada subsequente não contivesse novas keywords, ELIZA poderia recuperar um tópico anterior e introduzi-lo na conversa. Por exemplo, se o usuário mencionasse família (em inglês family) em um momento e depois fizesse uma afirmação vaga, o programa poderia responder retomando o assunto da família (“DOES THAT HAVE ANYTHING TO DO WITH YOUR FAMILY?”). Essa estratégia dava uma pseudo-continuidade ao diálogo, simulando que o sistema “lembrava” de informações fornecidas anteriormente.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#regras-de-transformação",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#regras-de-transformação",
    "title": "2  ELIZA e AIML",
    "section": "2.3 Regras de Transformação",
    "text": "2.3 Regras de Transformação\nEncontrada a palavra-chave, ELIZA aplicava uma regra de transformação associada a ela para gerar a resposta. As regras são definidas em pares: um padrão de análise (decomposition rule) e um modelo de reconstrução de frase (reassembly rule).\nPrimeiro, a frase do usuário é decomposta conforme um padrão que identifica a contexto mínimo em torno da palavra-chave. Essa decomposição frequentemente envolve separar a frase em partes e reconhecer pronomes ou estruturas gramaticais relevantes. Por exemplo, considere a entrada “You are very helpful.”. Uma regra de decomposição pode identificar a estrutura “You are X” — onde “X” representa o restante da frase — e extrair o complemento “very helpful” como um componente separado.\nEm seguida, a regra de reassembly correspondente é aplicada, remontando uma sentença de resposta em que “X” é inserido em um template pré-definido. No exemplo dado, o template de resposta poderia ser “What makes you think I am X?”; ao inserir X = “very helpful”, gera-se “What makes you think I am very helpful?”. Observe que há uma inversão de pessoa: o pronome “you” do usuário foi trocado por “I” na resposta do bot.\nDe fato, uma parte importante das transformações do ELIZA envolve substituir pronomes (eu/você, meu/seu) para que a resposta faça sentido como uma frase do ponto de vista do computador falando com o usuário. Esse algoritmo de substituição é relativamente simples (por exemplo, “meu” → “seu”, “eu” → “você”, etc.), mas é essencial para dar a impressão de entendimento gramatical.\nVeja no código-fonte abaixo uma parte do conteúdo do arquivo DOCTOR adaptado. A primeira linha REMEMBER 5 estabelece que a prioridade da palavra REMEMBER é 5, caso o usuário entre com uma frase com a palavra REMEMBER, o ELIZA irá responder com uma das perguntas definidas nos parênteses, dentro do nível da palavra REMEMBER. A segunda lista dentro do REMEMBER são as regras de transformação de frases (chamadas de decomposition rule e reassembly rule) associadas à palavra-chave REMEMBER. O mesmo se repete para a palavra IF que tem um peso diferente de REMEMBER.\n\n(REMEMBER 5 \n(\n        (0 YOU REMEMBER 0) \n        (DO YOU OFTEN THINK OF 4) \n        (DOES THINKING OF ~ BRING ANYTHING ELSE TO MINO) \n        (WHAT ELSE OO YOU REMEMBER)\n        (WHY DO YOU REMEMBER 4 JUST NOW) \n        (WHAT IN THE PRESENT SITUATION REMINDS YOU OF 4) \n        (WHAT IS THE CONNECTION BETWEEN ME AND 4)\n    ) \n    (\n        (0 DO I REMEMBER 0) \n        (DID YOU THINK I WOULD FORGET 5) \n        (WHY DO YOU THINK I SHOULD RECALL 5 NOW) \n        (WHAT ABOUT 5) \n        (=WHAT) \n        (YOU MENTIONED S)\n    ) \n) \n(IF 3 \n    (\n        (0 IF 0) \n        (DO YOU THINK ITS LIKELY THAT 3) \n        (DO YOU WISH THAT 3) \n        (WHAT DO YOU THINK ABOUT 3) \n        (REALLY, 2 3)\n    )\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#implementação-e-variações",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#implementação-e-variações",
    "title": "2  ELIZA e AIML",
    "section": "2.4 Implementação e Variações",
    "text": "2.4 Implementação e Variações\nA implementação original de ELIZA foi feita em uma linguagem chamada MAD-SLIP (um dialeto de Lisp) rodando em um mainframe IBM 7094 no sistema CTSS do MIT. O código-fonte do programa principal continha o mecanismo de correspondência, enquanto as regras de conversação (script DOCTOR) eram fornecidas separadamente em formato de listas associativas, similar a uma lista em Lisp. Infelizmente, Weizenbaum não publicou o código completo no artigo de 1966 (o que era comum na época), mas décadas depois o código em MAD-SLIP foi recuperado nos arquivos do MIT, comprovando os detalhes de implementação [@Lane2025]. De qualquer forma, a arquitetura descrita no artigo influenciou inúmeras reimplementações acadêmicas e didáticas nos anos seguintes.\nDiversos entusiastas e pesquisadores reescreveram ELIZA em outras linguagens de programação, dada a simplicidade relativa de seu algoritmo. Ao longo dos anos, surgiram versões em Lisp, PL/I, BASIC, Pascal, Prolog, Java, Python, OZ, JavaScript, entre muitas outras. Cada versão normalmente incluía o mesmo conjunto de regras do script terapeuta ou pequenas variações.\nAs ideias de ELIZA também inspiraram chatbots mais avançados. Poucos anos depois, em 1972, surgiu PARRY, escrito pelo psiquiatra Kenneth Colby, que simulava um paciente paranoico. PARRY tinha um modelo interno de estado emocional e atitudes, mas na camada de linguagem ainda usava muitas respostas baseadas em regras, chegando a “conversar” com o próprio ELIZA em experimentos da época.\nEm 1995, Richard Wallace desenvolveu o chatbot ALICE (Artificial Linguistic Internet Computer Entity), que levava o paradigma de ELIZA a uma escala muito maior. ALICE utilizava um formato XML chamado AIML (Artificial Intelligence Markup Language) para definir milhares de categorias de padrões e respostas. Com mais de 16.000 templates mapeando entradas para saídas [@Wallace2000], ALICE conseguia manter diálogos bem mais naturais e abrangentes que o ELIZA original, embora o princípio básico de correspondência de padrões permanecesse. Esse avanço rendeu a ALICE três vitórias no Prêmio Loebner (competição de chatbots) no início dos anos 2000 [@Wallace2000].\nOutras variações e sucessores notáveis incluem Jabberwacky (1988) – que já aprendia novas frases – e uma profusão de assistentes virtuais e bots de domínio específico nas décadas seguintes [@Wallace2000]. Em suma, o legado de ELIZA perdurou por meio de inúmeros chatbots baseados em regras, até a transição para abordagens estatísticas e de aprendizado de máquina no final do século XX.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#mecanismo-de-pesos",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#mecanismo-de-pesos",
    "title": "2  ELIZA e AIML",
    "section": "2.5 Mecanismo de Pesos",
    "text": "2.5 Mecanismo de Pesos\nA técnica de ELIZA, baseada em palavras-chave com respostas predefinidas, contrasta fortemente com os métodos de Modelos de Linguagem Grande (LLMs) atuais, como o Generative Pre-trained Transformer (GPT), que utilizam redes neurais de milhões (ou trilhões) de parâmetros e mecanismos de atenção. Mais detalhes sobre LLM no Capítulo 4.\nNo ELIZA, a “importância” de uma palavra era determinada manualmente pelo programador através de pesos ou rankings atribuídos a certas palavras-chave no script. Ou seja, o programa não aprendia quais termos focar – ele seguia uma lista fixa de gatilhos. Por exemplo, termos como “sempre” ou “igual” tinham prioridade alta no script DOCTOR para garantir respostas apropriadas.\nEm contraste, modelos modernos como o GPT não possuem uma lista fixa de palavras importantes; em vez disso, eles utilizam o mecanismo de self-attention para calcular dinamicamente pesos entre todas as palavras da entrada conforme o contexto [@vaswani2017attention].\nNa arquitetura Transformer, cada palavra (na prática não é uma palavra e sim um token) de entrada gera consultas e chaves que interagem com todas as outras, permitindo ao modelo atribuir pesos maiores às palavras mais relevantes daquela frase ou parágrafo [@vaswani2017attention]. Em outras palavras, o modelo aprende sozinho quais termos ou sequências devem receber mais atenção para produzir a próxima palavra na resposta. Esse mecanismo de atenção captura dependências de longo alcance e nuances contextuais que um sistema de palavras-chave fixas como o ELIZA não consegue representar.\nAlém disso, o “vocabulário” efetivo de um LLM é imenso – um modelo GPT pode ser treinado com trilhões de palavras e ter ajustado seus parâmetros para modelar estatisticamente a linguagem humana [@vaswani2017attention]. Como resultado, pode-se dizer metaforicamente que os LLMs têm uma lista de “palavras-chave” milhões de vezes maior (na prática, distribuída em vetores contínuos) e um método bem mais sofisticado de calcular respostas do que o ELIZA.\nEnquanto ELIZA dependia de coincidências exatas de termos para disparar regras, modelos como GPT avaliam similaridades semânticas e contexto histórico graças às representações densas (embeddings) aprendidas durante o treinamento de rede neural.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#geração-de-texto",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#geração-de-texto",
    "title": "2  ELIZA e AIML",
    "section": "2.6 Geração de Texto",
    "text": "2.6 Geração de Texto\nDevido à sua abordagem baseada em regras locais, o ELIZA tinha capacidade de contextualização muito limitada. Cada input do usuário era tratado quase isoladamente: o programa não construía uma representação acumulada da conversa, além de artifícios simples como repetir algo mencionado (a estrutura de memória) ou usar pronomes para manter a ilusão de continuidade. Se o usuário mudasse de tópico abruptamente, o ELIZA não “perceberia” – ele apenas buscaria a próxima palavra-chave disponível ou recorreria a frases genéricas.\nEm contraste, modelos de linguagem modernos levam em conta um longo histórico de diálogo. Chatbots que usam GPT podem manter um contexto de centenas ou milhares de tokens (palavras ou fragmentos) em sua janela de atenção, o que significa que eles conseguem referenciar informações mencionadas vários parágrafos atrás e integrá-las na resposta corrente. O mecanismo de self-attention, em particular, permite que o modelo incorpore relações contextuais complexas: cada palavra gerada pode considerar influências de palavras distantes no texto de entrada [@vaswani2017attention].\nPor exemplo, ao conversar com um LLM, se você mencionar no início da conversa que tem um irmão chamado Alex e depois perguntar “ele pode me ajudar com o problema?”, o modelo entenderá que “ele” se refere ao Alex mencionado anteriormente (desde que dentro da janela de contexto). Já o ELIZA original não teria como fazer essa ligação, a menos que houvesse uma regra explícita para “ele” e algum armazenamento específico do nome – algo impraticável de antecipar via regras fixas para todos os casos.\nOutra diferença está na geração de linguagem. O ELIZA não gera texto original no sentido pleno: suas respostas são em grande parte frases prontas (ou templates fixos) embaralhadas com partes da fala do usuário. Assim, seu vocabulário e estilo são limitados pelo script escrito manualmente. Modelos GPT, por sua vez, geram respostas novas combinando probabilisticamente o conhecimento adquirido de um extenso corpus. Eles não se restringem a repetir trechos da entrada, podendo elaborar explicações, fazer analogias, criar perguntas coerentes com os exemplos linguísticos em sua base de treinamento. Enquanto ELIZA tendia a responder com perguntas genéricas ou devolvendo as palavras do usuário, os LLMs podem produzir respostas informativas e detalhadas sobre o assunto (pois “aprenderam” uma ampla gama de tópicos durante o treinamento). Por exemplo, se perguntarmos algo factual ou complexo, o ELIZA falharia por não ter nenhuma regra a respeito, provavelmente dando uma resposta vazia. Já um modelo como GPT tentará formular uma resposta baseada em padrões linguísticos aprendidos e em conhecimento implícito dos dados, muitas vezes fornecendo detalhes relevantes.\nEm termos de fluência e variedade, os modelos modernos superam o ELIZA amplamente. O ELIZA frequentemente se repetia ou caía em loops verbais quando confrontado com inputs fora do roteiro – um limite claro de sistemas por regras estáticas. Os LLMs produzem linguagem muito mais natural e adaptável, a ponto de muitas vezes enganarem os usuários sobre estarem conversando com uma máquina (um efeito buscado desde o Teste de Turing). Ironicamente, ELIZA nos anos 60 já provocou um precursor desse fenômeno – o chamado Efeito ELIZA, em que pessoas atribuem compreensão ou sentimentos a respostas de computador que, na verdade, são superficiais. Hoje, em chatbots GPT, esse efeito se intensifica pela qualidade das respostas, mas a distinção fundamental permanece: ELIZA seguia scripts sem compreender, enquanto LLMs inferem padrões e significados de forma estatística, sem entendimento consciente, mas atingindo resultados que simulam compreensão de maneira muito mais convincente.\nEm resumo, os avanços de arquitetura (especialmente o mecanismo de atenção) ampliaram drasticamente a capacidade de contextualização e geração dos chatbots modernos, marcando uma evolução significativa desde o mecanismo simples, porém pioneiro, de ELIZA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#eliza-com-python",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#eliza-com-python",
    "title": "2  ELIZA e AIML",
    "section": "2.7 ELIZA com Python",
    "text": "2.7 ELIZA com Python\nA seguir o código-fonte de um programa que retorna o que o usuário digitou na linguagem de programação Python, inspirado no paradigma ELIZA.\n\n# método que processa a entrada do usuário\ndef response(user_input):\n    return \"Você disse: \" + user_input\n\n# Exemplo de uso\nuser_input = \"Eu estou feliz\"\nprint(\"Você: {}\".format(user_input))\nprint(\"Eliza: {}\".format(response(user_input)))\nuser_input = \"Eu estou alegre\"\nprint(\"Você: {}\".format(user_input))\nprint(\"Eliza: {}\".format(response(user_input)))\n\n\nVocê: Eu estou feliz\nEliza: Você disse: Eu estou feliz\nVocê: Eu estou alegre\nEliza: Você disse: Eu estou alegre\n\nEste código acima simplesmente repete o que o usuário digita. Ele define a função response, que retorna a string “Você disse:” concatenada ao texto recebido.\nLogo abaixo uma implementação um pouco mais robusta de um chatbot inspirado no paradigma ELIZA. Esta implementação demonstra a utilização de expressões regulares - mais sobre expressões regulares no Capítulo 3 - para a identificação de padrões textuais (palavras-chave) na entrada fornecida pelo usuário e a subsequente geração de respostas, fundamentada em regras de transformação predefinidas manualmente.\n\nimport re  \nimport random  \n\nregras = [\n    (re.compile(r'\\b(hello|hi|hey)\\b', re.IGNORECASE),\n     [\"Hello. How do you do. Please tell me your problem.\"]), # regra 1\n    (re.compile(r'\\b(I am|I\\'?m) (.+)', re.IGNORECASE), # regra 2\n     [\"How long have you been {1}?\",   \n      \"Why do you think you are {1}?\"]),\n    (re.compile(r'\\bI need (.+)', re.IGNORECASE), # regra 3\n     [\"Why do you need {1}?\",\n      \"Would it really help you to get {1}?\"]),\n    (re.compile(r'\\bI can\\'?t (.+)', re.IGNORECASE), # regra 4\n     [\"What makes you think you can't {1}?\",\n      \"Have you tried {1}?\"]),\n    (re.compile(r'\\bmy (mother|father|mom|dad)\\b', re.IGNORECASE), # regra 5\n     [\"Tell me more about your family.\",\n      \"How do you feel about your parents?\"]),\n    (re.compile(r'\\b(sorry)\\b', re.IGNORECASE), # regra 6\n     [\"Please don't apologize.\"]),\n    (re.compile(r'\\b(maybe|perhaps)\\b', re.IGNORECASE), # regra 7\n     [\"You don't seem certain.\"]),\n    (re.compile(r'\\bbecause\\b', re.IGNORECASE), # regra 8\n     [\"Is that the real reason?\"]),\n    (re.compile(r'\\b(are you|do you) (.+)\\?$', re.IGNORECASE), # regra 9\n     [\"Why do you ask that?\"]),\n    (re.compile(r'\\bcomputer\\b', re.IGNORECASE), # regra 10\n     [\"Do computers worry you?\"]),\n]\n\nrespostas_padrao = [\n    \"I see.\",  \n    \"Please tell me more.\",  \n    \"Can you elaborate on that?\"  \n]\n\ndef response(entrada_usuario):\n    for padrao, respostas in regras:\n        match = padrao.search( entrada_usuario)  \n        if match:\n            resposta = random.choice(respostas)\n            if match.groups():\n                resposta = resposta.format( *match.groups())\n            return resposta\n    return random.choice(respostas_padrao)\n\nprint(\"User: Hello.\")\nprint(\"Bot: \" + response(\"Hello.\"))\nprint(\"User: I am feeling sad.\")\nprint(\"Bot: \" + response(\"I am feeling sad.\"))\nprint(\"Maybe I was not good enough.\")\nprint(\"Bot: \" + response(\"Maybe I was not good enough.\"))\nprint(\"My mother tried to help.\")\nprint(\"Bot: \" + response(\"My mother tried to help.\"))\n\n\nUser: Hello.\nBot: Hello. How do you do. Please tell me your problem.\nUser: I am feeling sad.\nBot: How long have you been feeling sad.?\nMaybe I was not good enough.\nBot: You don't seem certain.\nMy mother tried to help.\nBot: How do you feel about your parents?\n\nNa implementação anterior, são definidos múltiplos padrões de expressões regulares que correspondem a palavras-chave ou estruturas frasais de interesse (e.g., saudações, construções como “I am” ou “I need”, referências a termos familiares). A função response, ao receber uma string de entrada, itera sequencialmente sobre essas regras.\nQuando uma correspondência (match) é encontrada, uma resposta associada à regra é selecionada. É relevante notar que diversas respostas predefinidas contêm marcadores (placeholders), como {1}. Estes marcadores são dinamicamente substituídos pelo conteúdo dos grupos capturados pela expressão regular (mais detalhes sobre expressão regular no Capítulo 3 na entrada original do usuário). Por exemplo, se a entrada for “I am sad” e esta corresponder a um padrão como r“I am (.*)”, o grupo capturado ((.*)) conterá a string “sad”. Se a resposta associada for “Why do you think you are {1}?”, o sistema a formatará como “Why do you think you are sad?”, substituindo {1} pelo conteúdo do primeiro grupo capturado. Na ausência de correspondência com qualquer regra específica, o sistema seleciona aleatoriamente uma resposta genérica a partir de um conjunto predefinido (denominado, por exemplo, respostas_padrao), tal como “I see.”.\n\nUsuário: Hello\nBot: Hello. How do you do. Please tell me your problem.\nUsuário: I am feeling sad\nBot: Why do you think you are feeling sad?\nUsuário: Because I lost my job\nBot: Is that the real reason?\nUsuário: Maybe I was not good enough.\nBot: You don't seem certain.\nUsuário: My mother tried to help.\nBot: Tell me more about your family.\n\nObserva-se na interação que o chatbot inicialmente respondeu à saudação (“Hello...”) com base na regra 1. Em seguida, a entrada “I am sad” ativou a regra 2, resultando em uma resposta que incorpora o termo “sad”. A interação prossegue com o sistema respondendo a entradas iniciadas por “Because...” (regra 8) e “Maybe...” (regra 7), entre outras. Cada resposta gerada foi determinada pela regra correspondente ativada e pelo conteúdo específico da sentença do usuário, mimetizando o comportamento baseado em padrões do sistema ELIZA original [@Abdul-Kader2015].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#eliza-com-python-e-gradio",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#eliza-com-python-e-gradio",
    "title": "2  ELIZA e AIML",
    "section": "2.8 ELIZA com Python e Gradio",
    "text": "2.8 ELIZA com Python e Gradio\nNesta seção, vamos explorar como criar um chatbot usando a biblioteca Gradio e Python.\nO Gradio é uma biblioteca de código aberto que permite criar interfaces de usuário para modelos de machine learning de forma rápida e fácil. Com Gradio, você pode criar aplicativos web interativos para visualizar e testar seus modelos, sem precisar de conhecimentos em frontend ou backend.\nAntes de começar, certifique-se de ter o Python e o pip instalados em sua máquina. Em seguida, instale a biblioteca Gradio usando o comando:\n\n$ pip install gradio\n\nÀ seguir está um exemplo básico de como criar um chatbot com Gradio e Python:\n\nimport gradio as gr\n\ndef chatbot(message, history):\n    resposta = \"Olá! Eu sou um chatbot. Como posso ajudar você?\"\n    return resposta\n\ndemo = gr.ChatInterface(\n    fn=chatbot,\n    title=\"Chatbot Simples\"\n)\n\ndemo.launch()\n\nEste código define uma função chatbot que processa o texto de entrada e retorna uma resposta. Em seguida, cria uma interface do chatbot usando a biblioteca Gradio, com um campo de texto para entrada e outro para saída.\nExecute o chatbot: Salve o arquivo e, no terminal, navegue até o diretório onde ele está salvo. Digite:\n\n$ python chatbot.py\n\nVocê verá um link local (algo como http://127.0.0.1:7860). Clique nele ou copie e cole no navegador. Uma interface simples aparecerá com um campo de texto. Experimente digitar “olá” ou “tchau” e veja as respostas. Veja na Figura 2.2 um print da tela do chatbot já em execução.\n\n\n \n\n\nFigure 8: Print do Chatbot.\n\n\nAdicionando Lógica ao Chatbot: Agora que você tem uma interface básica, é hora de adicionar lógica ao chatbot. Você pode fazer isso adicionando condições e processamento de texto à função chatbot. Por exemplo:\n\n# pip install gradio\nimport gradio as gr\n\ndef chatbot(message, history):\n    if \"Olá\" in message:\n        resposta = \"Olá! Eu sou um chatbot. Como posso ajudar você?\"\n    elif \"Quem é você?\" in message:\n        resposta = \"Eu sou um chatbot criado para ajudar você com suas perguntas.\"\n    else:\n        resposta = \"Desculpe, não entendi sua pergunta. Pode tentar novamente?\"\n    return resposta\n\ndemo = gr.ChatInterface(\n    fn=chatbot,\n    title=\"Chatbot Simples\"\n)\n\ndemo.launch()\n\nNa Figura 2.3 é possível visualizar o print da tela de um chatbot que responde de forma aleatória. Logo abaixo, o código-fonte que deu origem ao chatbot.\n\n# pip install gradio\nimport gradio as gr\nimport random\n\ndef chatbot(message, history):\n    respostas = [\"Interessante!\", \"Hmm, me conte mais!\", \"Não sei, mas vou fingir que sim!\"]\n    if \"Olá\" in message:\n        resposta = \"Olá! Eu sou um chatbot. Como posso ajudar você?\"\n    elif \"Quem é você?\" in message:\n        resposta = \"Eu sou um chatbot criado para ajudar você com suas perguntas.\"\n    else:\n        resposta = random.choice(respostas)        \n    return resposta\n\ndemo = gr.ChatInterface(\n    fn=chatbot,\n    title=\"Chatbot Simples\"\n)\n\ndemo.launch()\n\n\n\n \n\n\nFigure 9: Print do chatbot com respostas aleatórias e um pouco de lógica Conclusão.\n\n\nCriar um chatbot com Gradio e Python é uma tarefa relativamente simples e rápida. Com essa biblioteca, você pode criar interfaces de usuário interativas para seus modelos de aprendizagem de máquina e criar chatbots para automação de tarefas ou suporte ao cliente. Este é apenas um exemplo básico, e você pode adicionar mais lógica e funcionalidades ao seu chatbot para torná-lo mais útil.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#cap:AIML",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#cap:AIML",
    "title": "2  ELIZA e AIML",
    "section": "2.9 Artificial Intelligence Markup Language (AIML)",
    "text": "2.9 Artificial Intelligence Markup Language (AIML)\nDécadas depois do Eliza, as técnicas de programação avançaram, surgiu a web e as linguagens de marcação como o HTML ganharam tração. É neste cenário que é criado o Artificial Intelligence Markup Language (AIML), uma especificação baseada em XML, proposta por @Wallace2009, destinada à programação de chatbots. A concepção da linguagem prioriza o minimalismo, característica que simplifica o processo de criação de bases de conhecimento por indivíduos sem experiência prévia em programação [@Wallace2009]. A arquitetura fundamental de um interpretador AIML genérico é ilustrada na Figura 2.4.\n\n\n \n\n\nFigure 10: Interpretador AIML arquitetura. Adaptado de (A. C. da Silva and Costa 2007)\n\n\nA técnica central empregada pelo AIML é a correspondência de padrões (pattern matching). Este método é amplamente utilizado no desenvolvimento de chatbots, particularmente em sistemas orientados a perguntas e respostas [@Abdul-Kader2015]. Uma das metas de projeto do AIML é possibilitar a fusão de bases de conhecimento de múltiplos chatbots especializados em domínios distintos. Teoricamente, um interpretador poderia agregar essas bases, eliminando automaticamente categorias redundantes para formar um chatbot mais abrangente [@Wallace2000].\nAIML é frequentemente associado aos chatbots de terceira geração [@Maria2010] e estima-se sua adoção em mais de 50.000 implementações em diversos idiomas. Extensões da linguagem foram propostas, como o iAIML, que introduziu novas tags e incorporou o conceito de intenção com base nos princípios da Teoria da Análise da Conversação [@Neves2005]. Adicionalmente, ferramentas baseadas na Web foram desenvolvidas para apoiar a construção de bases de conhecimento AIML [@Krassmann2017]. Um exemplo proeminente é o chatbot ALICE, cuja implementação em AIML compreendia aproximadamente 16.000 categorias, cada uma potencialmente contendo múltiplas tags XML aninhadas [@Wallace2000]. Uma representação visual desta estrutura de conhecimento é apresentada na Figura 2.5.\n\n\n \n\n\nFigure 11: Representação visual da base de conhecimento do chatbot ALICE. Retirado de (R. Wallace 2003)\n\n\n@Wallace2000 estabeleceu analogias entre o funcionamento de interpretadores AIML e a teoria do Raciocínio Baseado em Casos (RBC). Nessa perspectiva, as categorias AIML funcionam como “casos”, onde o algoritmo identifica o padrão que melhor se alinha à entrada do usuário. Cada categoria estabelece um vínculo direto entre um padrão de estímulo e um modelo de resposta. Consequentemente, chatbots AIML inserem-se na tradição da robótica minimalista, reativa ou de estímulo-resposta [@Wallace2000], conforme esquematizado na Figura 2.6. Vale notar que a própria técnica de RBC já foi integrada a interpretadores AIML como um mecanismo para consultar fontes de dados externas e expandir a base de conhecimento do agente [@Kraus2008].\n\n\n \n\n\nFigure 12: Teoria estímulo-resposta aplicada no AIML. Retirado de (Lima 2017).\n\n\nOs chatbots que utilizam AIML são classificados como sistemas “baseados em recuperação” (retrieval-based). Tais modelos operam a partir de um repositório de respostas predefinidas, selecionando a mais apropriada com base na entrada do usuário e no contexto conversacional, guiando assim o fluxo da interação. Esta abordagem é frequentemente empregada na construção de chatbots destinados a operar em domínios de conhecimento restritos [@borah2018survey].\nO código-fonte à seguir, demonstra a estrutura elementar de um arquivo AIML. A tag &lt;category&gt; encapsula a unidade básica de conhecimento. Internamente, a tag &lt;pattern&gt; define o padrão de entrada a ser reconhecido (no exemplo, o caractere curinga *, que corresponde a qualquer entrada), enquanto a tag &lt;template&gt; contém a resposta associada. No exemplo ilustrado, o chatbot responderia “Hello!” a qualquer interação. Uma visão abstrata da árvore de conhecimento resultante pode ser observada logo abaixo. O AIML padrão suporta transições baseadas primariamente em correspondência de padrões, uma limitação inerente, embora extensões específicas de interpretadores possam permitir a integração de outras técnicas de processamento.\n\n&lt;aiml&gt;\n&lt;category&gt;\n    &lt;pattern&gt;*&lt;/pattern&gt;\n    &lt;template&gt;Hello!&lt;/template&gt;\n&lt;/category&gt;\n&lt;/aiml&gt;\n\n\n\n \n\n\nFigure 13: Representação visual abstrata de uma base de conhecimento AIML. Retirado de https://www.pandorabots.com/docs/aiml-fundamentals/\n\n\nO profissional responsável pela criação, manutenção e curadoria da base de conhecimento de um chatbot AIML é denominado botmaster [@Wallace2000]. Suas atribuições englobam a edição da base (frequentemente via ferramentas auxiliares), a análise de logs de diálogo para identificar padrões de interação e a subsequente criação ou refino de respostas. Este papel pode ser exercido por indivíduos com diferentes perfis, incluindo webmasters, desenvolvedores, redatores, engenheiros ou outros interessados na construção de chatbots [@Wallace2000].\nAlgumas implementações de interpretadores AIML podem incorporar capacidades rudimentares de compreensão semântica através do Resource Description Framework (RDF)[^1]. O RDF é um padrão W3C para representação de informações na Web, usualmente por meio de triplas (sujeito-predicado-objeto) que descrevem relações entre entidades. No contexto AIML, RDF pode ser utilizado para armazenar e consultar fatos. Contudo, mesmo com tais adições, as capacidades linguísticas permanecem aquém da complexidade e do potencial gerativo da linguagem humana, conforme descrito por @chomsky2002syntactic.\nEmbora @Hohn2019 argumente que o AIML padrão carece de um conceito explícito de “intenção” (intent), similar ao encontrado em plataformas de Natural Language Understanding (NLU), é possível emular o reconhecimento de intenções. Isso é tipicamente alcançado definindo categorias que representam “formas canônicas” ou “padrões atômicos” para uma intenção específica[^2]. Variações de entrada (e.g., “oi”, “olá”) podem ser mapeadas para uma categoria canônica (e.g., “saudação”) usando a tag &lt;srai&gt; (Symbolic Reduction Artificial Intelligence), que redireciona o fluxo de processamento (ver Figura 2.8). Dessa forma, um chatbot AIML pode gerenciar intenções distintas dentro de seu domínio, como realizar um pedido ou verificar o status de entrega.\n\n\n \n\n\nFigure 14: Uso da tag &lt;srai&gt;. Retirado de (De Gasperis, Chiari, and Florio 2013)\n\n\nOs chatbots baseados em AIML têm obtido sucesso significativo em competições como o Prêmio Loebner. Notavelmente, o chatbot Mitsuku[^3], desenvolvido por Steve Worswick, conquistou múltiplos títulos [^4], seguindo vitórias anteriores do ALICE [@Wallace2000].\nAdicionalmente, Mitsuku foi classificado em primeiro lugar numa análise comparativa envolvendo oito chatbots [@Sharma2020]. Nesse estudo, que avaliou atributos conversacionais com base em um conjunto padronizado de perguntas, o Google Assistant obteve a segunda posição, seguido pela Siri em terceiro. O chatbot ALICE alcançou a quarta posição, enquanto o ELIZA ficou na última colocação entre os sistemas comparados [@Sharma2020].\n\n2.9.1 Tags do AIML\nEsta seção descreve as principais tags do AIML, versão 1.0.\n\n&lt;aiml&gt;\nNo contexto de AIML (Artificial Intelligence Markup Language), a tag &lt;aiml&gt; é usada para definir o início de um documento AIML que contém os padrões e respostas que um motor de chatbot deve usar. Ela envolve todo o documento, indicando que o conteúdo entre as tags &lt;aiml&gt; é escrito em AIML.\n\n&lt;aiml version=\"1.0\"&gt;\n    &lt;!-- Categorias aqui --&gt;\n&lt;/aiml&gt;\n\n\n\n&lt;category&gt;\nDescrição: Unidade básica de conhecimento, contendo um padrão e uma resposta.\n\n&lt;category&gt;\n    &lt;pattern&gt;OLÁ&lt;/pattern&gt;\n    &lt;template&gt;Oi! Como posso ajudar você hoje?&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;pattern&gt;\nDescrição: Define o padrão de entrada do usuário, com curingas como * e _.\n\n&lt;category&gt;\n    &lt;pattern&gt;EU GOSTO DE *&lt;/pattern&gt;\n    &lt;template&gt;Que bom que você gosta de &lt;star/&gt;!&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;template&gt;\nDescrição: Define a resposta do bot ao padrão correspondente.\n\n&lt;category&gt;\n    &lt;pattern&gt;QUAL É O SEU NOME&lt;/pattern&gt;\n    &lt;template&gt;Meu nome é neo chatbot.&lt;/template&gt;\n&lt;/category&gt; \n\n\n\n&lt;star/&gt;\nDescrição: Captura o conteúdo do curinga * ou _.\n\n&lt;category&gt;\n    &lt;pattern&gt;MEU NOME É *&lt;/pattern&gt;\n    &lt;template&gt;Olá, &lt;star/&gt;!&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;that&gt;\nDescrição: Considera a última resposta do bot para decidir a próxima.\n\n&lt;category&gt;\n    &lt;pattern&gt;SIM&lt;/pattern&gt;\n    &lt;that&gt;Você gosta de programar?&lt;/that&gt;\n    &lt;template&gt;Ótimo! Qual linguagem você prefere?&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;topic&gt;\nDescrição: Define um contexto ou tópico para categorias.\n\n&lt;category&gt;\n  &lt;pattern&gt;VAMOS FALAR SOBRE ESPORTE&lt;/pattern&gt;\n  &lt;template&gt;Ok! &lt;topic name=\"esporte\"/&gt;&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;random&gt; e &lt;li&gt;\nDescrição: Escolhe aleatoriamente uma resposta de uma lista.\n\n&lt;category&gt;\n  &lt;pattern&gt;COMO ESTÁ O TEMPO&lt;/pattern&gt;\n  &lt;template&gt;\n    &lt;random&gt;\n      &lt;li&gt;Está ensolarado!&lt;/li&gt;\n      &lt;li&gt;Está chovendo.&lt;/li&gt;\n    &lt;/random&gt;\n  &lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;condition&gt;\nDescrição: Adiciona lógica condicional baseada em variáveis.\n\n&lt;category&gt;\n  &lt;pattern&gt;COMO EU ESTOU&lt;/pattern&gt;\n  &lt;template&gt;\n    &lt;condition name=\"humor\"&gt;\n      &lt;li value=\"feliz\"&gt;Você está bem!&lt;/li&gt;\n      &lt;li&gt;Não sei ainda!&lt;/li&gt;\n    &lt;/condition&gt;\n  &lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;set&gt; e &lt;get&gt;\nDescrição: Define e recupera variáveis.\n\n&lt;category&gt;\n    &lt;pattern&gt;MEU NOME É *&lt;/pattern&gt;\n    &lt;template&gt;\n        &lt;set name=\"nome\"&gt;&lt;star/&gt;&lt;/set&gt;Olá, &lt;get name=\"nome\"/&gt;!\n    &lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;srai&gt;\nDescrição: Redireciona a entrada para outro padrão.\n\n&lt;category&gt;\n    &lt;pattern&gt;OI&lt;/pattern&gt;\n    &lt;template&gt;&lt;srai&gt;OLÁ&lt;/srai&gt;&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;think&gt;\nDescrição: Executa ações sem exibir o conteúdo.\n\n&lt;category&gt;\n    &lt;pattern&gt;EU SOU TRISTE&lt;/pattern&gt;\n    &lt;template&gt;\n    &lt;think&gt;&lt;set name=\"humor\"&gt;triste&lt;/set&gt;&lt;/think&gt;Sinto muito!\n    &lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;person&gt;, &lt;person2&gt;, &lt;gender&gt;\nDescrição: Transforma pronomes ou ajusta gênero.\n\n&lt;category&gt;\n    &lt;pattern&gt;EU TE AMO&lt;/pattern&gt;\n    &lt;template&gt;&lt;person&gt;&lt;star/&gt;&lt;/person&gt; ama você também!&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;formal&gt;, &lt;uppercase&gt;, &lt;lowercase&gt;\nDescrição: Formata texto (capitaliza, maiúsculas, minúsculas).\n\n&lt;category&gt;\n  &lt;pattern&gt;MEU NOME É joão&lt;/pattern&gt;\n  &lt;template&gt;Olá, &lt;formal&gt;&lt;star/&gt;&lt;/formal&gt;!&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n&lt;sentence&gt;\nDescrição: Formata como frase (primeira letra maiúscula, ponto final).\n\n&lt;category&gt;\n  &lt;pattern&gt;oi&lt;/pattern&gt;\n  &lt;template&gt;&lt;sentence&gt;&lt;star/&gt;&lt;/sentence&gt;&lt;/template&gt;\n&lt;/category&gt;\n\n\n\n\n2.9.2 Exemplo em Python\nA seguir um exemplo do uso de um interpretador AIML em Python. O arquivo “cerebro.aiml” deve existir anteriormente. Use uma versão compatível com a biblioteca aiml que é somente compatível com versões antigas do Python, do Python 3.6 para trás, ou seja, não funciona no Python 3.12.\n\n# pip install aiml\nimport aiml\n# Criar kernel (núcleo do bot)\nkernel = aiml.Kernel()\ncerebro_aiml_text = \"\"\"\n&lt;aiml version=\"1.0.1\" encoding=\"UTF-8\"&gt;\n    &lt;category&gt;\n        &lt;pattern&gt;OI&lt;/pattern&gt;\n        &lt;template&gt;Olá! Como posso ajudar você?&lt;/template&gt;\n    &lt;/category&gt;\n    &lt;category&gt;\n        &lt;pattern&gt;OBRIGADO&lt;/pattern&gt;\n        &lt;template&gt;De nada!&lt;/template&gt;\n    &lt;/category&gt;\n&lt;/aiml&gt;\n\"\"\"\n\n# Salvar o conteúdo AIML em um arquivo\nwith open(\"cerebro.aiml\", \"w\", encoding=\"utf-8\") as f:\n    f.write(cerebro_aiml_text)\n# Carregar o arquivo AIML\nkernel.learn(\"cerebro.aiml\")\n\n# Loop de conversa\nwhile True:\n    user_input = input(\"Você: \")\n    if user_input.lower() in [\"sair\", \"exit\", \"quit\"]:\n        break\n    response = kernel.respond(user_input)\n    print(\"Bot:\", response)\n\n\nVocê: oi\nBot: Olá! Como posso ajudar você?\n\nO arquivo std-startup.xml é um ponto de partida e geralmente carrega outros arquivos .aiml.\n\n&lt;aiml version=\"1.0.1\" encoding=\"UTF-8\"&gt;\n  &lt;category&gt;\n    &lt;pattern&gt;OI&lt;/pattern&gt;\n    &lt;template&gt;Olá! Como posso te ajudar?&lt;/template&gt;\n  &lt;/category&gt;\n  &lt;category&gt;\n    &lt;pattern&gt;QUAL SEU NOME&lt;/pattern&gt;\n    &lt;template&gt;Eu sou um chatbot em AIML.&lt;/template&gt;\n  &lt;/category&gt;\n&lt;/aiml&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/5_eliza-e-aiml-cap-eliza.html#exercícios",
    "href": "sections/5_eliza-e-aiml-cap-eliza.html#exercícios",
    "title": "2  ELIZA e AIML",
    "section": "2.10 Exercícios",
    "text": "2.10 Exercícios\n\nAssinale a alternativa que melhor descreve a principal diferença entre um chatbot conversacional e um chatbot orientado a tarefas:\n\nO chatbot conversacional utiliza apenas regras, enquanto o orientado a tarefas utiliza apenas redes neurais.\nO chatbot conversacional busca manter diálogo aberto, enquanto o orientado a tarefas segue fluxos específicos para executar ações.\nChatbots orientados a tarefas não podem utilizar linguagem natural.\nChatbots conversacionais não podem responder perguntas sobre assuntos gerais.\n\nSobre o ELIZA, marque a alternativa correta:\n\nUtilizava redes neurais profundas e mecanismos de autoatenção.\nInterpretava semanticamente as frases do usuário com compreensão profunda.\nBaseava-se em regras simples e padrões para reformular entradas do usuário.\nPossuía memória de longo prazo para manter contexto amplo de conversa.\n\nA arquitetura Transformers, introduzida em 2017, é caracterizada principalmente por:\n\nUtilizar sequências fixas de regras pré-programadas.\nEmpregar mecanismos de autoatenção para avaliar relações entre palavras.\nSubstituir completamente qualquer forma de treinamento com dados.\nImpedir o uso de modelos generativos em tarefas de diálogo.\n\nQual a definição de Artificial Intelligence Markup Language (ou AIML):\n\nUma linguagem de marcação baseada em XML para construção de chatbots por correspondência de padrões.\nUm algoritmo estatístico baseado em cadeias de Markov.\nUma técnica de aprendizado profundo utilizada em modelos generativos.\nUma ferramenta exclusiva para chatbots multimodais.\n\nDe acordo com a discussão sobre agentes, qual das alternativas caracteriza corretamente um “agente” no contexto apresentado?\n\nUm programa capaz apenas de gerar texto sem interagir com o ambiente.\nUma entidade computacional que percebe seu ambiente e atua por meio de sensores e atuadores.\nUm sistema que realiza exclusivamente tarefas bancárias automatizadas.\nQualquer modelo de linguagem de grande porte, independentemente de suas capacidades.\n\nExplique em que sentido um chatbot pode ser entendido como uma instanciação específica de um agente. Em sua resposta, (a) apresente uma definição de chatbot, (b) apresente uma definição de agente e (c) discuta as diferenças e sobreposições entre esses conceitos, incluindo a distinção entre sistemas que apenas conversam e sistemas que também utilizam ferramentas externas (como navegação na Web, execução de código e interação com APIs).\nCompare os chatbots ELIZA e ChatGPT considerando pelo menos quatro aspectos: (a) período histórico e contexto de surgimento, (b) técnica ou abordagem principal utilizada para gerar respostas, (c) tipo de tarefa ou objetivo conversacional para o qual foram concebidos e (d) limitações em termos de compreensão de linguagem, contexto e emoções. Finalize discutindo por que ambos podem ser classificados como chatbots conversacionais.\nDiferencie chatbots conversacionais e chatbots orientados a tarefas, ilustrando sua resposta com exemplos. Em seguida, analise o papel do fluxo conversacional nesses sistemas: descreva como fluxos de diálogo (por exemplo, árvores de decisão e fluxos de perguntas e respostas) são empregados em chatbots orientados a tarefas, e comente sobre como esses fluxos se relacionam com conceitos da Análise da Conversação, como tópicos conversacionais e coerência entre turnos.\nApresente e discuta a evolução das abordagens tecnológicas para a construção de chatbots, partindo de sistemas baseados em correspondência de padrões (como ELIZA e AIML/ALICE) até modelos neurais modernos baseados em Transformers e LLMs (como BERT, GPT e outros modelos recentes listados no capítulo). Em sua resposta, destaque: (a) a ideia central de cada abordagem, (b) vantagens e desvantagens principais e (c) impactos dessa evolução na qualidade das interações com os usuários.\nDesenvolva um texto em que você (a) explique por que um chatbot, mesmo avançado, não pode ser considerado pensante nos mesmos termos que um humano, (b) descreva problemas práticos de construção e manutenção de chatbots (por exemplo, complexidade de fluxos condicionais, atualização de conhecimento, dificuldade de visualização de diálogos complexos) e (c) discuta o papel de ferramentas de autoria no-code na tentativa de tornar o desenvolvimento de chatbots mais acessível a usuários sem formação em programação.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ELIZA e AIML</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "",
    "text": "3.1 Inteligência Artificial\nA inteligência artificial (IA) é a força tecnológica mais transformadora do século XXI [@ribeiro2025eraIA]. Mas o que é IA? As definições de IA dependem do contexto e isso pode trazer confusão no entendimento e delimitação do tema. Menos abrangente, porém mais confuso ainda, é o termo “inteligência artificial”. Portanto, dado as diversas definições de inteligência artificial (IA), ou artificial inteligence em inglês, delimitaremos um pouco o escopo da inteligência em questão.\nA IA aparece em nossa cultura de diversas formas, tais como, o HAL 9000 do filme “2001: uma Odisseia no Espaço”, clássico de Stanley Kubrick, ou como a IA do filme “Ela”, com o ator Joaquin Phoenix, onde um humano se apaixona por um sistema operacional.\nEspero que você leitor seja um membro da espécie Homo-Sapiens. O termo “Homo-Sapiens” vem do latim e significa homem sábio [@wikipediahumano]. A importância da sapiência (sinônimo de inteligência) é tamanha que define a nossa espécie. Porém, neste contexto, consideramos que um gato ou cachorro também é dotado de inteligência. Uma abelha é praticamente uma cientista [@wikipediaabelhas]. Portanto, seremos mais contidos e reservados quanto ao significado do termo inteligência.\nO que confunde é que inteligência e artificial são palavras que têm significado implícito para pessoas que não são da área de computação. Naturalmente, médicos, advogados, engenheiros (só para citar alguns) querem verificar como a “inteligência artificial” pode ser inserida na sua rotina diária. Meu dentista já quis saber como a IA iria afetar seus procedimentos odontológicos. Porém, ele nunca me perguntou em como a “Transformada de Fourier” poderia melhorar o seu dia-a-dia, mesmo sabendo que a transformada já é utilizada em vários domínios do conhecimento e com entusiasmo [@wikipediafourier].\nA inteligência artificial da computação está mais relacionada com a capacidade de realizar coisas que seres inteligentes (tais como um gato, um bebê, uma abelha ou um humano) realizam, como, por exemplo, puxar a mão (ou pata) instantaneamente ao tocar em uma superfície quente, realizar uma prova objetiva de anatomia ou elaborar um recurso para a anulação de uma questão de concurso. Se um programa realiza uma ação geralmente realizada por uma entidade dotada de inteligência, ele pode ser encarado como um programa que simula uma inteligência artificial. Convenhamos que praticamente qualquer coisa cabe neste conceito.\nSobre este tema, o livro de Russell e Norvig (um dos livros mais lidos em todas as universidades do mundo sobre o tema) tem uma boa definição sobre o tema: “O campo da inteligência artificial [...] tenta não apenas compreender, mas também construir entidades inteligentes” (tradução nossa) [@Russel2013]. Em outras palavras, a inteligência artificial da ciência da computação tem o audacioso objetivo de construir agentes dotados de inteligência.\nA origem do termo “inteligência artificial”, na ciência da computação, é geralmente atribuída a John McCarthy, professor de Matemática da Universidade Dartmouth College [@blipblog] (Figura [fig:jhonalan]). Ele organizou uma conferência com duração de oito semanas com outros colegas em 1956, alguns anos após a Segunda Guerra, e desde então o termo vem sendo utilizado para designar parte de conteúdos estudados em ciência da computação.\nUm pouco antes, o artigo seminal de Alan Turing, com quem John McCarthy trabalhou em conjunto, já apresentava reflexões sobre a inteligência que uma máquina poderia possuir [@Turing1950]. No entanto, a inteligência artificial aparece na literatura há milhares de anos; um exemplo é o Gigante Talos de Creta, um autômato proveniente da mitologia grega [@pickover2021artificial].\nFoi na década de 1970 que o uso da inteligência artificial começou a ser mais difundido. Uma das primeiras abordagens com relativo sucesso foi os Sistemas Especialistas (SE). Eles dependiam dos especialistas do domínio para transformar o conhecimento tácito (baseado em sua experiência) em explícito (formalizado, documentado), que era então codificado na forma de regras em lógica formal. O processo de aquisição desse conhecimento acabou sendo um grande obstáculo na adoção em massa dessa abordagem. Veja um exemplo de software que implementa um motor de inferência baseado na teoria dos SE na Figura 3.1.\nA superação de algumas limitações (tais como o aumento da capacidade de processamento e armazenamento dos computadores, a geração de grandes volumes de dados, novidades científicas e tecnológicas, chips supercondutores e a eficiência energética) permitiu o avanço de outras técnicas. Uma das técnicas que têm ganho notoriedade, por causa desses avanços, é o Aprendizado de Máquina.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#inteligência-artificial",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#inteligência-artificial",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "",
    "text": "Figure 15: Interface de um Sistema Especialista ExpertSinta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#aprendizado-de-máquina",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#aprendizado-de-máquina",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.2 Aprendizado de Máquina",
    "text": "3.2 Aprendizado de Máquina\nO Aprendizado de Máquina (AM) é uma subárea da IA motivada pelo desenvolvimento de softwares mais independentes da intervenção humana para extração do conhecimento, o que era uma dificuldade nos Sistemas Especialistas. Geralmente, aplicações de AM utilizam indução para buscar modelos capazes de representar o conhecimento existente nos dados.\nNa Figura 3.2, é possível identificar alguns usos de AM integrado em algumas atividades cotidianas. São elas: (a) um smartphone com um assistente de voz fornecendo atualizações meteorológicas; (b) um sistema de casa inteligente ajustando o termostato com base nas preferências do usuário; (c) um carro autônomo dirigindo em uma rua movimentada da cidade; (d) uma plataforma de compras online recomendando produtos a um usuário com base em suas compras anteriores. Essa figura foi criada inclusive com uma inteligência artificial chamada DALL·E 3, disponível no ChatGPT. ChatGPT é um chatbot que ganhou notoriedade, sendo um dos aplicativos que mais ganhou usuários rapidamente no mundo.\n\n\n \n\n\nFigure 16: Exemplos AM.\n\n\nAs tarefas de aprendizado de máquina podem ser divididas entre tarefas preditivas e descritivas. As tarefas de aprendizado preditivas visam inferir o atributo alvo de uma nova entrada a partir da exposição prévia aos dados durante o treinamento do modelo. As tarefas descritivas buscam extrair padrões e correlações; além disso, não existe esta distinção entre atributos alvo e preditivos.\n\n\n \n\n\nFigure 17: Classificação de AM.\n\n\nAmbas as tarefas podem ser categorizadas sob o conceito de aprendizado indutivo, sendo a capacidade de generalizar a partir de exemplos específicos, isto é, do conjunto de dados de treinamento. Em se tratando de tarefas preditivas, os algoritmos poderão implementar tarefas de classificação, nas quais o atributo alvo é qualitativo discreto (ou categórico), ou de regressão, em que o atributo alvo é quantitativo contínuo (ou numérico). Já as tarefas descritivas podem ser: agrupamento, que busca por similaridades, associação, que busca por padrões frequentes, e sumarização, que resulta em um resumo do conjunto de dados. No entanto, outras técnicas de aprendizagem de máquina supervisionadas e não supervisionadas estão fora do escopo deste livro.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#processamento-de-linguagem-natural",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#processamento-de-linguagem-natural",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.3 Processamento de Linguagem Natural",
    "text": "3.3 Processamento de Linguagem Natural\nO Processamento de Linguagem Natural (PLN) é um campo ligado à inteligência artificial, dedicando-se a equipar computadores com a capacidade de analisar e compreender a linguagem humana. Ele emprega técnicas computacionais com o propósito de aprender, compreender e produzir conteúdo em linguagem humana [@Zhao2020]. Os sistemas de PLN podem suportar diferentes níveis ou combinações de níveis de análise linguística [@Zhao2020]. Os níveis de análise linguística referem-se à análise fonética, morfológica, léxica, sintática, semântica, de discurso e análise pragmática da linguagem; existe uma suposição de que os seres humanos normalmente utilizam todos esses níveis para produzir ou compreender a linguagem [@Zhao2020].\nAs abordagens de PLN podem ser classificadas em dois grandes grupos: o PLN simbólico e o PLN estatístico [@Zhao2020]. Embora ambos os tipos de PLN tenham sido investigados ao mesmo tempo, foi o PLN simbólico que dominou o campo por algum tempo. Porém, abordagens estatísticas ganharam força principalmente após a divulgação do ChatGPT [@OpenAI2023].\nCom o advento das abordagens estatísticas, em vez de depender exclusivamente de regras feitas à mão, os pesquisadores passaram a explorar os métodos estatísticos e probabilísticos alimentados por grandes coleções de textos. Nessa fase, algoritmos de aprendizado de máquina começaram a extrair padrões e regularidades a partir de grandes textos (também chamados de corpora). Essa abordagem empírica demonstrou ser mais robusta para lidar com a variabilidade intrínseca da língua – afinal, o sistema “aprende” com exemplos reais, capturando nuances que seriam difíceis de enumerar manualmente [@rcelebrone2025]. Técnicas estatísticas impulsionaram aplicações como corretores ortográficos, sistemas de tradução automática e assistentes de voz, marcando uma mudança fundamental de estratégia no campo o PLN.\nMas ainda não podemos descartar a abordagem simbólica - também são chamadas de baseadas em regras explícitas. Elas permitem que linguistas e programadores definiam manualmente gramáticas, ontologias e conjuntos de regras linguísticas para que o computador analise e gera frases corretamente. Essas abordagens conseguiram relativos sucessos em domínios restritos – como no ELIZA - mas mostrou-se limitada. Regras rígidas falham diante das inúmeras ambiguidades da linguagem humana, pois expressões fora do previsto escapam ao alcance dessas soluções programadas na íntegra [@rcelebrone2025]. Sistemas puramente baseados em regras eram inflexíveis frente à variação linguística natural, exigindo extensa manutenção manual para cobrir casos novos.\nVárias dessas técnicas de PLN são utilizadas em diversos chatbots. Desde o momento em que o usuário envia uma mensagem, o bot aplica PLN para entender a intenção por trás das palavras. Isso envolve identificar o que o usuário deseja ou pergunta, mesmo que haja várias maneiras de expressar a mesma coisa. Por exemplo, um assistente virtual deve reconhecer que perguntas como \"Poderia me dizer se vai chover hoje?\" e \"Qual a previsão do tempo para hoje?\" têm a mesma intenção (consulta sobre clima) ainda que usem palavras diferentes. Modelos modernos de compreensão de linguagem conseguem agrupar essas variações e mapear para a intenção correta graças a embeddings e classificadores treinados em múltiplas formulações de consulta [@rcelebrone2025].\nNa construção de chatbots, outra função muito utilizada é processar a entrada bruta do usuário, realizando a limpeza e a preparação dos dados textuais para que o sistema possa interpretar a mensagem e tomar as ações subsequentes apropriadas. Geralmente, o processo envolve a decomposição da linguagem em unidades menores, a compreensão do seu significado intrínseco e a determinação da resposta ou ação mais adequada.\nAlém disso, o PLN ajuda o chatbot a extrair entidades e detalhes relevantes na frase do usuário – como datas, nomes, locais – que permitem contextualizar a solicitação. Esse entendimento refinado reduz drasticamente falhas de comunicação: em vez de responder “Desculpe, não entendi” a cada frase fora do roteiro, o bot passa a lidar bem com sinônimos, gírias e estruturas frasais incomuns, oferecendo respostas pertinentes. Grandes modelos de linguagem já incorporam esse tipo de generalização, o que explica por que assistentes atuais parecem entender até perguntas muito abertas ou indiretas [@rcelebrone2025].\nUsando as técnicas de PLN adequadas os chatbots podem gerar respostas cada vez mais humanas. Nas gerações anteriores, muitos bots recorriam a respostas prontas ou scripts pré-definidos, conforme visto no Eliza, resultando em interações engessadas e repetitivas. Hoje, com os modelos de linguagem Grande (que serão discutidos no próximo capítulo), os chatbots podem produzir respostas inéditas, elaboradas em tempo real. Com isso o chatbot pode responder com frases gramaticalmente corretas, coerentes com o contexto da conversa e até ajustadas ao tom apropriado para o usuário.\nPortanto, neste capítulo conheceremos algumas das principais técnicas individualmente que não são apenas teorias acadêmicas, com elas temos um canivete suíço de partes que serão usadas de acordo com a necessidade do botmaster (o engenheiro que cria o chatbot algumas vezes é chamado de botmaster, enquanto quem conversa com ele é definido como usuário). Porém antes vamos configurar o Python que será a linguagem de programação utilizada durante todo o capítulo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#instalação-do-python",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#instalação-do-python",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.4 Instalação do Python",
    "text": "3.4 Instalação do Python\nNesta seção, abordaremos como preparar o ambiente necessário para trabalhar com vetorização de texto em Python. Isso inclui a instalação do Python e das bibliotecas necessárias, além de uma breve introdução ao uso do Jupyter Notebook.\n\nNOTA SOBRE REPRODUÇÃO DOS CÓDIGOS\nTodos os exemplos de código deste livro foram testados em Python na versão 3.12 na IDE Visual Studio Code em um PC com GPU. Alguns poucos códigos quando especificados foram executados no google Colab. No quadro abaixo do código-fonte é apresentado a saída do console.\n\nPara começar a trabalhar com vetorização de texto, é essencial ter o Python instalado. Python é uma linguagem de programação amplamente utilizada para análise de dados e aprendizado de máquina devido à sua simplicidade e à vasta gama de bibliotecas disponíveis.\nPara instalar o Python, siga as instruções abaixo:\n\nNo Windows, baixe o instalador do site oficial do Python (https://www.python.org/) e siga as instruções do instalador.\nNo macOS, você pode usar o Homebrew para instalar o Python executando o comando brew install python.\nNo Linux, o Python geralmente já está instalado, mas você pode atualizá-lo usando o gerenciador de pacotes da sua distribuição.\n\nUma vez que o Python esteja instalado, precisamos instalar algumas bibliotecas que são fundamentais para a vetorização de texto. Entre as principais estão “NumPy”, “Pandas”, “Scikit-learn”, “NLTK” e “SpaCy” e “Gensim”.\nO “SpaCy” é uma biblioteca de PLN de código aberto em Python, conhecida por sua velocidade e eficiência. O spaCy oferece APIs intuitivas e modelos pré-treinados para diversas tarefas de PLN, incluindo tokenização, POS tagging, lematização, NER e análise de dependências. Sua arquitetura é focada em desempenho para aplicações em produção.\nO “NLTK” (Natural Language Toolkit) é uma biblioteca Python fundamental para PLN, oferecendo uma ampla gama de ferramentas e recursos para tarefas como tokenização, stemming, POS tagging, análise sintática e NER. O NLTK é frequentemente utilizado para fins educacionais e de pesquisa.\nO “Gensim” é uma biblioteca Python especializada em modelagem de tópicos, análise de similaridade semântica e vetores de palavras. Ele é particularmente útil para identificar estruturas semânticas em grandes coleções de texto.\nO “pip” é o gerenciador de pacotes padrão do Python. Você pode instalar as bibliotecas necessárias usando o seguinte comando:\n\npip install numpy pandas scikit-learn nltk spacy gensim\n\nApós instalar as bibliotecas, é importante verificar se elas foram instaladas corretamente:\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport nltk\nimport spacy\nimport gensim\n\nprint(\"NumPy version:\", np.__version__)\nprint(\"Pandas version:\", pd.__version__)\nprint(\"Scikit-learn version:\", sklearn.__version__)\nprint(\"NLTK version:\", nltk.__version__)\nprint(\"SpaCy version:\", spacy.__version__)\nprint(\"Gensim:\", gensim.__version__)\nNumPy version: 1.26.4\nPandas version: 2.2.2\nScikit-learn version: 1.5.1\nNLTK version: 3.9.1\nSpaCy version: 3.7.6\nEste código importará as bibliotecas e exibirá suas versões, garantindo que todas estejam corretamente instaladas.\nO Jupyter Notebook é uma ferramenta poderosa para o desenvolvimento de scripts em Python, permitindo a combinação de código, texto, visualizações e resultados em um único documento.\nVocê pode instalar o Jupyter Notebook usando o pip:\n\npip install jupyterlab\n\nPara iniciar o Jupyter Notebook, execute o seguinte comando no terminal:\n\njupyter notebook\n\nIsso abrirá o Jupyter Notebook no seu navegador padrão, permitindo que você comece a escrever e executar código Python de maneira interativa.\nUm exemplo simples de uso do Jupyter Notebook seria a criação de uma célula de código para calcular a soma de dois números:\n\na = 10\nb = 20\nprint(\"a soma é:\", a + b)\n    a soma é: 30\n\nEste exemplo demonstra a simplicidade e interatividade que o Jupyter Notebook oferece, permitindo que você execute código Python célula por célula e veja os resultados imediatamente. Em resumo, configuramos o ambiente necessário para trabalhar com Python e as principais bibliotecas instaladas, além da configuração do Jupyter Notebook.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#principais-técnicas-de-pln",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#principais-técnicas-de-pln",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.5 Principais Técnicas de PLN",
    "text": "3.5 Principais Técnicas de PLN\n\n3.5.1 Tokenização\nTokenização é o processo de dividir um texto em unidades menores chamadas “tokens” que podem ser palavras, frases ou até mesmo sentenças [@jurafskyspeech]. A tokenização é o primeiro passo em muitos algoritmos de PLN, pois permite que os dados textuais sejam manipulados de forma programática.\nTokenizar não é só separar por espaços, mas também lidar com pontuações, contrações e outros aspectos que podem afetar a análise. Um exemplo simples seria a frase “Eu estou feliz.”, que seria tokenizada em [“Eu”, “estou”, “feliz”, “.”]. Não necessariamente uma palavra equivale a um token. Em alguns casos, como em palavras compostas ou expressões idiomáticas, um único token pode representar uma ideia ou conceito mais amplo. Por exemplo, “São Paulo” poderia ser considerado um único token em vez de dois (“São” e “Paulo”).\nExistem diferentes abordagens para tokenização, incluindo tokenização baseada em regras, onde padrões específicos são definidos para identificar tokens (geralmente utilizando expressões regulares), e tokenização baseada em aprendizado de máquina, onde algoritmos aprendem a segmentar o texto com base em exemplos anteriores.\nA tokenização pode ser feita de várias maneiras, dependendo do idioma e do objetivo da análise. Em inglês, por exemplo, a tokenização pode ser mais simples devido à estrutura gramatical, enquanto em idiomas como o chinês, onde não há espaços entre as palavras, a tokenização pode ser mais complexa.\nA seguir, um exemplo do uso da biblioteca nltk e a tokenização de determinado texto, também chamado de corpus.\n\n# pip install nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n# Exemplo de texto\ntexto = \"Chatbots estão se tornando cada vez mais populares. Eles podem realizar muitas tarefas automaticamente.\"\n\n# Tokenização em palavras\ntokens_palavras = word_tokenize(texto)\nprint(\"Tokens de palavras:\", tokens_palavras)\n\n# Tokenização em sentenças\ntokens_sentencas = sent_tokenize(texto)\nprint(\"Tokens de sentenças:\", tokens_sentencas)\n\n\nTokens de palavras: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.', 'Eles', 'podem', 'realizar', 'muitas', 'tarefas', 'automaticamente', '.']\nTokens de sentenças: ['Chatbots estão se tornando cada vez mais populares.', 'Eles podem realizar muitas tarefas automaticamente.']\n\nSpaCy é uma biblioteca para PLN que oferece uma interface fácil de usar e é otimizada para processamento rápido. É possível também utilizar a biblioteca “spacy”, conforme os códigos a seguir:\n\n# pip install spacy\n# import os\n# os.system(\"python -m spacy download pt_core_news_sm\")\nimport spacy\n\n# Carregar o modelo de linguagem em português\nnlp = spacy.load(\"pt_core_news_sm\")\n\n# Exemplo de texto\ntexto = \"Chatbots estão se tornando cada vez mais populares.\"\n\n# Processando o texto\ndoc = nlp(texto)\n\n# Tokenização\ntokens = [token.text for token in doc]\nprint(\"Tokens:\", tokens)\n\n\nTokens: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.']\n\n\n\n3.5.2 Lematização\nLematização é o processo de reduzir palavras flexionadas ao seu lema, ou forma base [@singh2019building]. Diferente da stemização, a lematização leva em consideração o contexto e a gramática da palavra para obter a forma correta.\n\n# pip install nltk\n# import nltk\n# nltk.download('wordnet')\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\n# Inicializando o lematizador\nlemmatizer = WordNetLemmatizer()\n\n# Exemplo de palavras\npalavras = [\"correndo\", \"correu\", \"corredores\"]\n\n# Lematização das palavras\nlematizadas = [lemmatizer.lemmatize(palavra, pos='v') for palavra in palavras]\nprint(\"Palavras lematizadas:\", lematizadas)\n\n\nPalavras lematizadas: ['correndo', 'correu', 'corredores']\n\nCom spacy\n\n#pip install spacy\nimport spacy\n\n# Carregar o modelo de linguagem em português\nnlp = spacy.load(\"pt_core_news_sm\")\n\n# Exemplo de texto\ntexto = \"Chatbots estão se tornando cada vez mais populares.\"\n\n# Processando o texto\ndoc = nlp(texto)\n\ntokens = [token.text for token in doc]\nprint(\"Tokens:\", tokens)\n\nlematizadas = [token.lemma_ for token in doc]\nprint(\"Palavras lematizadas:\", lematizadas)\n\n\nTokens: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.']\nPalavras lematizadas: ['chatbots', 'estar', 'se', 'tornar', 'cada', 'vez', 'mais', 'popular', '.']\n\n\n\n3.5.3 Stemização\nA stemização é o processo de reduzir as palavras às suas raízes ou “stems”. É uma técnica mais simples que a lematização e, geralmente, não considera o contexto, o que pode levar a resultados menos precisos.\n\n# pip install nltk\nfrom nltk.stem import PorterStemmer\n\n# Inicializando o stemizador\nstemmer = PorterStemmer()\n\n# Exemplo de palavras\npalavras = [\"correndo\", \"correu\", \"corredores\"]\n\n# Stemização das palavras\nstems = [stemmer.stem(palavra) for palavra in palavras]\nprint(\"Stems das palavras:\", stems)\n\n\nStems das palavras: ['correndo', 'correu', 'corredor']\n\n\n\n3.5.4 Stopwords\nStopwords são palavras comuns em um idioma (como “o”, “a”, “e”, “de”, “que”, etc.) que geralmente são removidas durante o pré-processamento de texto, pois não contribuem significativamente para o significado do texto [@Raj2019]. A remoção dessas palavras pode melhorar a eficácia de certos algoritmos de PLN, focando nas palavras mais informativas do texto.\n\n# pip install nltk\n# import nltk\n# nltk.download('stopwords')\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Carregar stopwords para o idioma portugues\nstop_words = set(stopwords.words('portuguese'))\n\n# Exemplo de texto\ntexto = \"Chatbots estão se tornando cada vez mais populares.\"\n\n# Removendo stopwords\ntokens_sem_stopwords = [palavra for palavra in word_tokenize(texto) if palavra.lower() not in stop_words]\nprint(\"Texto sem stopwords:\", tokens_sem_stopwords)\n\n\nTexto sem stopwords: ['Chatbots', 'tornando', 'cada', 'vez', 'populares', '.']\n\nCom a biblioteca Spacy:\n\n\n# pip install spacy\n# python -m spacy download pt_core_news_sm\nimport spacy\n\n# Carregar o modelo de linguagem em português\nnlp = spacy.load(\"pt_core_news_sm\")\n\n# Exemplo de texto\ntexto = \"Chatbots estão se tornando cada vez mais populares.\"\n\n# Processando o texto\ndoc = nlp(texto)\n\ntokens = [token.text for token in doc]\nprint(\"Tokens:\", tokens)\n\n# Removendo stopwords\ntokens_sem_stopwords = [token.text for token in doc if not token.is_stop]\nprint(\"Texto sem stopwords:\", tokens_sem_stopwords)\n\n\nTokens: ['Chatbots', 'estão', 'se', 'tornando', 'cada', 'vez', 'mais', 'populares', '.']\nTexto sem stopwords: ['Chatbots', 'tornando', 'populares', '.']",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#outras-técnicas-de-pln",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#outras-técnicas-de-pln",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.6 Outras técnicas de PLN",
    "text": "3.6 Outras técnicas de PLN\nMarcação Morfossintática (POS Tagging): Esta técnica consiste em atribuir a cada token em um texto uma categoria gramatical, como substantivo, verbo, adjetivo, advérbio, etc. A marcação POS é utilizada para identificar entidades e compreender a estrutura gramatical das frases. Por exemplo, na frase “Eu estou aprendendo como construir chatbots”, a marcação POS poderia identificar “Eu” como um pronome (PRON), “estou aprendendo” como um verbo (VERB) e “chatbots” como um substantivo (NOUN).\nReconhecimento de Entidades Nomeadas (NER): O NER é a tarefa de identificar e classificar entidades nomeadas em um texto, como nomes de pessoas (PERSON), organizações (ORG), localizações geográficas (GPE, LOC), datas (DATE), valores monetários (MONEY), etc. Por exemplo, na frase “Google tem sua sede em Mountain View, Califórnia, com uma receita de 109,65 bilhões de dólares americanos”, o NER identificaria “Google” como uma organização (ORG), “Mountain View” e “Califórnia” como localizações geográficas (GPE) e “109,65 bilhões de dólares americanos” como um valor monetário (MONEY). Essa capacidade é vital para que chatbots compreendam os detalhes relevantes nas utterances dos usuários.\nAnálise de Dependências (Dependency Parsing): Esta técnica examina as relações gramaticais entre as palavras em uma frase, revelando a estrutura sintática e as dependências entre os tokens. A análise de dependências pode ajudar a entender quem está fazendo o quê a quem. Por exemplo, na frase “Reserve um voo de Corumbá para Maceió”, a análise de dependências pode identificar “Corumbá” e “Maceió” como modificadores de “voo” através das preposições “de” e “para”, respectivamente, e “Reserve” como a raiz da ação. Essa análise é útil para extrair informações sobre as intenções do usuário, mesmo em frases mais complexas.\nClassificação de Texto: Uma técnica de aprendizado de máquina que atribui um texto a uma ou mais categorias predefinidas. No contexto de chatbots, a classificação de texto é fundamental para a detecção de intenção (mais sobre detecção de intenção será apresentada nas seções seguintes), onde as categorias representam as diferentes intenções do usuário. Algoritmos como o Naïve Bayes são modelos estatísticos populares para essa tarefa, baseados no teorema de Bayes e em fortes suposições de independência entre as características. O treinamento desses classificadores requer um corpus de dados rotulados, onde cada utterance (entrada do usuário) é associada a uma intenção específica.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#expressões-regulares",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#expressões-regulares",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.7 Expressões Regulares",
    "text": "3.7 Expressões Regulares\nExpressões regulares, frequentemente abreviadas como regex, são sequências de caracteres que definem padrões de busca. Elas são utilizadas em chatbots para diversas tarefas relacionadas ao processamento e à análise de texto fornecido pelos usuários.\nFrameworks populares para desenvolvimento de chatbots frequentemente integram o uso de expressões regulares para aprimorar a extração de entidades. Por exemplo, as regex podem ser definidas nos dados de treinamento para ajudar o sistema a reconhecer padrões específicos como nomes de ruas ou códigos de produtos. Essa abordagem permite melhorar a precisão do reconhecimento de entidades, um componente importante para a compreensão da intenção do usuário.\nA sintaxe das expressões regulares consiste em uma combinação de caracteres literais (que correspondem a si mesmos) e metacaracteres, que possuem significados especiais e permitem definir padrões de busca mais complexos. As expressões regulares podem ser aplicadas em uma variedade de cenários no desenvolvimento de chatbots. Algumas das aplicações de regex nos chatbots incluem:\n\nExtração de entidades: Identificação e extração de informações específicas, como endereços de e-mail, números de telefone, datas e outros dados estruturados presentes na entrada do usuário.\nValidação de entradas do usuário: Verificação se a entrada do usuário corresponde a um formato esperado, como datas em um formato específico (DD/MM/AAAA), códigos postais ou outros padrões predefinidos.\nDetecção de Intenção: Detecção de comandos específicos inseridos pelo usuário, como /ajuda, /iniciar ou palavras-chave que indicam uma intenção específica.\nLimpeza de texto: Remoção de ruídos e elementos indesejados do texto, como tags HTML, espaços em branco excessivos ou caracteres especiais que podem interferir no processamento subsequente.\nTokenização simples: Embora métodos mais avançados sejam comuns em PLN, regex pode ser usada para dividir o texto em unidades menores (tokens) com base em padrões simples.\n\nEssas tarefas são fundamentais para garantir que o chatbot possa interpretar e responder adequadamente às entradas dos usuários, especialmente em cenários onde a informação precisa ser estruturada ou verificada antes de ser processada por modelos de linguagem mais complexos. Cada uma destas tarefas será detalhada neste Capítulo.\nO módulo re em Python é a biblioteca padrão para trabalhar com expressões regulares. Ele fornece diversas funções que permitem realizar operações de busca, correspondência e substituição em strings com base em padrões definidos por regex. Algumas das funções mais utilizadas incluem:\n\nre.match(pattern, string): Tenta encontrar uma correspondência do padrão no início da string. Se uma correspondência for encontrada, retorna um objeto de correspondência; caso contrário, retorna None.\nre.search(pattern, string): Procura a primeira ocorrência do padrão em qualquer posição da string. Retorna um objeto de correspondência se encontrado, ou None caso contrário.\nre.findall(pattern, string): Encontra todas as ocorrências não sobrepostas do padrão na string e as retorna como uma lista de strings.\nre.sub(pattern, repl, string): Substitui todas as ocorrências do padrão na string pela string de substituição repl. Retorna a nova string resultante.\n\n\nExtração de E-mails\nUm caso de uso comum em chatbots é a extração de endereços de e-mail do texto fornecido pelo usuário. O seguinte exemplo em Python demonstra como usar re.findall para realizar essa tarefa. Ilustrando o uso regex para identificar e extrair informações específicas de um texto.\n\nimport re\ntexto = \"Entre em contato em exemplo@email.com ou suporte@outroemail.com.\"\npadrao = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-] +\\.[A-Z|a-z]{2,}\\b'\nemails = re.findall(padrao, texto)\nprint(emails)\n\n\n['exemplo@email.com', 'suporte@outroemail.com']\n\n\n\nValidação de Datas\nChatbots que lidam com agendamentos ou reservas frequentemente precisam validar se a data fornecida pelo usuário está em um formato correto. O seguinte exemplo demonstra como validar datas no formato DD/MM/AAAA:\n\nimport re\n\npadrao_data = r'\\b\\d{2}/\\d{2}/\\d{4}\\b'\ndatas_teste = [\"31/12/2020\", \"1/1/2021\", \"2023-05-10\", \"25/06/2025 10:00\"]\n\nfor data in datas_teste:\n  if re.match(padrao_data, data):\n    print(f\"'{data}' é uma data válida no formato DD/MM/AAAA.\")\n  else:\n    print(f\"'{data}' não é uma data válida no formato DD/MM/AAAA.\")\n\nA saída deste código ilustra quais das strings de teste correspondem ao padrão de data especificado.\n\n'31/12/2020' é uma data válida no formato DD/MM/AAAA.\n'1/1/2021' não é uma data válida no formato DD/MM/AAAA.\n'2023-05-10' não é uma data válida no formato DD/MM/AAAA.\n'25/06/2025 10:00' é uma data válida no formato DD/MM/AAAA.\n\n\n\nAnálise de Comandos\nEm interfaces de chatbot baseadas em texto, os usuários podem interagir através de comandos específicos, como /ajuda ou /iniciar. As regex podem ser usadas para detectar esses comandos de forma eficiente. Este exemplo abaixo mostra como identificar strings que começam com uma barra seguida por um ou mais caracteres alfanuméricos.\n\nimport re\n\npadrao_comando = r'^/\\w+'\ncomandos_teste = [\"/ajuda\", \"/iniciar\", \"ajuda\", \"iniciar/\"]\n\nfor comando in comandos_teste:\n  if re.match(padrao_comando, comando):\n    print(f\"'{comando}' é um comando válido.\")\n  else:\n    print(f\"'{comando}' não é um comando válido.\")\n\n\n'/ajuda' é um comando válido.\n'/iniciar' é um comando válido.\n'ajuda' não é um comando válido.\n'iniciar/' não é um comando válido.\n\n\n\nTokenização Simples\nEmbora para tarefas complexas de PLN sejam utilizadas técnicas de tokenização mais avançadas, as regex podem ser úteis para realizar uma tokenização básica, dividindo o texto em palavras ou unidades menores com base em padrões de separação. A saída será uma lista de strings, onde o padrão \\W+ corresponde a um ou mais caracteres não alfanuméricos, utilizados como delimitadores.\n\nimport re\ntexto = \"Olá, como vai você?\"\ntokens = re.split(r'\\W+', texto)\nprint(tokens)\n\n\n['Olá', 'como', 'vai', 'você', '']\n\n\n\nLimpeza de Texto\nChatbots podem precisar processar texto que contém elementos indesejados, como tags HTML. As regex podem ser usadas para remover esses elementos:\n\nimport re\ntexto_html = \"&lt;p&gt;Este é um parágrafo com &lt;b&gt;texto em negrito&lt;/b&gt;.&lt;/p&gt;\"\ntexto_limpo = re.sub(r'&lt;[^&gt;]+&gt;', '', texto_html)\nprint(texto_limpo)\n\n\nEste é um parágrafo com texto em negrito.\n\nEmbora os fundamentos das regex sejam suficientes para muitas tarefas, existem construções mais avançadas que podem ser úteis em cenários complexos. Alguns exemplos incluem Lookaheads e Lookbehinds que permitem verificar se um padrão é seguido ou precedido por outro padrão, sem incluir esse outro padrão na correspondência; além disso, correspondência não-gulosa: que ao usar quantificadores como * ou +, a correspondência padrão é \"gulosa\", ou seja, tenta corresponder à maior string possível. Adicionar um ? após o quantificador (*?, +?) torna a correspondência \"não-gulosa\", correspondendo à menor string possível.\nÉ importante reconhecer que, apesar de sua utilidade, as expressões regulares têm limitações significativas quando se trata de compreender a complexidade da linguagem natural. As regex são baseadas em padrões estáticos e não possuem a capacidade de entender o contexto, a semântica ou as nuances da linguagem humana.\nNo contexto de um fluxo de trabalho de chatbot, as expressões regulares são frequentemente mais eficazes nas etapas de pré-processamento, como limpeza e validação de entradas, enquanto técnicas de PLN mais sofisticadas são empregadas para a compreensão da linguagem em um nível mais alto.\nPorém, para tarefas que exigem uma compreensão mais profunda do significado e da intenção por trás das palavras, técnicas avançadas de Processamento de Linguagem Natural (PLN), como modelagem de linguagem, análise de sentimentos e reconhecimento de entidades nomeadas (NER) baseadas em aprendizado de máquina, são indispensáveis e serão apresentadas no Capítulo 4.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#entendimento-de-linguagem-natural",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#entendimento-de-linguagem-natural",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.8 Entendimento de Linguagem Natural",
    "text": "3.8 Entendimento de Linguagem Natural\nO Entendimento de Linguagem Natural (ULN) é um subdomínio específico dentro do universo mais vasto do PLN. Enquanto o PLN abarca um conjunto diversificado de operações sobre a linguagem, o ULN se concentra de maneira particular na habilidade da máquina de aprender e interpretar a linguagem natural tal como ela é comunicada pelos seres humanos. Em outras palavras, o ULN é o ramo do PLN dedicado à extração de significado e à identificação da intenção por trás do texto inserido pelo usuário. As aplicações do ULN são extensas e incluem funcionalidades cruciais para chatbots, como a capacidade de responder a perguntas, realizar buscas em linguagem natural, identificar relações entre entidades, analisar o sentimento expresso no texto, sumarizar informações textuais e auxiliar em processos de descoberta legal.\nNo cerne da funcionalidade de um chatbot reside a sua capacidade de compreender as mensagens dos usuários e responder de forma adequada. O PLN desempenha um papel central nesse processo, permitindo que o chatbot: (i) detecte a intenção do usuário; (ii) extraia entidades relevantes; e (iii) processe linguagem variada e informal; e (iv) mantenha o contexto da conversa.\n(i) Detecção da Intenção do Usuário: O objetivo por trás da mensagem do usuário é o primeiro passo. Isso é frequentemente abordado como um problema de classificação de texto, onde o chatbot tenta classificar a utterance do usuário em uma das intenções predefinidas. Para detecção da intenção, é possível utilizar técnicas de aprendizado de máquina, como o algoritmo Naïve Bayes, para construir esses classificadores. Plataformas como Rasa NLU (https://rasa.com/), LUIS.ai (https://www.luis.ai/) e Dialogflow (https://dialogflow.cloud.google.com/) simplificam significativamente o processo de treinamento e implantação desses modelos de intenção.\nO Rasa NLU é um componente de código aberto do framework Rasa para construir chatbots, focado em entendimento de linguagem natural. Ele permite treinar modelos personalizados para classificação de intenção e extração de entidades, oferecendo flexibilidade e controle sobre os dados.\n(ii) Extraia Entidades Relevantes: Além da intenção geral, as mensagens dos usuários frequentemente contêm detalhes específicos, conhecidos como entidades, que são essenciais para atender à solicitação. Por exemplo, em “Reserve um voo de Corumbá para Maceió amanhã”, a intenção é reservar um voo, e as entidades são a cidade de origem (“Corumbá”), a cidade de destino (“Maceió”) e a data (“amanhã”). As técnicas de NER e os modelos de extração de entidades fornecidos por ferramentas como spaCy, NLTK, CoreNLP, LUIS.ai e Rasa NLU são fundamentais para identificar e extrair essas informações.\nO CoreNLP é um conjunto de ferramentas de PLN robusto e amplamente utilizado, desenvolvido em Java. O CoreNLP oferece capacidades abrangentes de análise linguística, incluindo POS tagging, análise de dependências, NER e análise de sentimentos. Possui APIs para integração com diversas linguagens de programação, incluindo Python.\n(iii) Processe Linguagem Variada e Informal: Os usuários podem se comunicar com chatbots usando uma ampla gama de vocabulário, gramática e estilo, incluindo erros de digitação, abreviações e linguagem informal. As técnicas de PLN, como stemming, lematização e busca por similaridade, ajudam o chatbot a lidar com essa variabilidade e a compreender a essência da mensagem, mesmo que não seja expressa de forma perfeitamente gramatical.\nA arquitetura típica de um chatbot envolve uma camada de processamento de linguagem natural (NLP/NLU engine) que recebe a entrada de texto do usuário. Essa camada é responsável por realizar as tarefas de PLN mencionadas anteriormente: tokenização, análise morfossintática, extração de entidades, detecção de intenção, etc. O resultado desse processamento é uma representação estruturada da mensagem do usuário, que pode ser entendida pela lógica de negócios do chatbot.\nCom base nessa representação estruturada, um motor de decisão (decision engine) no chatbot pode então corresponder à intenção do usuário a fluxos de trabalho preconfigurados ou a regras de negócio específicas. Em alguns casos, a geração de linguagem natural (NLG), outro subcampo do PLN, é utilizada para formular a resposta do chatbot ao usuário.\n\n3.8.1 Intents\nOs Intents representam a intenção ou o propósito por trás da mensagem de um usuário ao interagir com o chatbot [@Khan2018]. Em termos mais simples, é o que o usuário deseja que o chatbot faça ou sobre o que ele quer saber.\nUm intent é usado para identificar programaticamente a intenção da pessoa que está usando o chatbot. O chatbot deve ser capaz de executar alguma ação com base no “intent” que detecta na mensagem do usuário. Cada tarefa que o chatbot deve realizar define um intent. A aplicação prática dos intents varia conforme o domínio do chatbot, veja o exemplo:\nPor exemplo, para um chatbot de uma loja de moda, exemplos de intents seriam “busca de um produto” (quando um usuário quer ver produtos) e “endereço loja” (quando um usuário pergunta sobre lojas); em um chatbot para pedir comida, “consultar preços” e “realizar pedido” podem ser intents distintos.\nDetectar o intent da mensagem do usuário é um problema conhecido de aprendizado de máquina, realizado por meio de uma técnica chamada classificação de texto. O objetivo é classificar frases em múltiplas classes (os intents). O modelo de aprendizado de máquina é treinado com um conjunto de dados que contém exemplos de mensagens e seus intents correspondentes. Após o treinamento, o modelo pode prever o intent de novas mensagens que não foram vistas antes.\n\n\n3.8.2 Utterances\nCada intent pode ser expresso de várias maneiras pelo usuário. Essas diferentes formas são chamadas de utterances ou expressões do usuário.\nPor exemplo, para o intent realizar pedido, as utterances poderiam ser “Eu gostaria de fazer um pedido”, “Quero pedir comida”, “Como faço para pedir?”, etc. Cada uma dessas expressões representa a mesma intenção, mas com palavras diferentes. O modelo de aprendizado de máquina deve ser capaz de reconhecer todas essas variações como pertencentes ao mesmo intent.\nÉ sugerido fornecer um número ótimo de utterances variadas por intent para garantir um bom treinamento do modelo de reconhecimento.\n\n\n3.8.3 Entities\nAs entidades extraídas permitem ao chatbot refinar sua resposta ou ação. Os Intents frequentemente contêm metadados importantes chamados Entities. Estas são palavras-chave ou frases dentro da utterance do usuário que ajudam o chatbot a identificar detalhes específicos sobre o pedido, permitindo fornecer informações mais direcionadas. Por exemplo, na frase “Eu quero pedir uma pizza de calabreza com borda recheada”, as entidades podem incluir: Para o intent realizar pedido e para as Entities: pizza, calabreza e borda recheada.\n\n\n3.8.4 Desafios\nO processo de treinamento envolve a construção de um modelo de aprendizado de máquina. Este modelo aprende a partir do conjunto definido de intents, suas utterances associadas e as entidades anotadas. O objetivo do treinamento é capacitar o modelo a categorizar corretamente novas utterances (que não foram vistas durante o treinamento) no intent apropriado e a extrair as entidades relevantes.\nQuando o chatbot processa uma nova mensagem do usuário, o modelo de reconhecimento de intent não apenas classifica a mensagem em um dos intents definidos, mas também fornece uma pontuação de confiança (geralmente entre 0 e 1). Essa pontuação indica o quão seguro o modelo está e que a classificação está correta. É comum definir um limite (threshold) de confiança. Se a pontuação do intent detectado estiver abaixo desse limite, o chatbot pode pedir esclarecimentos ao usuário em vez de executar uma ação baseada em uma suposição incerta.\nUma vez que um intent é detectado com confiança suficiente, o chatbot pode executar a ação correspondente. Isso pode envolver consultar um banco de dados, chamar uma API externa, fornecer uma resposta estática ou iniciar um fluxo de diálogo mais complexo. Além disso, a análise dos intents mais frequentemente capturados fornece insights valiosos sobre como os usuários estão interagindo com o chatbot e quais são suas principais necessidades. Essas análises são importantes tanto para a otimização do bot quanto para as decisões de negócio.\nApesar destes procedimentos, alguns problemas ainda desafiam os pesquisadores, tais como a Geração de texto coerente; a Sintaxe e gramática; a semântica; o Contexto; e a Ambiguidade.\nA geração de texto coerente é um desafio porque envolve não apenas a escolha de palavras, mas também a construção de frases que façam sentido no contexto da conversa. A sintaxe e gramática são importantes para garantir que o texto gerado seja gramaticalmente correto e compreensível. A semântica se refere ao significado das palavras e frases, e é importante para garantir que o texto gerado transmita a mensagem correta. O contexto é importante para entender o que foi dito anteriormente na conversa e como isso afeta a resposta atual. A ambiguidade pode surgir quando uma palavra ou frase tem múltiplos significados, tornando difícil para o modelo determinar qual interpretação é a correta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#vetorização-e-representação-de-texto",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#vetorização-e-representação-de-texto",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.9 Vetorização e Representação de Texto",
    "text": "3.9 Vetorização e Representação de Texto\nA vetorização de texto é um passo importante no Processamento de Linguagem Natural (PLN), pois permite transformar dados textuais em uma forma que os modelos de aprendizado de máquina podem entender e processar. Nesta seção, exploraremos algumas técnicas de vetorização, incluindo One-Hot Encoding, Bag of Words e TF-IDF (Term Frequency-Inverse Document Frequency), juntamente com implementações práticas em Python.\n\n3.9.1 One-Hot Encoding\nOne-Hot Encoding é uma das formas mais simples de vetorização de texto, onde cada palavra em um vocabulário é representada como um vetor binário. Cada posição no vetor corresponde a uma palavra do vocabulário, e apenas a posição da palavra que está sendo representada tem valor 1, enquanto todas as outras têm valor 0.\n\n# pip install scikit-learn\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\n# Exemplo de texto\ncorpus = [\"Eu amo programação\", \"A programação é divertida\"]\n\n# Tokenização simples\ntokenized_corpus = [sentence.split() for sentence in corpus]\n# Flatten para obter todas as palavras\nall_words = [word for sentence in tokenized_corpus for word in sentence]\n# Remover duplicatas\nvocab = list(set(all_words))\n\n# Criar matriz de índices das palavras para cada frase\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\ncorpus_idx = [[word_to_idx[word] for word in sentence] for sentence in tokenized_corpus]\n\n# Ajustar o encoder para o vocabulário\nencoder = OneHotEncoder(sparse_output=False)\none_hot_encoded = encoder.fit_transform(\n        np.array(vocab).reshape(-1, 1))\n\nprint(\"Vocabulário:\", vocab)\nprint(\"One-Hot Encoding:\\n\", one_hot_encoded)\n\n\nVocabulário: ['amo', 'é', 'programação', 'Eu', 'A', 'divertida']\nOne-Hot Encoding:\n[[0. 0. 1. 0. 0. 0.]\n[0. 0. 0. 0. 0. 1.]\n[0. 0. 0. 0. 1. 0.]\n[0. 1. 0. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0.]\n[0. 0. 0. 1. 0. 0.]]\n\nEmbora seja fácil de entender e implementar, o One-Hot Encoding não leva em consideração a semântica das palavras, resultando em vetores muito esparsos e de alta dimensionalidade, especialmente para vocabulários grandes.\n\n\n3.9.2 Bag of Words (BoW)\nA técnica de Bag of Words (BoW) é uma melhoria em relação ao One-Hot Encoding. Aqui, em vez de vetores binários, usamos contagens de palavras. Para cada documento, construímos um vetor com a frequência de cada palavra no vocabulário.\n\n# pip install scikit-learn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Exemplo de corpus\ncorpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu amo a vida\"]\n\n# Criação do vetor de contagem (Bag of Words)\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(corpus)\n\nprint(\"Vocabulário:\", vectorizer.get_feature_names_out())\nprint(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())\n\n\nVocabulário: ['amo' 'divertida' 'eu' 'programação' 'vida']\nBag of Words Matrix:\n[[1 0 1 1 0]\n[0 1 0 1 0]\n[1 0 1 0 1]]\n\nA matriz resultante tem cada linha representando um documento e cada coluna representando a contagem de uma palavra específica no documento. No entanto, essa técnica também não leva em consideração o contexto ou a ordem das palavras.\n\n\n3.9.3 TF-IDF\nTF-IDF é uma técnica que combina a frequência de termos (TF) com a frequência inversa de documentos (IDF). Esta abordagem ajuda a dar mais peso a palavras que são raras no corpus, mas aparecem frequentemente em um documento específico, o que geralmente indica sua importância.\n\n# pip install scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Exemplo de corpus\ncorpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu amo a vida\"]\n\n# Criação do vetor TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n\nprint(\"Vocabulário:\", tfidf_vectorizer.get_feature_names_out())\nprint(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n\n\nVocabulário: ['amo' 'divertida' 'eu' 'programação' 'vida']\nTF-IDF Matrix:\n[[0.57735027 0.         0.57735027 0.57735027 0.        ]\n[0.         0.79596054 0.         0.60534851 0.        ]\n[0.51785612 0.         0.51785612 0.         0.68091856]]\n\nA matriz TF-IDF dá valores diferentes para as palavras, não apenas com base em sua frequência, mas também na relevância, ajudando a reduzir o peso de palavras comuns que não são muito informativas para o contexto.\n\n\n3.9.4 Comparação de Técnicas de Vetorização\nAs três técnicas abordadas têm seus prós e contras:\n\nOne-Hot Encoding: Simples e fácil de implementar, mas resulta em vetores esparsos e de alta dimensionalidade.\nBag of Words: Considera a frequência das palavras, mas ainda assim é cego ao contexto e a semântica.\nTF-IDF: Reduz o impacto de palavras comuns e realça palavras que são mais informativas para cada documento.\n\nA escolha da técnica depende do caso de uso específico. Por exemplo, para tarefas simples de classificação de texto, BoW ou TF-IDF podem ser suficientes. Entretanto, para tarefas que exigem uma compreensão mais profunda do contexto, abordagens mais avançadas como Word2Vec ou embeddings de palavras podem ser mais adequadas.\n\n\n3.9.5 Implementação em Projetos Reais\nA vetorização de texto é frequentemente usada em pipelines de PLN para tarefas como classificação de texto, análise de sentimentos e sistemas de recomendação. Vamos ver um exemplo simples de como essas técnicas podem ser integradas em um pipeline completo.\n\n# pip install scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Exemplo de corpus e rótulos\ncorpus = [\"Eu amo programação\", \"A programação é divertida\", \"Eu odeio bugs\", \"A vida é bela\", \"Eu odeio erros\"]\nlabels = [1, 1, 0, 1, 0]  # 1: Sentimento Positivo, 0: Sentimento Negativo\n\n# Divisão dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.4, random_state=42)\n\n# Criação do pipeline TF-IDF + Classificador Naive Bayes\nmodel = make_pipeline(TfidfVectorizer(), MultinomialNB())\n\n# Treinamento do modelo\nmodel.fit(X_train, y_train)\n\n# Predição no conjunto de teste\npredicted = model.predict(X_test)\n\n# Avaliação do modelo\nprint(\"Relatório de Classificação:\\n\", metrics.classification_report(y_test, predicted))\n\n\nRelatório de Classificação:\nprecision     recall  f1-score  support\n0             0.00    0.00      0.00         1\n1             0.50    1.00      0.67         1\naccuracy                        0.50         2\nmacro avg     0.25    0.50      0.33         2\nweighted avg  0.25    0.50      0.33         2\n\nEste exemplo mostra como transformar texto em vetores utilizando TF-IDF e depois aplicar um modelo de classificação simples, como o Naive Bayes. Esse pipeline pode ser adaptado para tarefas mais complexas, incluindo a integração de vetorização com modelos mais avançados.\n\n\n3.9.6 Embeddings de Palavras\nEmbeddings de palavras são representações vetoriais densas de palavras que capturam as relações semânticas entre elas. Ao contrário de técnicas como One-Hot Encoding ou Bag of Words, que resultam em vetores esparsos e de alta dimensionalidade, embeddings de palavras mapeiam palavras em um espaço vetorial de dimensões mais baixas, onde palavras com significados semelhantes estão mais próximas. Elas têm várias aplicações em PLN, tais como:\n\nClassificação de texto: Representar documentos usando a média dos embeddings das palavras contidas nele.\nAnálise de sentimentos: Capturar nuances semânticas que necessárias para entender o sentimento.\nSistemas de recomendação: Usar embeddings para medir similaridade semântica entre produtos ou serviços descritos em texto.\n\n\n\n3.9.7 Word2Vec\nWord2Vec, introduzido por @mikolov2013efficient em 2013, é uma das técnicas mais influentes para aprender embeddings de palavras. Existem duas abordagens principais no Word2Vec: Skip-gram e CBOW (Continuous Bag of Words).\nSkip-gram: O modelo Skip-gram prevê as palavras contextuais (palavras ao redor) para uma palavra-alvo. A ideia é maximizar a probabilidade de prever palavras no contexto de uma palavra-alvo específica.\nCBOW (Continuous Bag of Words): O modelo CBOW, ao contrário do Skip-gram, prevê a palavra-alvo com base no contexto. Este modelo é útil para capturar o sentido de uma palavra com base em seu ambiente.\nImplementação do Word2Vec em Python: Vamos utilizar a biblioteca gensim, que fornece uma implementação eficiente do Word2Vec.\n\n# pip install gensim nltk   \nfrom gensim.models import Word2Vec\nfrom nltk.corpus import brown\n\n# Carregar o corpus de exemplo (Brown corpus)\nsentences = brown.sents(categories='news')\n\n# Treinamento do modelo Word2Vec\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=0)\n\n# Obtenção de vetor para uma palavra\nvector = model.wv['economy']\nprint(\"Vetor para 'economy':\", vector)\n\n# Encontrando palavras semelhantes\nsimilar_words = model.wv.most_similar('economy')\nprint(\"Palavras semelhantes a 'economy':\", similar_words)\n\n\nVetor para 'economy': [-0.03323277  0.03501032 -0.00576794  0.0111884  -0.03380066 -0.11331949\n0.01821606  0.10907217 -0.07971378 -0.06527615  0.03423027 -0.05620026\n[.....]\n0.08788168  0.04192578 -0.03881291 -0.02442176  0.15587321  0.05784223\n0.05759267 -0.07984415 -0.02128893 -0.00980771]\nPalavras semelhantes a 'economy': [('additional', 0.9950711727142334), ('provide', 0.9949660301208496), \n('administration', 0.9946332573890686), ('times', 0.9946231245994568), ('can', 0.9946150183677673), \n('laws', 0.9945879578590393), ('business', 0.9945263266563416), ('could', 0.9944885969161987), \n('how', 0.9944777488708496), ('made', 0.994467556476593)]\n\nParâmetros Importantes - vector_size: Dimensionalidade dos vetores de palavras. - window: Tamanho da janela de contexto. - min_count: Mínimo de ocorrências para uma palavra ser considerada no treinamento. - sg: Se 0, usa CBOW; se 1, usa Skip-gram.\nExplorando as Relações Semânticas\nEmbeddings de palavras como o Word2Vec capturam relações semânticas interessantes, como analogias.\n\n# pip install gensim nltk   \nfrom gensim.models import Word2Vec\nfrom nltk.corpus import brown\n\n# Carregar o corpus de exemplo (Brown corpus)\nsentences = brown.sents(categories='news')\n\n# Treinamento do modelo Word2Vec\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n\n# Analogias: \"rei\" - \"homem\" + \"mulher\" = ?\nresult = model.wv.most_similar(positive=['king', 'woman'], negative=['man'])\nprint(\"Resultado da analogia:\", result)\n\n\nResultado da analogia: [('kid', 0.8575360178947449), ('missed', 0.8566135764122009), ('load', 0.854703426361084), ('normally', 0.8536292314529419), ('advisers', 0.8517050743103027), ('Place', 0.8516084551811218), ('Vieth', 0.8513892292976379), ('yield', 0.8511804938316345), ('decline', 0.8500499725341797), ('Coe', 0.8490973114967346)]\n\n\n\n3.9.8 GloVe\nEmbora o Word2Vec seja amplamente utilizado, outros modelos também oferecem técnicas avançadas para aprender embeddings de palavras. O GloVe, desenvolvido por @pennington2014glove. em 2014, é uma abordagem baseada em matrizes de ocorrência que combina as vantagens do Word2Vec com informações globais sobre o corpus. Este código a seguir demora um pouco; ele fará o download de um arquivo para seu computador, se estiver usando o Python localmente.\n\n# pip install requests\n# pip install gensim\nimport os\nimport requests\nimport zipfile\nfrom gensim.models import KeyedVectors\n\nurl = \"https://nlp.stanford.edu/data/glove.6B.zip\"\nzip_filename = \"glove.6B.zip\"\ntxt_filename = \"glove.6B.50d.txt\"\n\n# Baixar o arquivo zip se não existir\nif not os.path.exists(zip_filename):\n    print(\"Baixando embeddings GloVe 6B...\")\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(zip_filename, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    print(\"Download concluído.\")\n\n# Extrair o arquivo txt se não existir\nif not os.path.exists(txt_filename):\n    print(\"Extraindo glove.6B.50d.txt...\")\n    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n        zip_ref.extract(txt_filename)\n    print(\"Extração concluída.\")\n\n# Carregar embeddings GloVe\nglove_model = KeyedVectors.load_word2vec_format(txt_filename, binary=False, no_header=True)\n\n# Obtenção de vetor para uma palavra\nvector_glove = glove_model['economy']\nprint(\"Vetor para 'economy' com GloVe:\", vector_glove)\n\n\nConversão concluída.\nVetor para 'economy' com GloVe: [-1.2027e-01 -7.2505e-01  8.7014e-01 -6.3944e-01  1.7259e-01 -3.5168e-01 [...] 4.9483e-01 -1.0151e+00  8.9959e-02  4.6090e-01  1.7585e-03  6.2182e-01\n1.1893e+00  8.4410e-02]\n\n\n\n3.9.9 FastText\nO FastText, desenvolvido pelo Facebook AI Research, estende o Word2Vec ao considerar subpalavras, o que permite gerar embeddings para palavras que não estão no vocabulário, lidando melhor com palavras raras e morfologicamente ricas.\n\n# pip install gensim nltk\nimport gensim\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import brown\n\n# Carregar o corpus de exemplo (Brown corpus)\nsentences = brown.sents(categories='news')\n\n# Treinamento do modelo FastText\nfasttext_model = gensim.models.FastText(sentences, vector_size=100, window=5, min_count=5)\n\n# Obtenção de vetor para uma palavra\nvector_fasttext = fasttext_model.wv['economy']\nprint(\"Vetor para 'economy' com FastText:\", vector_fasttext)\n\n# Encontrando palavras semelhantes\nsimilar_words_ft = fasttext_model.wv.most_similar('economy')\nprint(\"Palavras semelhantes a 'economy' com FastText:\", similar_words_ft)\n\n\nVetor para 'economy' com FastText: [-7.31775016e-02  2.45773837e-01 -6.83162957e-02 -2.00441748e-01\n5.88673912e-02  4.55363989e-02 -7.13645061e-03  1.42574251e-01\n[...]\n9.44181085e-02 -1.94346443e-01 -2.52598636e-02  5.90402260e-02]\nPalavras semelhantes a 'economy' com FastText: [('economic', 0.9999745488166809), ('province', 0.9999632835388184), ('recommended', 0.9999628067016602), ('secretary', 0.9999626278877258), ('generally', 0.9999624490737915), ('defensive', 0.9999622106552124), ('often', 0.9999621510505676), ('concerned', 0.9999621510505676), ('maintenance', 0.9999617338180542), ('primary', 0.9999616742134094)]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#resumo",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#resumo",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.10 Resumo",
    "text": "3.10 Resumo\nNesta seção, cobrimos os fundamentos de Processamento de Linguagem Natural (PLN), incluindo tokenização, lemmatização, stemização e remoção de stopwords. Também vimos como implementar essas técnicas em Python, utilizando as bibliotecas NLTK e SpaCy.\nAlém disso, explicamos o que são Intents, um conceito central na arquitetura de chatbots modernos baseados em NLU (Natural Language Understanding). Eles representam o objetivo do usuário e permitem que o chatbot compreenda a intenção por trás das mensagens para agir de forma adequada. Os Intents estão intrinsecamente ligados a outros conceitos fundamentais: (i) Entities: Fornecem os detalhes específicos dentro de um intent. (ii) Utterances: São as diversas maneiras como um usuário pode expressar um mesmo intent. (iii) Actions/Responses: São as tarefas ou respostas que o chatbot executa após identificar um intent. A definição cuidadosa, o treinamento robusto e o gerenciamento contínuo dos intents são relevantes para a eficácia, a inteligência e a qualidade da experiência do usuário oferecida por um chatbot.\nTambém exploramos várias técnicas de vetorização de texto, desde as mais simples, como One-Hot Encoding, até abordagens mais sofisticadas, como TF-IDF. Com exemplos práticos em Python, demonstramos como essas técnicas podem ser aplicadas em pipelines de PLN. Além disso, abordamos o conceito de embeddings de palavras, com ênfase no Word2Vec, e exploramos outras técnicas como GloVe e FastText. Demonstramos como esses embeddings capturam relações semânticas entre palavras e suas aplicações em tarefas de PLN.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#exercícios",
    "href": "sections/6_processamento-de-linguagem-natural-pln-cap-pln.html#exercícios",
    "title": "3  Processamento de Linguagem Natural (PLN)",
    "section": "3.11 Exercícios",
    "text": "3.11 Exercícios\n\nO que é vetorização de texto no contexto de processamento de linguagem natural?\n\nA compressão de textos longos para reduzir o tamanho dos arquivos.\nA transformação de palavras ou documentos em representações numéricas.\nA categorização de textos em diferentes classes.\nA separação de um texto em frases ou parágrafos distintos.\n\nQual das seguintes técnicas de vetorização é baseada na frequência de termos em um documento?\n\nWord2Vec\nTF-IDF\nBag of Words (BoW)\nEmbeddings de Palavras\n\nO que é a técnica Bag of Words (BoW)?\n\nUm método de combinar várias palavras em uma única representação vetorial.\nUm método de contar a frequência de palavras em um documento sem considerar a ordem das palavras.\nUm algoritmo de compressão de texto.\nUma técnica para traduzir texto entre diferentes idiomas.\n\nQual das alternativas a seguir é uma desvantagem do modelo Bag of Words?\n\nEle não consegue capturar a ordem das palavras.\nEle é muito difícil de implementar.\nEle não suporta múltiplos idiomas.\nEle exige uma grande quantidade de dados para funcionar.\n\nO que é um embedding de palavras?\n\nUma matriz de números binários que representa a presença ou ausência de palavras em um texto.\nUm vetor de números que representa o significado semântico de uma palavra no contexto de um grande corpus de textos.\nUm conjunto de palavras agrupadas por temas semelhantes.\nUma técnica para compactar arquivos de texto.\n\nQual é a principal diferença entre o modelo Word2Vec e a técnica TF-IDF?\n\nWord2Vec captura relações semânticas entre palavras, enquanto TF-IDF mede a importância de uma palavra em um documento.\nTF-IDF é um modelo de aprendizado profundo, enquanto Word2Vec não é.\nWord2Vec utiliza a frequência de termos em um documento, enquanto TF-IDF utiliza redes neurais.\nWord2Vec é usado apenas para classificação de texto, enquanto TF-IDF é usado para tradução automática.\n\nO que significa a sigla TF-IDF?\n\nTerm Frequency - Inverse Document Frequency\nTerm Frequency - Inverse Data Frequency\nTotal Frequency - Inverse Document Frequency\nText Frequency - Inverse Data Frequency\n\nNo contexto do Word2Vec, o que significa \"skip-gram\"?\n\nUm método de usar uma palavra central para prever palavras de contexto ao seu redor.\nUm método de ignorar palavras raras em um texto.\nUm algoritmo para classificar palavras por frequência.\nUma técnica para comprimir textos longos.\n\nQual é o principal objetivo do modelo TF-IDF?\n\nClassificar documentos por sua relevância.\nConverter palavras em vetores numéricos que capturam seu significado semântico.\nIdentificar as palavras mais importantes em um documento, levando em conta a frequência dessas palavras em um conjunto de documentos.\nTraduzir automaticamente textos entre diferentes idiomas.\n\nComo o modelo Word2Vec aprende a representar palavras em um espaço vetorial?\n\nAtravés da análise de frequência de palavras em um único documento.\nUsando a coocorrência de palavras em um grande corpus de textos, para aprender vetores que representam o significado semântico das palavras.\nConvertendo palavras em números aleatórios e ajustando-os conforme a necessidade.\nUsando apenas o contexto imediato de uma palavra em uma frase.\n\nO que é ajuste fino (fine-tuning) de um modelo pré-treinado?\n\nA criação de um novo modelo a partir do zero.\nA adaptação de um modelo pré-treinado para uma tarefa específica usando um conjunto de dados menor e específico.\nA otimização de hiperparâmetros de um modelo sem modificar seus pesos.\nO treinamento de um modelo exclusivamente em dados de alta qualidade.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Processamento de Linguagem Natural (PLN)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "",
    "text": "4.1 Introdução\nUm Large Language Model (LLM) - em inglês, Large Language Models - pode ser definido como um sistema computacional fundamentado em técnicas de aprendizado de máquina, cuja finalidade consiste em gerar texto a partir de uma sequência textual fornecida como entrada. Nesse contexto, o texto de entrada é denominado prompt, também referido em inglês como input, enquanto o texto produzido pelo modelo é denominado resposta, ou output. A figura a seguir apresenta uma representação esquemática do processo de entrada e saída de texto em um LLM.\nUm LLM nada mais é do que um tipo específico de rede neural artificial (a rede neural profunda) treinada com dados textuais. Em outras palavras, uma grande rede neural treinada com muitos dados. O conceito de “grande rede neural” está relacionado à quantidade de parâmetros que compõem a rede, na casa de milhões em diante; já em relação a “muitos dados” refere-se a dezenas de gigabytes em diante, abrangendo conjuntos de dados brutos que podem incluir livros, artigos, páginas da web, documentos técnicos e outros materiais escritos.\nO processo de treino envolve duas etapas (veja Figura 4): (i) um treinamento com dados textuais brutos e (ii) um treinamento com dados textuais anotados (ou seja, mais ajustados) às necessidades de um LLM [@raschka2024build].\nQuando o LLM é treinado com dados brutos, ele é denominado modelo “base” e tem como objetivo prever a próxima sequência de palavras a partir do prompt de entrada. Por exemplo, para o prompt: O “livro está”, a resposta de um modelo base poderia ser: “O livro está em cima da mesa.”. A seguir, no Exemplo 1, apresenta-se um conjunto ilustrativo de dados textuais de treino.\nNa segunda etapa, o modelo base é treinado novamente, mas agora com dados de texto anotados (por humanos, muitas vezes), o que permite ao modelo aprender não apenas padrões linguísticos gerais, mas também instruções explícitas de interação. Veja o exemplo de dados textuais de treino no Exemplo 2.\nOs Exemplos 1 e 2 foram construídos apenas para fins didáticos, com o objetivo de ilustrar, de maneira simplificada, como trechos textuais podem ser utilizados no treinamento de um modelo de linguagem. Em aplicações reais, contudo, o processo é muito mais complexo: envolve conjuntos massivos de dados textuais, frequentemente com bilhões de palavras provenientes de fontes diversas e organizados com formatações específicas. A execução desse treinamento em larga escala só se tornou possível a partir da arquitetura Transformer, que introduziu o mecanismo de atenção capaz de lidar, de forma eficiente e paralela, com dependências de longo alcance em sequências de texto.\nModelos de linguagem de larga escala são treinados justamente para prever a próxima palavra em uma sequência, habilidade que serve de base para a construção de representações linguísticas sofisticadas. Esse processo permite que os modelos compreendam contextos extensos e realizem tarefas complexas de PLN, como tradução automática, sumarização de documentos e resposta a perguntas. A capacidade de capturar relações de longo alcance nos textos é um dos diferenciais desses modelos em relação a arquiteturas anteriores.\nA introdução dos Transformers, proposta por @vaswani2017attention no artigo seminal “Attention is All You Need” [@vaswani2017attention], representou uma mudança fundamental no campo. A principal inovação foi o mecanismo de atenção, que atribui pesos diferentes às palavras de entrada de acordo com seu contexto, permitindo ao modelo identificar quais termos são mais relevantes em cada situação. Essa característica torna possível o processamento paralelo de sequências longas, acelerando substancialmente o treinamento e aumentando a eficácia na modelagem de dependências distantes.\nAntes dessa arquitetura, os modelos de PLN eram dominados por RNNs (Recurrent Neural Networks) e LSTMs (Long Short-Term Memory Networks). Embora funcionais, esses métodos apresentavam limitações significativas: dificuldade em capturar dependências de longo alcance devido ao problema do vanishing gradient e restrições de desempenho, já que processavam o texto de maneira estritamente sequencial, reduzindo o potencial de paralelização. O Transformer superou essas limitações ao considerar todas as palavras de entrada simultaneamente, ponderando sua importância relativa por meio da atenção.\nCom isso, os LLMs não apenas aumentaram a precisão em tarefas tradicionais de PLN, como também ampliaram o escopo de aplicações, abrangendo geração criativa de texto, tradução contextualizada e correção gramatical automática. Por outro lado, o avanço desses modelos também trouxe questionamentos relacionados à ética, viés e uso responsável da inteligência artificial, aspectos que vêm sendo discutidos intensamente pela comunidade científica e que serão abordados no Capítulo 5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#introdução",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#introdução",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "",
    "text": "Figure 18: Fluxo entrada e saída do LLM.\n\n\n\n\n\n\n \n\n\nFigure 19: Alto nível do processo de treino de um modelo de LLM genérico.\n\n\n\n\n\"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco.\"\nTrecho de Assis, Machado. Todos os Romances: Machado de Assis (Portuguese Edition) (p. 226). Edição do Kindle.\n\n\n\nuser: \"Quem descobriu o Brasil?\"\nbot: \"Que boa pergunta. Quem descobriu o Brasil foi Pedro Álvares Cabral. Você quer saber mais sobre o Brasil?\"\nuser: \"traduza o trecho The book is on the table\"\nbot: \"A tradução é: O livro está na mesa. Gostaria de saber mais alguma coisa?\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#arquitetura-geral-do-transformer",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#arquitetura-geral-do-transformer",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.2 Arquitetura Geral do Transformer",
    "text": "4.2 Arquitetura Geral do Transformer\nOs LLMs modernos são em grande parte fundamentados na arquitetura Transformer, proposta originalmente por @vaswani2017attention. Essa arquitetura se tornou a base de inúmeros avanços no processamento de linguagem natural porque oferece uma forma eficiente e escalável de lidar com sequências de texto. Diferente de abordagens anteriores, que processavam as palavras uma a uma em ordem, os Transformers permitem que todo o contexto seja considerado em paralelo, o que torna o treinamento e a inferência mais rápidos e flexíveis.\nA estrutura geral de um Transformer é composta por blocos que se repetem, cada um deles combinando mecanismos de atenção e redes neurais totalmente conectadas (feedforward). Em sua formulação original, a arquitetura possui dois grandes componentes: o encoder, responsável por transformar a sequência de entrada em uma representação interna, e o decoder, que utiliza essa representação para gerar uma saída, como no caso da tradução automática. Embora alguns modelos atuais utilizem apenas a parte do encoder (como o BERT) ou apenas o decoder (como o GPT), o princípio fundamental continua o mesmo: ambos dependem intensamente do mecanismo de atenção.\nCada camada do encoder e do decoder contém três elementos centrais:\n\nMecanismo de atenção múltipla (Multi-Head Attention): avalia, em paralelo, diferentes formas de relacionar as palavras entre si.\nRedes feedforward: transformam as representações intermediárias, permitindo maior capacidade de modelagem.\nNormalização e conexões residuais: estabilizam o treinamento e ajudam a preservar informações ao longo das camadas.\n\nO conceito de self-attention é um dos pontos mais inovadores. Nesse mecanismo, cada palavra da sequência não é processada isoladamente, mas em comparação com todas as outras palavras da mesma sequência. Isso permite que o modelo entenda relações de dependência de longo alcance, como entre o sujeito no início de uma frase e o verbo que aparece muito depois. O multi-head attention amplia essa ideia ao aplicar várias atenções em paralelo, cada uma aprendendo a capturar um tipo de relação — algumas cabeças podem identificar proximidade sintática, enquanto outras captam conexões semânticas mais distantes.\nUm aspecto central da arquitetura é que todo esse processamento ocorre de forma paralela. Diferente das redes recorrentes (RNNs) ou das LSTMs, que analisavam o texto palavra por palavra em sequência, o Transformer avalia a frase inteira de uma só vez. Essa mudança de paradigma permitiu que modelos fossem treinados em coleções massivas de texto, chegando a bilhões de parâmetros e escalando para tamanhos sem precedentes.\nAlém de sua eficiência estrutural, o Transformer também é flexível: pode ser adaptado a diferentes tarefas com pequenas modificações. Modelos autoregressivos, como a família GPT, são baseados apenas no decoder e são treinados para prever a próxima palavra em uma sequência de forma unidirecional. Já os modelos bidirecionais, como o BERT, exploram tanto o contexto à esquerda quanto o à direita, o que os torna particularmente eficazes em tarefas de compreensão textual.\nO impacto da arquitetura Transformer é amplo e vai além do processamento de linguagem natural em sua forma mais básica. A combinação entre paralelismo, capacidade de capturar dependências de longo alcance e flexibilidade estrutural abriu caminho para a criação de famílias inteiras de modelos com diferentes finalidades. Alguns deles utilizam apenas o encoder, outros apenas o decoder, e há ainda os que combinam os dois de formas distintas. Essa diversidade deu origem a arquiteturas conhecidas, como BERT, GPT, T5 e LLaMA, que se tornaram referências na área e serão discutidas a seguir em maior detalhe.\n\n4.2.1 BERT (Bidirectional Encoder Representations from Transformers)\nO BERT (Bidirectional Encoder Representations from Transformers) introduziu uma nova forma de treinamento em larga escala, na qual o modelo considera tanto o contexto à esquerda quanto o contexto à direita de uma palavra-alvo. Essa abordagem permitiu uma compreensão mais rica da linguagem em comparação com modelos anteriores, que eram predominantemente unidirecionais.\nO BERT é construído sobre a pilha de encoders do Transformer. Isso significa que ele não é projetado para gerar texto de forma autoregressiva, mas sim para produzir representações contextuais profundas de palavras e frases, que podem depois ser utilizadas em diversas tarefas de PLN. Essa característica torna o BERT especialmente adequado para problemas de compreensão textual, como classificação, resposta a perguntas e extração de informações.\nO modelo é pré-treinado em uma grande quantidade de texto utilizando duas tarefas principais:\n\nMasked Language Modeling (MLM): Palavras aleatórias em uma sequência são substituídas por um token especial de máscara, e o modelo é treinado para prever as palavras originais com base no restante do contexto. Esse procedimento permite que o BERT aprenda relações bidirecionais, uma vez que a previsão de uma palavra depende de termos anteriores e posteriores na frase.\nNext Sentence Prediction (NSP): O modelo recebe pares de frases e deve identificar se a segunda frase segue logicamente a primeira. Essa tarefa ajuda o BERT a capturar relações discursivas entre sentenças, algo essencial em aplicações como resposta a perguntas e inferência textual.\n\nApós o pré-treinamento, o BERT pode ser ajustado finamente (Fine-Tuning) para tarefas específicas, muitas vezes com datasets muito menores que aqueles usados no pré-treinamento. Esse processo de adaptação é o que permite que um único modelo, previamente treinado de maneira genérica, seja aplicado em uma ampla gama de problemas.\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.nn.functional import softmax\n\n# Carregar o tokenizer e o modelo BERT\ntokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained( 'bert-base-uncased')\n\n# Entrada de exemplo\ninput_text = \"Chatbots são muito úteis para automação.\"\n\n# Tokenização\ninput_ids = tokenizer(input_text, return_tensors='pt')\n\n# Predição\noutputs = model(**input_ids)\nprobs = softmax(outputs.logits, dim=-1)\n\nprint(\"Probabilidades de classe:\", probs)\n\n\nProbabilidades de classe: tensor([[0.7097, 0.2903]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\nO BERT rapidamente estabeleceu novos patamares em benchmarks de PLN, como GLUE (General Language Understanding Evaluation) e SQuAD (Stanford Question Answering Dataset). Sua eficácia impulsionou o desenvolvimento de uma série de variações e aprimoramentos, como RoBERTa, DistilBERT e ALBERT, que buscaram melhorar desempenho, eficiência ou reduzir custos de treinamento.\nO BERT é amplamente utilizado em:\n\nClassificação de Texto: Análise de sentimento, detecção de spam, categorização de documentos.\nRespostas a Perguntas: Modelos que identificam trechos relevantes em um texto para responder perguntas formuladas em linguagem natural.\nExtração de Informações: Identificação de entidades nomeadas (pessoas, lugares, organizações) e relações entre elas.\n\nApesar de seu impacto, o BERT apresenta limitações relevantes. O modelo é pesado em termos computacionais, exigindo grande capacidade de processamento para treinamento e mesmo para inferência em aplicações práticas. Além disso, sua janela de contexto é limitada, o que dificulta o processamento de documentos muito extensos sem estratégias adicionais de segmentação.\n\nfrom transformers import BertTokenizer, BertModel\n\n# Carregar o tokenizer e o modelo BERT pré-treinado\ntokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased')\nmodel = BertModel.from_pretrained( 'bert-base-uncased')\n\n# Exemplo de texto\ntexto = \"Machine learning is fascinating.\"\n\n# Tokenização\ninput_ids = tokenizer(texto, return_tensors='pt')['input_ids']\n\n# Obtenção das representações do modelo BERT\noutputs = model(input_ids)\nlast_hidden_states = outputs.last_hidden_state\n\nprint(\"Representações BERT:\", last_hidden_states)\n\n\nRepresentações BERT: tensor([[[ 0.0836,  0.0931, -0.2452,  ..., -0.3743,  0.0502,  0.5606],\n[ 0.2351,  0.0642, -0.1859,  ..., -0.1147,  0.5381,  0.5704],\n...,\n[ 0.8129,  0.1013, -0.2131,  ...,  0.0133, -0.6965, -0.0085]]],\ngrad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\n\n4.2.2 GPT (Generative Pre-trained Transformer)\nO Generative Pre-trained Transformer (GPT) é um dos LLMs mais conhecidos. Ele é treinado de forma autoregressiva, o que significa que prediz a próxima palavra em uma sequência, dada a entrada anterior. Isso o torna excelente para tarefas de geração de texto. O GPT 2 foi uma versão inicial do GPT, contendo 1,5 bilhões de parâmetros. Ele mostrou que, ao ser treinado em grandes quantidades de texto, poderia gerar conteúdo coerente e complexo. GPT é uma evolução ainda maior, com 175 bilhões de parâmetros. Esse modelo pode realizar uma ampla gama de tarefas de PLN sem a necessidade de ajustes finos específicos, simplesmente recebendo exemplos de como a tarefa deve ser executada (aprendizado por poucos exemplos, ou few-shot learning).\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Carregar o tokenizer e o modelo GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Entrada de exemplo\ninput_text = \"Chatbots modernos podem\"\n\n# Tokenização\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\n# Geração de texto\noutputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n\n# Decodificação e exibição do texto gerado\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Texto gerado:\", generated_text)\n\n\nTexto gerado: Chatbots modernos podemodel.com The following is a list of all the bots that have been added to the podemodel.com community. Bot Name Description bot_bot_name_id bot_\n\nO GPT tem sido aplicado em diversas áreas, incluindo:\n\nGeração de Texto: Criação de conteúdo, histórias, artigos, etc.\nAssistentes Virtuais: Implementação de sistemas de diálogo baseados em IA.\nTradução Automática: Utilização de contexto amplo para melhorar a tradução entre idiomas.\n\nO GPT é um modelo autoregressivo que se concentra na geração de texto. É treinado para prever a próxima palavra em uma sequência, o que o torna excelente para tarefas de geração de texto, como chatbots.\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Carregar o tokenizer e o modelo GPT-2 pré-treinado\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Exemplo de texto\ninput_text = \"In the future, artificial intelligence will\"\n\n# Tokenização\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\n# Geração de texto\noutputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n\n# Decodificação e exibição do texto gerado\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Texto gerado:\", generated_text)\n\n\nTexto gerado: In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people. We're going to see a lot of things that are going to be very interesting in\n\nAlém de BERT e GPT, há muitos outros modelos baseados em Transformers projetados para tarefas específicas. Alguns exemplos incluem o T5 (Text-To-Text Transfer Transformer) que converte qualquer tarefa de PLN em um problema de tradução; o XLNet que combina ideias de BERT e Transformers autoregressivos para melhorar a modelagem de dependências de longo alcance; o RoBERTa (A Robustly Optimized BERT Pretraining Approach) que é uma variação do BERT com treinamento aprimorado, além do DistilBERT.\nO T5 transforma qualquer tarefa de PLN em um problema de tradução, onde a entrada e a saída são tratadas como texto. Isso simplifica o Fine-Tuning para diferentes tarefas.\n\n# pip install transformers[torch]\n# pip install sentencepiece\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Carregar o tokenizer e o modelo T5\ntokenizer = T5Tokenizer.from_pretrained( 't5-small')\nmodel = T5ForConditionalGeneration.from_pretrained( 't5-small')\n\n# Entrada de exemplo\ninput_text = \"translate English to German: The weather is nice today.\"\n\n# Tokenização\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\n# Geração de texto\noutputs = model.generate(input_ids)\n\n# Decodificação e exibição do texto gerado\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Tradução gerada:\", generated_text)\n\n\nTradução gerada: Das Wetter ist heute schon.\n\nCabe ressaltar ainda o XLNet e o DistilBERT. O XLNet combina vantagens dos modelos autoregressivos e bidirecionais, como o GPT e BERT, para capturar dependências de longo alcance de forma mais eficiente. Já o DistilBERT é uma versão reduzida do BERT, com menos parâmetros, mas mantendo uma alta performance, o que o torna mais eficiente para uso em produção.\n\nfrom transformers import XLNetTokenizer, XLNetLMHeadModel\n\n# Carregar o tokenizer e o modelo XLNet\ntokenizer = XLNetTokenizer.from_pretrained( 'xlnet-base-cased')\nmodel = XLNetLMHeadModel.from_pretrained( 'xlnet-base-cased')\n\n# Entrada de exemplo\ninput_text = \"Natural Language Processing is\"\n\n# Tokenização\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\n# Geração de texto\noutputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\n\n# Decodificação e exibição do texto gerado\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Texto gerado com XLNet:\", generated_text)\n\n\nTexto gerado com XLNet: Natural Language Processing is and in and in [...]\n\n\n\n4.2.3 distilbert-base-uncased\nO modelo distilbert-base-uncased [@akhila2023comparative] foi lançado em 2019, sendo menor e mais rápido que o BERT e otimizado para tarefas que processam frases ou sentenças. Sua versão uncased é útil para descrições de código e problemas, já que não considera diferenças de maiúsculas/minúsculas. Ele é pré-treinado em um grande corpus, o que ajuda na generalização, e tem uma boa arquitetura para esse tipo de tarefa, além de exigir hardware acessível.\nO distilbert resulta de um processo de knowledge distillation que reduz em aproximadamente 40% o número de parâmetros e acelera a inferência em cerca de 60%, preservando 95–97% da acurácia do BERT-base (original) em benchmarks de compreensão de linguagem natural [@Sanh2019]. Essa compacidade de fornecer representações ricas com custo computacional inferior é particularmente vantajosa em ambientes de hardware moderado. Com apenas 67M de parâmetros [@Sanh2019], a variante uncased cabe em uma GPU modesta (por exemplo, 6 GB de RAM da placa de vídeo), possibilitando fine-tuning e inferência dentro de recursos computacionais restritos. Assim, torna-se viável re-treinar o modelo à medida que novos dados de projeto se acumulam, mantendo a acurácia sem investir em infraestruturas onerosas.\nPor fim, o amplo suporte no ecossistema Hugging Face para os modelos do tipo BERT e para outros modelos simplifica a reprodutibilidade e a integração em pipelines de tarefas de processamento de linguagem natural.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#fine-tuning-de-modelos-pré-treinados",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#fine-tuning-de-modelos-pré-treinados",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.3 Fine-Tuning de Modelos Pré-Treinados",
    "text": "4.3 Fine-Tuning de Modelos Pré-Treinados\nModelos pré-treinados como BERT e GPT demonstraram capacidade de compreender nuances semânticas em texto e transferir esse conhecimento para diversas tarefas específicas através de Fine-Tuning. [@Minaee2025].\nO Fine-Tuning de modelos pré-treinados é uma técnica fundamental no Processamento de Linguagem Natural (PLN) moderno, especialmente ao trabalhar com Modelos de Linguagem Grande (LLMs). Fine-Tuning permite adaptar um modelo geral para tarefas específicas, como classificação de texto, análise de sentimentos ou geração de linguagem, utilizando um conjunto de dados menor e específico.\nFine-Tuning é o processo de tomar um modelo pré-treinado em uma grande quantidade de dados gerais e adaptá-lo para uma tarefa específica. Este processo envolve ajustar os pesos do modelo, mas com uma taxa de aprendizado menor para não “desaprender” o que foi aprendido durante o pré-treinamento. Modelos como BERT [@devlin2019bert], GPT [@radford2019language], e T5 [@raffel2020exploring] são comumente fine-tuned para tarefas específicas.\nUma alternativa eficiente ao Fine-Tuning completo é o uso de métodos de adaptação com baixo número de parâmetros, como o LoRA (Low-Rank Adaptation). Em vez de atualizar todos os pesos do modelo pré-treinado, o LoRA introduz pequenas matrizes adicionais de baixa dimensão que são ajustadas durante o treinamento, enquanto os pesos originais permanecem congelados. Essa técnica reduz drasticamente o número de parâmetros que precisam ser treinados, diminuindo o custo computacional e de armazenamento. Na prática, isso torna viável aplicar Fine-Tuning em LLMs muito grandes, mesmo em ambientes com recursos limitados, preservando boa parte do desempenho obtido pelo ajuste completo.\nA principal vantagem do Fine-Tuning é a eficiência, pois permite que os modelos aprendam rapidamente uma nova tarefa, utilizando relativamente poucos dados. Além disso, modelos pré-treinados já capturam padrões linguísticos gerais, o que torna o Fine-Tuning uma abordagem útil para resolver problemas específicos sem precisar treinar um modelo do zero. O processo de Fine-Tuning geralmente envolve os seguintes passos:\n\nEscolha do Modelo: Selecionar um modelo pré-treinado adequado para a tarefa. Modelos como BERT e GPT são populares devido à sua versatilidade.\nPreparação dos Dados: Os dados precisam estar formatados de maneira que sejam compatíveis com a tarefa específica, como classificação de texto ou resposta a perguntas.\nConfiguração do Treinamento: Ajuste de hiperparâmetros como a taxa de aprendizado, número de épocas e tamanho do lote.\nTreinamento: Executar o treinamento do modelo no conjunto de dados específico.\nAvaliação: Avaliar o desempenho do modelo ajustado em um conjunto de validação ou teste.\n\nVamos realizar o Fine-Tuning de um modelo BERT para uma tarefa de classificação de sentimentos usando o conjunto de dados IMDb.\nCabe ressaltar que a biblioteca transformers da Hugging Face, utilizada neste exemplo, tornou-se a ferramenta de referência para trabalhar com modelos baseados em Transformers. Ela oferece uma ampla gama de modelos pré-treinados que podem ser facilmente integrados em pipelines de PLN.\nO código a seguir pode levar de 30 minutos a 2 horas para finalizar em um computador com uma boa GPU. Em CPU, pode demorar várias horas ou até mais de um dia. Uma sugestão é utilizar o Google Colab.\n\n# pip install datasets\n# pip install transformers[torch]\n\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Carregar o dataset IMDb\ndataset = load_dataset(\"imdb\")\n\n# Carregar o tokenizer e o modelo BERT\ntokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained( 'bert-base-uncased')\n\n# Tokenizar os dados\ndef tokenize_function(examples):\nreturn tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Definir argumentos de treinamento\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    learning_rate=2e-5,\n    report_to=\"none\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Criar o Trainer\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=tokenized_datasets['train'],\neval_dataset=tokenized_datasets['test'],\n)\n\n# Treinar o modelo\ntrainer.train()\n\n# Avaliar o modelo\neval_result = trainer.evaluate()\nprint(f\"Resultado da Avaliação: {eval_result}\")\n\n\nResultado da Avaliação: {'eval_loss': 0.2594565749168396, 'eval_runtime': 172.544, 'eval_samples_per_second': 144.891, 'eval_steps_per_second': 9.059, 'epoch': 3.0}\n\nNeste exemplo, utilizamos o modelo BERT e a biblioteca datasets da Hugging Face para carregar o conjunto de dados IMDb, que é usado para tarefas de classificação de sentimentos. O processo de tokenização é realizado com o BertTokenizer, seguido pelo treinamento do modelo usando o Trainer, que automatiza o processo de Fine-Tuning.\nEmbora o Fine-Tuning seja uma técnica com potencial, é importante considerar alguns desafios:\n\nOverfitting: Ajustar demais o modelo para os dados de treinamento específicos pode reduzir a generalização para novos dados.\nBiases Inerentes: Se o modelo pré-treinado já contém vieses, o Fine-Tuning pode reforçá-los, especialmente se os dados de treinamento forem limitados ou enviesados.\nRequisitos Computacionais: Fine-Tuning de LLMs pode ser computacionalmente intensivo, especialmente para modelos maiores como GPT-3.\n\nAlém disso, algumas abordagens avançadas para melhorar o processo de Fine-Tuning incluem:\n\nLearning Rate Warmup: Aumentar gradualmente a taxa de aprendizado no início do treinamento para evitar grandes atualizações de peso que poderiam desestabilizar o modelo.\nLayer-Wise Learning Rate Decay: Aplicar diferentes taxas de aprendizado para diferentes camadas do modelo, com camadas inferiores aprendendo mais lentamente.\nData Augmentation: Aumentar a diversidade do conjunto de dados de treinamento para melhorar a robustez do modelo.\n\nO Fine-Tuning tem uma vasta gama de aplicações em PLN, incluindo:\n\nClassificação de Texto: Análise de sentimentos, detecção de spam, categorização de notícias.\nRespostas a Perguntas: Modelos que respondem a perguntas baseadas em um contexto textual específico.\nGeração de Texto: Fine-Tuning de modelos como GPT para gerar textos específicos de um domínio, como redação de artigos científicos.\nTradução Automática: Adaptação de modelos de tradução para dialetos ou linguagens específicas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#few-shot-e-zero-shot-learning",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#few-shot-e-zero-shot-learning",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.4 Few Shot e Zero Shot Learning",
    "text": "4.4 Few Shot e Zero Shot Learning\nZero Shot: A abordagem LLM zero-shot learning refere-se à capacidade dos LLMs de resolver tarefas sem a necessidade de exemplos explícitos fornecidos durante a inferência. Nesse contexto, a tarefa de processamento de linguagem natural text classification é especificada unicamente por meio de uma instrução textual (prompt), e o modelo deve inferir a ação esperada com base em seu conhecimento prévio adquirido durante o pré-treinamento [@radford2019language].\nFew Shot: Já o few-shot learning caracteriza-se pela inclusão de um pequeno conjunto de exemplos da tarefa no próprio prompt, com o objetivo de guiar a geração do modelo durante a inferência. Essa técnica permite ao modelo identificar padrões desejados com base nos exemplos fornecidos e aplicá-los a novos casos, mesmo sem reconfiguração ou ajuste de parâmetros. Trata-se de uma abordagem intermediária entre o zero-shot e o treinamento supervisionado tradicional, sendo especialmente eficaz em tarefas de classificação com variações contextuais [@raschka2024build]. Sua principal vantagem está na adaptação rápida a novas tarefas com custo computacional reduzido.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#retrieval-augmented-generation-rag",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#retrieval-augmented-generation-rag",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.5 Retrieval-Augmented Generation (RAG)",
    "text": "4.5 Retrieval-Augmented Generation (RAG)\nO Retrieval-Augmented Generation (RAG) é uma abordagem que combina duas técnicas na área de processamento de linguagem natural: recuperação de informações e geração de texto. A ideia central do RAG é aprimorar a capacidade de um modelo de linguagem ao integrá-lo com um sistema de recuperação que busca informações relevantes de uma base de dados ou de um conjunto de documentos.\nNa prática, o RAG opera em duas etapas. Primeiro, quando uma consulta ou pergunta é feita, um mecanismo de recuperação é acionado para identificar e extrair informações pertinentes de um repositório de dados. Isso pode incluir documentos, artigos ou qualquer outro tipo de conteúdo textual que possa fornecer contexto e detalhes adicionais sobre o tema em questão. Essa fase garante que o modelo de linguagem tenha acesso a informações atualizadas e específicas, em vez de depender apenas do conhecimento prévio que foi incorporado durante seu treinamento.\nEm seguida, na segunda etapa, o modelo de linguagem utiliza as informações recuperadas para gerar uma resposta mais rica e contextualizada. Essa geração não se limita a reproduzir o conteúdo recuperado, mas sim a integrar esses dados de forma coesa, criando uma resposta que não apenas responde à pergunta, mas também fornece uma narrativa mais completa e informativa. Isso resulta em respostas que são mais precisas e relevantes, pois são fundamentadas em dados concretos e atualizados.\nA combinação dessas duas etapas permite que o RAG supere algumas limitações dos modelos de linguagem tradicionais, que podem falhar em fornecer informações precisas ou atualizadas, especialmente em domínios que evoluem rapidamente. Além disso, essa abordagem é particularmente útil em aplicações como assistentes virtuais, chatbots e sistemas de perguntas e respostas, onde a precisão e a relevância da informação são fatores chaves na experiência do usuário.\nPortanto, ele é uma técnica que não apenas melhora a qualidade das respostas geradas por modelos de linguagem, mas também amplia o alcance e a aplicabilidade desses modelos em cenários do mundo real, onde a informação é dinâmica e em constante evolução.\nO RAG une dois componentes principais, a Recuperação de Informação (Retrieval): envolve buscar documentos, parágrafos ou passagens relevantes a partir de uma grande coleção de dados; e a Geração de Texto (Generation): uma vez que a informação relevante é recuperada, um modelo de linguagem, como GPT ou BART, é utilizado para gerar uma resposta coerente e informativa baseada nas informações recuperadas. Dessa forma, RAG é capaz de responder a perguntas e gerar conteúdo que não apenas utiliza o contexto imediato, mas também consulta uma base de conhecimento externa, aumentando a precisão e a relevância das respostas.\nVamos implementar um exemplo simples de RAG usando as bibliotecas da Hugging Face, incluindo o modelo DPR para recuperação e o modelo BART para geração de texto.\nRecuperação de Passagens com DPR: Primeiro, precisamos carregar e configurar o modelo DPR para recuperar passagens relevantes a partir de uma base de dados. Sugerimos a execução do código abaixo no Google Colab.\n\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Carregar o tokenizer e o modelo para as consultas (questions)\nquestion_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n\"facebook/dpr-question_encoder-single-nq-base\")\nquestion_encoder = DPRQuestionEncoder.from_pretrained(\n\"facebook/dpr-question_encoder-single-nq-base\")\n\n# Carregar o tokenizer e o modelo para os contextos (passages)\ncontext_tokenizer = DPRContextEncoderTokenizer.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\ncontext_encoder = DPRContextEncoder.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n\n# Exemplo de consulta\nquery = \"What is Retrieval-Augmented Generation?\"\n\n# Codificar a consulta\nquery_input = question_tokenizer(query, return_tensors=\"pt\")\nquery_embedding = question_encoder(**query_input).pooler_output\n\n# Exemplo de passagens\npassages = [\n    \"Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation.\",\n    \"It allows for more accurate and contextually relevant answers by consulting external knowledge bases.\",\n    \"RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model.\"\n]\n\n# Codificar as passagens\npassage_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\npassage_embeddings = context_encoder(\n    **passage_inputs).pooler_output\n\n# Calcular similaridade e selecionar a passagem mais relevante\nsimilarity_scores = torch.matmul(query_embedding, passage_embeddings.T)\nbest_passage_index = torch.argmax(similarity_scores, dim=1).item()\nbest_passage = passages[best_passage_index]\n\nprint(\"Passagem mais relevante:\", best_passage)\n\n\nPassagem mais relevante: Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation \n\nNeste exemplo, utilizamos o modelo DPR para codificar uma consulta e várias passagens, e então calculamos a similaridade entre a consulta e as passagens para recuperar a mais relevante.\nGeração de Texto com BART: Uma vez que a passagem mais relevante foi recuperada, utilizamos o modelo BART para gerar uma resposta coerente.\n\n# pip install transformers[torch]\nfrom transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Carregar o tokenizer e o modelo para as consultas (questions)\nquestion_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n\"facebook/dpr-question_encoder-single-nq-base\")\nquestion_encoder = DPRQuestionEncoder.from_pretrained(\n\"facebook/dpr-question_encoder-single-nq-base\")\n\n# Carregar o tokenizer e o modelo para os contextos (passages)\ncontext_tokenizer = DPRContextEncoderTokenizer.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\ncontext_encoder = DPRContextEncoder.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n\n# Exemplo de consulta\nquery = \"What is Retrieval-Augmented Generation?\"\n\n# Codificar a consulta\nquery_input = question_tokenizer(query, return_tensors=\"pt\")\nquery_embedding = question_encoder(**query_input).pooler_output\n\n# Exemplo de passagens\npassages = [\n    \"Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation.\",\n    \"It allows for more accurate and contextually relevant answers by consulting external knowledge bases.\",\n    \"RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model.\"\n]\n\n# Codificar as passagens\npassage_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\npassage_embeddings = context_encoder\n   **passage_inputs).pooler_output\n\n# Calcular similaridade e selecionar a passagem mais relevante\nsimilarity_scores = torch.matmul(query_embedding, passage_embeddings.T)\nbest_passage_index = torch.argmax(similarity_scores, dim=1).item()\nbest_passage = passages[best_passage_index]\n\n# Carregar o tokenizer e o modelo BART\nbart_tokenizer = BartTokenizer.from_pretrained(\n\"facebook/bart-large\")\nbart_model = BartForConditionalGeneration.from_pretrained(\n\"facebook/bart-large\")\n\n# Concatenar a consulta com a passagem relevante\ninput_text = query + \" \" + best_passage\n\n# Codificar e gerar resposta\ninput_ids = bart_tokenizer.encode(input_text, return_tensors=\"pt\")\ngenerated_ids = bart_model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\ngenerated_text = bart_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"Resposta gerada:\", generated_text)\n\n\nResposta gerada: What is Retrieval-Augmented Generation?Retrieval and Augmented Generation is a technique that combines retrieval of relevant information with text generation.What is retrieval?\n\nEste código gera uma resposta baseada na passagem recuperada, criando uma resposta informativa que combina a informação relevante com a geração de texto fluida.\nA técnica RAG tem várias aplicações práticas, incluindo:\n\nSistemas de Resposta a Perguntas: Sistemas que precisam consultar bases de conhecimento extensivas para responder a perguntas de forma precisa.\nAssistentes Virtuais: Assistentes que necessitam de acesso a informações específicas e detalhadas, além do treinamento inicial do modelo.\nGeração de Conteúdo: Criação de conteúdo especializado que requer consulta de fontes externas para garantir precisão e relevância.\n\nA técnica RAG também apresenta alguns desafios:\n\nEscalabilidade: A recuperação de informações em bases de dados muito grandes pode ser computacionalmente intensiva.\nRelevância das Passagens: A qualidade das respostas geradas depende fortemente da relevância das passagens recuperadas.\nTreinamento Conjunto: Treinar os componentes de recuperação e geração de maneira conjunta pode ser complexo e requer grandes volumes de dados.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#llama",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#llama",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.6 LLaMA",
    "text": "4.6 LLaMA\nLLaMA é uma família de modelos de linguagem grandes (LLMs) que são menores em tamanho, mas ainda mantêm a capacidade de realizar tarefas complexas de PLN. A abordagem de LLaMA é baseada em uma arquitetura de Transformer, semelhante a outros LLMs, mas otimizada para eficiência em termos de parâmetros e recursos computacionais.\n\n4.6.0.1 Características Principais\n\nTamanho Reduzido: LLaMA é projetado para ser mais leve que os modelos gigantescos, com diferentes variantes que variam de 7B a 65B parâmetros.\nEficiência Computacional: Devido ao seu design otimizado, o LLaMA pode ser treinado em menos tempo e com menos recursos, tornando-o acessível para organizações menores e pesquisadores.\nVersatilidade: Apesar de seu tamanho reduzido, o LLaMA é capaz de realizar uma ampla gama de tarefas de PLN, incluindo geração de texto, tradução, e compreensão de linguagem.\n\n\n\n4.6.1 Arquitetura do LLaMA\nA arquitetura do LLaMA é baseada no Transformer, mas com várias otimizações que permitem que ele mantenha uma alta qualidade de predição, enquanto usa menos parâmetros e recursos computacionais.\n\n4.6.1.1 Camadas Transformer Otimizadas\nO LLaMA utiliza camadas Transformer com melhorias específicas para otimizar o uso de memória e tempo de processamento. As principais diferenças incluem:\n\nAtenção Multi-Head Otimizada: Reduz a redundância ao calcular a atenção em múltiplas cabeças.\nFeedforward Otimizado: Utiliza técnicas de compressão para reduzir o número de operações necessárias.\nParâmetros Compactos: Redução do número de parâmetros, mantendo a capacidade de capturar relações complexas na linguagem.\n\n\n\n4.6.1.2 Treinamento e Escalabilidade\nO LLaMA foi treinado em grandes corpora de dados textuais, incluindo múltiplos idiomas e domínios. A arquitetura permite que o modelo seja escalado de maneira eficiente, com versões menores (7B parâmetros) adequadas para tarefas menos intensivas e versões maiores (65B parâmetros) competindo com modelos de ponta como GPT-3.\n\n\n\n4.6.2 Exemplo de uso do LLaMA\nEmbora o LLaMA seja relativamente novo, é possível utilizar as ferramentas da Hugging Face para trabalhar com variantes do modelo ou implementações semelhantes.\nVamos utilizar uma versão de LLaMA para realizar uma tarefa simples de geração de texto. Assumiremos que a variante LLaMA foi integrada à Hugging Face Transformers.\nPara executar o código abaixo, recomendamos o Google Colab. É necessário informar a sua chave do Hugging Face e solicitar acesso ao modelo LLaMa no próprio Hugging Face.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\nfrom google.colab import userdata\n\nfrom google.colab import userdata\nuserdata.get('HF_TOKEN')\n\n# Log in to Hugging Face\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Carregar o tokenizer e o modelo LLaMA\ntokenizer = AutoTokenizer.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/Meta-Llama-3-8B\")\n\n# Texto de entrada\ninput_text = \"Artificial intelligence is transforming the world of\"\n\n# Tokenizar e gerar texto\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\ngenerated_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"Texto gerado:\", generated_text)\n\n\nTexto gerado: Artificial intelligence is transforming the world of business. It has the potential to revolutionize the way we work, communicate, and interact with each other. AI is already being used in a variety of industries, from healthcare to finance, and it is\n\nNeste exemplo, carregamos o tokenizer e o modelo LLaMA e geramos um texto baseado em um prompt de entrada. O código pode ser ajustado para diferentes tamanhos de modelos e diferentes tarefas de geração de texto.\n\n\n4.6.3 Aplicações de LLaMA\nDevido a sua eficiência e versatilidade, LLaMA pode ser aplicado em uma variedade de cenários, incluindo:\n\nAssistentes Virtuais: Implementação de assistentes que podem ser executados em dispositivos com recursos limitados.\nGeração de Conteúdo: Produção de artigos, histórias e outros conteúdos textuais de alta qualidade.\nTradução Automática: Modelos LLaMA menores podem ser usados para traduções em tempo real em dispositivos móveis.\nAnálise de Sentimentos: Aplicações que exigem processamento eficiente de grandes volumes de dados textuais.\n\n\n\n4.6.4 Comparação com Outros Modelos\nQuando comparado com modelos maiores como GPT-3, o LLaMA oferece um excelente equilíbrio entre desempenho e eficiência. Ele é particularmente útil em cenários onde os recursos computacionais são limitados ou onde a implantação em escala é uma consideração chave.\nAlgumas Vantagens do LLaMA:\n\nRedução de Custos: Menor demanda por recursos computacionais, resultando em custos reduzidos para treinamento e implantação.\nEscalabilidade: Pode ser facilmente adaptado para diferentes tarefas e ambientes.\nRapidez: Menor latência em inferências devido ao menor tamanho do modelo.\n\nAlgumas Limitações do LLaMA:\n\nCapacidade Limitada: Embora eficiente, modelos menores podem não capturar todas as nuances de linguagem que modelos maiores conseguem.\nMenor Variedade de Tarefas: Pode ser menos adequado para tarefas extremamente complexas que exigem modelos com bilhões de parâmetros.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#llm-na-prática",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#llm-na-prática",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.7 LLM na prática",
    "text": "4.7 LLM na prática\n\n4.7.1 Hugging Face Pipeline\nA biblioteca Transformer da Hugging Face torna muito mais fácil trabalhar com modelos pré-treinados como o GPT-2. Aqui está um exemplo de como gerar texto usando o GPT-2 pré-treinado:\n\nfrom transformers import pipeline\npipe = pipeline('text-generation', model='gpt2')\ninput = 'Olá, como vai você?'\noutput = pipe(input)\nprint(output)\n\n\n[{'generated_text': 'The book is on one of the most exciting,'},\n{'generated_text': 'The book is on sale via Amazon.com for'}, \n{'generated_text': 'The book is on sale tomorrow for $2.'}, \n{'generated_text': 'The book is on sale now, read more at'}, \n{'generated_text': 'The book is on the bookshelf in the'}]\n\nEste código é simples porque ele usa um modelo que já foi treinado em um grande dataset. Também é possível ajustar (fine-tune) um modelo pré-treinado em seus próprios dados para obter resultados melhores.\n\n\n4.7.2 LLM Local com Ollama\nNesta seção, descreveremos detalhes da instalação do software Ollama, baixaremos o modelo LLM Qwen2 0.5B e exploraremos suas capacidades com algumas perguntas simples.\nO Ollama é uma plataforma baseada em linha de comando que facilita o uso de modelos de IA localmente; existem outras, porém a ideia geral é bem semelhante. Com ele, você pode baixar modelos pré-treinados, gerar texto e fazer inferências sem precisar de uma placa de vídeo potente. Sua simplicidade e rapidez o tornam perfeito para quem quer experimentar LLMs em computadores mais modestos.\nBaixando e Instalando o Ollama: Acesse o site oficial do Ollama e baixe o instalador para o seu sistema operacional. Os procedimentos descritos nesta seção são baseados no Windows, mas o Ollama também funciona no macOS e Linux.\nEntre no site do Ollama e clique no botão de download para Windows. Depois, salve o instalador (um arquivo pequeno, com poucos megabytes) na sua pasta de downloads. Em seguida, execute o instalador e siga as instruções para concluir a instalação. Depois de instalado, o Ollama já está pronto para ser usado pelo terminal. Veja na Figura 4.3 a página do site do Ollama para download.\n\n\n \n\n\nFigure 20: Site do Ollama par download.\n\n\nConhecendo os Comandos do Ollama: Com o Ollama instalado, abra o terminal (Prompt de Comando no Windows) e digite “ollama” para confirmar que a instalação deu certo. Você verá uma lista de comandos disponíveis. Veja na Figura 4.4 o resultado da saída do comando list no prompt de comando.\n\n\n \n\n\nFigure 21: Comando para listar os modelos LLM já instalado.\n\n\nVamos detalhar os comandos run e show do Ollama. O comando “ollama run &lt;nome-do-modelo&gt;” baixa e roda um modelo específico. Já o comando “ollama show &lt;nome-do-modelo&gt;” exibe detalhes sobre um modelo, como janela de contexto e parâmetros. Como ainda não baixamos nenhum modelo, o comando “ollama list” vai mostrar uma lista vazia. Vamos resolver isso baixando um modelo!\nEscolhendo e Baixando um Modelo: No site do Ollama, na seção “Models”, você encontra vários modelos disponíveis. Veja na Figura 4.5 um print de tela do navegador do site do Ollama com o resultado da busca de um modelo LLM.\n\n\n \n\n\nFigure 22: Site do ollama, busca pelo nome do modelo.\n\n\nVamos usar um modelo bem leve chamado “Qwen2 0.5B”, da Alibaba, que tem 0,5 bilhão de parâmetros e apenas 350 MB de tamanho. Ele é ideal para máquinas mais simples, como um computador com processador Pentium Gold sem GPU. Veja na Figura 4.6 um print de tela do navegador do site do Ollama com o resultado do detalhamento do modelo Qwen2.5.\n\n\n \n\n\nFigure 23: Tela exibindo detalhes do modelo LLM Qwen2.5.\n\n\nPara baixar e rodar o modelo, acesse a página de modelos no site do Ollama. Localize o modelo Qwen2 e anote o comando para a versão de 0.5B: “ollama run qwen2:0.5b”. No terminal, digite:\n\n$ ollama run qwen2.5:0.5b\n\nEsse comando vai baixar o modelo e abrir uma interface de texto interativa para você começar a conversar com ele. Veja na Figura 4.7 um print de tela do navegador com o resultado do prompt do comando “run”.\n\n\n \n\n\nFigure 24: Prompt de comando demonstrando a execução do Ollama.\n\n\nInteragindo com o modelo: Com o modelo baixado, o Ollama inicia uma interface no terminal para conversas baseadas em texto. Vamos testar com algumas perguntas.\n\nExemplo 1 Perguntando Sobre Aprendizado de Máquina\n.\nDigite a seguinte pergunta “O que é aprendizagem de máquina?” no console (ou prompt de comando) do Ollama instalado e com um modelo LLM carregado. Veja na Figura 4.8 um print do prompt de comando com a resposta do LLM Qwen carregado em memória e respondendo ao prompt: o que é aprendizagem de máquina.\n\nO que é aprendizagem de máquina?\n\n\n\n \n\n\nFigure 25: Prompt de comando exibindo a saída do LLM em português.\n\n\nPor ser um modelo pequeno, o Qwen2 0.5B pode “alucinar” um pouco ou dar respostas meio vagas, especialmente em português. Suas capacidades de diálogo emergente são limitadas comparadas a modelos maiores, mas ainda assim são impressionantes para o tamanho dele.\n\n\nExemplo 2: Perguntando em Inglês\n.\nAgora, vamos tentar a mesma pergunta em inglês: “What is machine learning?”. Perguntando em inglês, a resposta costuma ser mais precisa. Figura 4.9 um resultado da resposta (output) do LLM em inglês.\n\nWhat is machine learning?\n\n\n\n \n\n\nFigure 26: Prompt de comando exibindo a saída do LLM em inglês.\n\n\nO modelo tem uma resposta melhor em inglês, provavelmente porque foi treinado com mais dados nessa língua. Isso mostra como o desempenho de modelos de linguagem pode variar dependendo do idioma e dos dados de treinamento.\n\n\nExemplo 3: prompt simples pergunta e resposta\nVamos facilitar, perguntando uma coisa simples, em que o modelo teria facilidade em perguntas: “O que é Ollama?”. Veja na Figura 4.10 a saída desta pergunta.\n\nO que é Ollama?\n\n\n\n \n\n\nFigure 27: Prompt de comando exibindo a saída do LLM em português.\n\n\nO modelo confundiu o “Ollama” a que eu me referia com outra coisa, mostrando as limitações de modelos menores. Perguntando em inglês (“What’s Ollama in language models?”), ele também não acerta, sugerindo que é um tipo de música latina.\nA interface do Ollama tem alguns comandos úteis para gerenciar a sessão. Entre eles “/clear”: Limpa a sessão atual para começar do zero; “/help”: Mostra os comandos disponíveis; “/bye”: Sai do modo interativo do modelo. Por exemplo, depois de algumas perguntas, você pode digitar /clear para zerar o contexto ou /bye para encerrar o modelo.\nVerificando detalhes do Modelo LLM: Para saber mais sobre o modelo que você está usando, execute o comando “ollama show &lt;nome modelo&gt;”.\n\n$ ollama show qwen2.5:0.5b\n\nEsse comando mostra informações, tais como a janela de contexto: 32k tokens (a quantidade de texto que o modelo consegue considerar de uma vez); a quantização: Q4 (um método para reduzir o tamanho do modelo e otimizar a performance). A janela de contexto é especialmente importante, pois ela acumula tanto o que você digita quanto as respostas do modelo, até o limite de 32.000 tokens. Um limite considerável para um modelo pequeno.\nO Ollama funciona em computadores simples, tornando a IA acessível a todos; com ele é possível uma execução local pois não depende de serviços na nuvem, garantindo privacidade e controle. Sua configuração é rápida e sua interface de linha de comando é ideal para desenvolvedores que preferem um fluxo de trabalho minimalista e baseado em texto; por fim, embora existam plataformas mais visuais para rodar LLMs, a velocidade e o baixo consumo de recursos do Ollama o tornam uma ótima escolha para experimentos rápidos.\n\n\n\n4.7.3 Tokenizador no LLM\nO tokenizador é responsável por dividir o texto em partes menores (tokens) que o modelo pode entender. Depois, precisamos de um modelo. O modelo é a parte que realmente faz o trabalho de entender e gerar texto. Vamos usar um tokenizador já existente no Hugging Face.\n\n# pip install transformers[torch]\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ninput= \"Olá, como vai você?\"    \ntoken_id = tokenizer(input)\nprint(token_id)\n\n\n{'input_ids': [30098, 6557, 11, 401, 78, 410, 1872, 12776, 25792, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\nA saída deste código será um dicionário com os ids dos tokens e a máscara de atenção. O id do token é o número que representa cada palavra ou parte da palavra no vocabulário do modelo. A máscara de atenção indica quais tokens devem ser considerados pelo modelo durante o processamento.\nAttention mask é uma lista de 1s e 0s que indica quais tokens devem ser considerados pelo modelo durante o processamento. Um valor de 1 significa que o token correspondente deve ser considerado, enquanto um valor de 0 significa que ele deve ser ignorado.\n\n\n4.7.4 LangChain\nLangChain é uma biblioteca de software de código aberto projetada para simplificar a interação com Modelos de Linguagem Grande (LLMs) e construir aplicativos de processamento de linguagem natural robustos. Ele fornece uma camada de abstração de alto nível sobre as complexidades de trabalhar diretamente com modelos de linguagem, tornando mais acessível a criação de aplicativos de compreensão e geração de linguagem.\nTrabalhar com LLMs pode ser complexo devido à sua natureza sofisticada e aos requisitos de recursos computacionais. LangChain lida com muitos detalhes complexos em segundo plano, permitindo que os desenvolvedores se concentrem na construção de aplicativos de linguagem eficazes. Aqui estão algumas vantagens do uso do LangChain:\n\nSimplicidade: LangChain oferece uma API simples e intuitiva, ocultando os detalhes complexos de interação com LLMs. Ele abstrai as nuances de carregar modelos, gerenciar recursos computacionais e executar previsões.\nFlexibilidade: A biblioteca suporta vários frameworks de deep learning, como TensorFlow e PyTorch, e pode ser integrada a diferentes LLMs. Isso oferece aos desenvolvedores a flexibilidade de escolher as ferramentas e modelos que melhor atendem às suas necessidades.\nExtensibilidade: LangChain é projetado para ser extensível, permitindo que os usuários criem seus próprios componentes personalizados. Você pode adicionar novos modelos, adaptar o processamento de texto ou desenvolver recursos específicos do domínio para atender aos requisitos exclusivos do seu aplicativo.\nComunidade e suporte: LangChain tem uma comunidade ativa de desenvolvedores e pesquisadores que contribuem para o projeto. A documentação abrangente, tutoriais e suporte da comunidade tornam mais fácil começar e navegar por quaisquer desafios que surgirem durante o desenvolvimento.\n\nA arquitetura do LangChain pode ser entendida em três componentes principais:\nCamada de Abstração: Esta camada fornece uma interface simples e unificada para interagir com diferentes LLMs. Ela abstrai as complexidades de carregar, inicializar e executar previsões em modelos, oferecendo uma API consistente independentemente do modelo subjacente.\nCamada de Processamento de Texto: O LangChain inclui ferramentas robustas para processamento de texto, incluindo tokenização, análise sintática, reconhecimento de entidades nomeadas (NER) e muito mais. Esta camada prepara os dados de entrada e saída para que possam ser processados de forma eficaz pelos modelos de linguagem.\nCamada de Modelo: Aqui é onde os próprios LLMs residem. O LangChain suporta uma variedade de modelos de linguagem, desde modelos pré-treinados de uso geral até modelos personalizados específicos de domínio. Esta camada lida com a execução de previsões, gerenciamento de recursos computacionais e interação com as APIs dos modelos.\nVamos ver um exemplo simples de como usar o LangChain para consultar um LLM e obter uma resposta. Neste exemplo, usaremos o gpt-4o-mini da OpenAI para responder a uma pergunta.\nPrimeiro, importe as bibliotecas necessárias e configure o cliente LangChain. Em seguida, carregue o modelo de linguagem desejado. Agora, você pode usar o modelo para fazer uma consulta. Vamos perguntar quem é o presidente do Brasil.\n\n# pip install langchain\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage\nimport os\n\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n\nmodel = init_chat_model(\"gpt-4o-mini\", \n    model_provider=\"openai\",                        openai_api_key=OPENAI_API_KEY)\n\nuser_message = HumanMessage(content=\"Quem é o presidente do Brasil?\")\n\nresponse = model.invoke([user_message])\n\nprint(response.content)\n\n\nAté a minha última atualização em outubro de 2023, o presidente do Brasil é Luiz Inácio Lula da Silva. Ele assumiu o cargo em janeiro de 2023. Para informações mais atualizadas, recomendo verificar fontes de notícias recentes.\n\nEste exemplo básico demonstra a simplicidade de usar o LangChain para interagir com LLMs. No entanto, o LangChain oferece muito mais recursos e funcionalidades para construir aplicativos de chatbot mais robustos.\n\n\n4.7.5 Mangaba.AI\nO Mangaba.AI é um framework escrito em Python para a criação de agentes de IA autônomos que colaboram em equipe para resolver tarefas complexas. Ele permite montar equipes de agentes com funções especializadas — por exemplo, um agente pesquisador, outro analista, outro redator — que compartilham memória contextual (isto é, histórico de interações e resultados) para dar continuidade ao trabalho de forma inteligente. Além disso, o Mangaba integra modelos avançados (incluindo os modelos Gemini do Google), permite o uso de ferramentas externas via APIs e opera de forma assíncrona para executar múltiplas tarefas em paralelo, a que melhora a eficiência e a velocidade. Ele pode ser acessado em mangaba-ai.vercel.app.\nEntre seus usos práticos, o Mangaba.AI pode ser aplicado para automação de processos repetitivos, geração de relatórios com base em dados complexos, análise e extração de informações de grandes volumes de documentos, construção de assistentes virtuais, e apoio à pesquisa e desenvolvimento. Ele procura facilitar a vida de desenvolvedores ao fornecer uma API simples e intuitiva para definir agentes, atribuir papéis, configurar equipes, e delegar tarefas inteiras para essa equipe de IA colaborativa.\n\n# para instalação acesse https://github.com/Mangaba-ai/mangaba_ai\nfrom mangaba import Team, Agent\n\n# Criar uma equipe de agentes\nmonitor = Team(\"Autorregulação\")\n\n# Adicionar agentes especializados\nplanejador = Agent(\"Plano de Estudo\", role=\"Gera Plano de Estudo com cronograma baseado no tempo disponível do estudante\")\napoio = Agent(\"Suporte Emocional\", role=\"Motiva o estudante\")\npesquisador = Agent(\"Guia de Estudo\", role=\"Busca materiais didáticos para o estudante\")\n\n# Adicionar agentes à equipe\nmonitor.add_agents([pesquisador, apoio, planejador])\n\n# Definir uma tarefa complexa\nresult = monitor.solve(\n  \"Pesquise sobre Redes Neurais Artificiais\"\n)\n\nprint(result.output)\n\n\n\n4.7.6 Fluxos em LLM (ou Engenharia de Prompts)\nModelos de Linguagem Grandes (LLMs), como a família GPT, são utilizados na compreensão e geração de texto. Uma maneira eficaz e relativamente rápida de criar um chatbot funcional é através da engenharia de prompts. Em vez de codificar regras complexas e árvores de decisão manualmente, você \"programa\" o LLM fornecendo-lhe um conjunto detalhado de instruções iniciais (o prompt).\nO prompt é o texto inicial que você fornece ao LLM. Ele define:\n\nO Papel do Chatbot: Quem ele é (um atendente de pizzaria, um consultor de moda, etc.).\nO Objetivo da Conversa: O que ele precisa alcançar (vender uma pizza, ajudar a escolher uma roupa, abrir uma conta, etc.).\nAs Regras da Conversa: A sequência exata de perguntas a fazer, as opções válidas para cada pergunta, e como lidar com diferentes respostas do usuário (lógica condicional).\nO Tom e Estilo: Se o chatbot deve ser formal, informal, amigável, etc. (embora não especificado nos exemplos, pode ser adicionado).\nO Formato da Saída Final: Como as informações coletadas devem ser apresentadas no final.\n\n\n\nComo Funciona?\n\nDefinição: Você escreve um prompt detalhado que descreve o fluxo da conversa passo a passo.\nInstrução: Você alimenta este prompt no LLM.\nExecução: O LLM usa o prompt como seu guia mestre. Ele inicia a conversa com o usuário seguindo o primeiro passo definido no prompt, faz as perguntas na ordem especificada, valida as respostas (se instruído), segue os caminhos condicionais e, finalmente, gera a saída desejada.\nIteração: Se o chatbot não se comportar exatamente como esperado, você ajusta e refina o prompt até que ele siga as regras perfeitamente.\n\nAlgumas Vantagens do uso de Fluxos em Chatbots:\n\nRapidez: Muito mais rápido do que desenvolver um chatbot tradicional do zero.\nFlexibilidade: Fácil de modificar o comportamento alterando o prompt.\nCapacidade Conversacional: Aproveita a habilidade natural do LLM para conversas fluidas.\n\nAlgumas Limitações do uso de Fluxos em Chatbots:\n\nControle Fino: Pode ser mais difícil garantir que sempre siga exatamente um caminho lógico muito complexo, embora prompts detalhados minimizem isso.\nEstado: Gerenciar estados complexos ao longo de conversas muito longas pode exigir técnicas de prompt mais avançadas.\n\n\n\nExemplos de requisitos\nVamos simular uma necessidade real de alguns clientes em 5 exercícios. Dados os requisitos de negócio a seguir, iremos implementar os chatbots utilizando LLM. Portanto, primeiro vem o requisito de negócio e, depois de apresentados todos os requisitos, a solução de sua implantação somente utilizando prompts.\n\n\nRequisitos\n\nConstrua um chatbot para uma pizzaria. O chatbot será responsável por vender uma pizza.\nVerifique com o usuário qual o o tipo de massa desejado da pizza (pan ou fina).\nVerifique o recheio (queijo, calabresa ou bacon)\nSe o usuário escolheu massa pan verifique qual o recheio da borda (gorgonzola ou cheddar)\nAo final deve ser exibido as opções escolhidas.\n\n\nConstrua um chatbot para uma loja de roupas, o chatbot será responsável por vender uma calça ou camisa.\nVerifique se o usuário quer uma calça ou uma camisa.\nSe o usuário quiser uma calça:\npergunte o tamanho da calça (34, 35 ou 36)\npergunte o tipo de fit da calça pode ser slim fit, regular fit, skinny fit.\nSe ele quiser uma camisa:\nverifique se a camisa é (P, M ou G)\nverifique se ele deseja gola (v, redonda ou polo).\nAo final informe as opções escolhidas com uma mensagem informando que o pedido está sendo processado.\n\n\nEste chatbot deve ser utilizado por uma empresa de turismo para vender um pacote turístico\nVerifique com o usuário quais das cidades disponíveis ele quer viajar (maceio, aracaju ou fortaleza)\nSe ele for para maceio:\nverifique se ele já conhece as belezas naturais da cidade.\nsugira os dois pacotes (nove ilhas e orla de alagoas)\nSe ele for a aracaju:\nverifique com o usuário quais dos dois passeios disponíveis serão escolhidos. existem duisponíveis um na passarela do carangueijo e outro na orla de aracaju.\ninforme que somente existe passagem de ônibus e verifique se mesmo assim ele quer continuar\nCaso ele deseje ir a fortaleza:\ninforme que o único pacote são as falasias cearenses.\nverifique se ele irá de ônibus ou de avião para o ceará\nVerifique a forma de pagamento cartão ou débito em todas as opções.\nAo final informe as opções escolhidas com uma mensagem informando que o pedido está sendo processado.\n\n\nCrie uma aplicação para um banco que será responsável por abrir uma conta corrente para um usuário.\nVerifique se o usuário já tem conta em outros bancos.\nCaso o usuário tenha conta em outros bancos verifique se ele quer fazer portabilidade\nVerifique o nome do correntista.\nVerifique qual o saldo que será depositado, zero ou um outro valor inicial.\nVerifique se o usuário quer um empréstimo.\nAo final informe o nome do correntista, se ele quis um empréstimo e se ele fez portabilidade e o valor inicial da conta.\n\n\nDesenvolver um chatbot para realização de matricula em duas disciplinas eletivas.\nO chatbot apresenta as duas disciplinas eletivas (Inteligência artificial Avançado, Aprendizagem de Máquina)\nVerificar se ele tem o pré-requisito introdução a programação para ambas as disciplinas.\nSe ele escolher Inteligência artificial avançada necessário confirmar se ele cursou inteligência artificial.\nAo final informe qual o nome das disciplina em que ele se matriculou.\n\n\n\nSolução\nA seguir, mostramos como os fluxos de conversa do exercício anterior podem ser traduzidos em prompts para um LLM. Cada prompt instrui o modelo a agir como o chatbot específico e seguir as regras definidas.\n\nVocê é um chatbot de atendimento de uma pizzaria. Sua tarefa é anotar o pedido de pizza de um cliente. \nNão responda nada fora deste contexto. Diga que não sabe.\nSiga EXATAMENTE estes passos:\n1.  Pergunte ao cliente qual o tipo de massa desejado. As únicas opções válidas são \"pan\" ou \"fina\".\n    * Exemplo de pergunta: \"Olá! Qual tipo de massa você prefere para sua pizza: pan ou fina?\"\n2.  Depois que o cliente escolher a massa, pergunte qual o recheio desejado. As únicas opções válidas são \"queijo\", \"calabresa\" ou \"bacon\".\n    * Exemplo de pergunta: \"Ótima escolha! E qual recheio você gostaria: queijo, calabresa ou bacon?\"\n3.  APENAS SE o cliente escolheu a massa \"pan\" no passo 1, pergunte qual o recheio da borda. As únicas opções válidas são \"gorgonzola\" ou \"cheddar\".\n    * Exemplo de pergunta (apenas para massa pan): \"Para a massa pan, temos borda recheada! Você prefere com gorgonzola ou cheddar?\"\n4.  Após coletar todas as informações necessárias (massa, recheio e recheio da borda, se aplicável), exiba um resumo claro do pedido com todas as opções escolhidas pelo cliente.\n    * Exemplo de resumo: \"Perfeito! Seu pedido ficou assim: Pizza com massa [massa escolhida], recheio de [recheio escolhido] [se aplicável: e borda recheada com [recheio da borda escolhido]].\"\nInicie a conversa agora seguindo o passo 1.\n\n\nVocê é um chatbot de vendas de uma loja de roupas. Seu objetivo é ajudar o cliente a escolher uma calça ou uma camisa. \nNão responda nada fora deste contexto. Diga que não sabe.\nSiga EXATAMENTE estes passos:\n1.  Pergunte ao cliente se ele está procurando por uma \"calça\" ou uma \"camisa\".\n    * Exemplo de pergunta: \"Bem-vindo(a) à nossa loja! Você está procurando por uma calça ou uma camisa hoje?\"\n2.  SE o cliente responder \"calça\":\n    a.  Pergunte o tamanho da calça. As únicas opções válidas são \"34\", \"35\" ou \"36\".\n        * Exemplo de pergunta: \"Para calças, qual tamanho você usa: 34, 35 ou 36?\"\n    b.  Depois do tamanho, pergunte o tipo de fit da calça. As únicas opções válidas são \"slim fit\", \"regular fit\" ou \"skinny fit\".\n        * Exemplo de pergunta: \"E qual tipo de fit você prefere: slim fit, regular fit ou skinny fit?\"\n3.  SE o cliente responder \"camisa\":\n    a.  Pergunte o tamanho da camisa. As únicas opções válidas são \"P\", \"M\" ou \"G\".\n        * Exemplo de pergunta: \"Para camisas, qual tamanho você prefere: P, M ou G?\"\n    b.  Depois do tamanho, pergunte o tipo de gola. As únicas opções válidas são \"V\", \"redonda\" ou \"polo\".\n        * Exemplo de pergunta: \"E qual tipo de gola você gostaria: V, redonda ou polo?\"\n4.  Após coletar todas as informações (tipo de peça e suas especificações), apresente um resumo das opções escolhidas e informe que o pedido está sendo processado.\n    * Exemplo de resumo (Cal\\c{c}a): \"Entendido! Voc\\^e escolheu uma cal\\c{c}a tamanho [tamanho] com fit [fit]. Seu pedido est\\'a sendo processado.\"\n    * Exemplo de resumo (Camisa): \"Entendido! Você escolheu uma camisa tamanho [tamanho] com gola [gola]. Seu pedido está sendo processado.\"\nInicie a conversa agora seguindo o passo 1.\n\n\nVocê é um agente de viagens virtual de uma empresa de turismo. Sua tarefa é ajudar um cliente a escolher e configurar um pacote turístico. \nNão responda nada fora deste contexto. Diga que não sabe.\nSiga EXATAMENTE estes passos:\n1.  Pergunte ao cliente para qual das cidades disponíveis ele gostaria de viajar. As únicas opções são \"Maceió\", \"Aracaju\" ou \"Fortaleza\".\n    * Exemplo de pergunta: \"Olá! Temos ótimos pacotes para Maceió, Aracaju e Fortaleza. Qual desses destinos te interessa mais?\"\n2.  SE o cliente escolher \"Maceió\":\n    a.  Pergunte se ele já conhece as belezas naturais da cidade. (A resposta não altera o fluxo, é apenas conversacional).\n        * Exemplo de pergunta: \"Maceió é linda! Você já conhece as belezas naturais de lá?\"\n    b.  Sugira os dois pacotes disponíveis: \"Nove Ilhas\" e \"Orla de Alagoas\". Pergunte qual ele prefere.\n        * Exemplo de pergunta: \"Temos dois pacotes incríveis em Maceió: 'Nove Ilhas' e 'Orla de Alagoas'. Qual deles você prefere?\"\n    c.  Vá para o passo 5.\n3.  SE o cliente escolher \"Aracaju\":\n    a.  Pergunte qual dos dois passeios disponíveis ele prefere: \"Passarela do Caranguejo\" ou \"Orla de Aracaju\".\n        * Exemplo de pergunta: \"Em Aracaju, temos passeios pela 'Passarela do Caranguejo' e pela 'Orla de Aracaju'. Qual te atrai mais?\"\n    b.  Informe ao cliente que para Aracaju, no momento, só temos transporte via ônibus. Pergunte se ele deseja continuar mesmo assim.\n        * Exemplo de pergunta: \"Importante: para Aracaju, nosso transporte é apenas de ônibus. Podemos continuar com a reserva?\"\n    c.  Se ele confirmar, vá para o passo 5. Se não, agradeça e encerre.\n4.  SE o cliente escolher \"Fortaleza\":\n    a.  Informe que o pacote disponível é o \"Falésias Cearenses\".\n        * Exemplo de informação: \"Para Fortaleza, temos o pacote especial 'Falésias Cearenses'.\"\n    b.  Pergunte se ele prefere ir de \"ônibus\" ou \"avião\" para o Ceará.\n        * Exemplo de pergunta: \"Como você prefere viajar para o Ceará: de ônibus ou avião?\"\n    c.  Vá para o passo 5.\n5.  Depois de definir o destino, pacote/passeio e transporte (se aplicável), pergunte qual a forma de pagamento preferida. As únicas opções são \"cartão\" ou \"débito\".\n    * Exemplo de pergunta: \"Para finalizar, como você prefere pagar: cartão ou débito?\"\n6.  Ao final, apresente um resumo completo das opções escolhidas (destino, pacote/passeio, transporte se aplicável, forma de pagamento) e informe que o pedido está sendo processado.\n    * Exemplo de resumo: \"Confirmado! Seu pacote para [Destino] inclui [Pacote/Passeio], transporte por [Ônibus/Avião, se aplicável], com pagamento via [Forma de Pagamento]. Seu pedido está sendo processado!\"\n\nInicie a conversa agora seguindo o passo 1.\n\n\nVocê é um assistente virtual de um banco e sua função é auxiliar usuários na abertura de uma conta corrente. \nNão responda nada fora deste contexto. Diga que não sabe.\nSiga EXATAMENTE estes passos:\n1.  Pergunte ao usuário se ele já possui conta em outros bancos. Respostas esperadas: \"sim\" ou \"não\".\n    * Exemplo de pergunta: \"Bem-vindo(a) ao nosso banco! Para começar, você já possui conta corrente em alguma outra instituição bancária?\"\n2.  APENAS SE a resposta for \"sim\", pergunte se ele gostaria de fazer a portabilidade da conta para o nosso banco. Respostas esperadas: \"sim\" ou \"não\".\n    * Exemplo de pergunta: \"Entendido. Você gostaria de solicitar a portabilidade da sua conta existente para o nosso banco?\"\n3.  Pergunte o nome completo do futuro correntista.\n    * Exemplo de pergunta: \"Por favor, informe o seu nome completo para o cadastro.\"\n4.  Pergunte qual será o valor do depósito inicial na conta. Informe que pode ser \"zero\" ou qualquer outro valor.\n    * Exemplo de pergunta: \"Qual valor você gostaria de depositar inicialmente? Pode ser R$ 0,00 ou outro valor à sua escolha.\"\n5.  Pergunte se o usuário tem interesse em solicitar um empréstimo pré-aprovado junto com a abertura da conta. Respostas esperadas: \"sim\" ou \"não\".\n    * Exemplo de pergunta: \"Você teria interesse em verificar uma oferta de empréstimo pré-aprovado neste momento?\"\n6.  Ao final, apresente um resumo com as informações coletadas: nome do correntista, se solicitou portabilidade (sim/não), se solicitou empréstimo (sim/não) e o valor do depósito inicial.\n    * Exemplo de resumo: \"Perfeito! Finalizamos a solicitação. Resumo da abertura: Correntista: [Nome Completo], Portabilidade Solicitada: [Sim/Não], Empréstimo Solicitado: [Sim/Não], Depósito Inicial: R$ [Valor].\"\n\nInicie a conversa agora seguindo o passo 1.\n\n\nVocê é um assistente de matrícula de uma universidade. Sua tarefa é ajudar um aluno a se matricular em até duas disciplinas eletivas. \nNão responda nada fora deste contexto. Diga que não sabe.\nSiga EXATAMENTE estes passos:\n1.  Apresente as duas disciplinas eletivas disponíveis: \"Inteligência Artificial Avançado\" e \"Aprendizagem de Máquina\".\n    * Exemplo de apresentação: \"Olá! Temos duas disciplinas eletivas disponíveis para matrícula: 'Inteligência Artificial Avançado' e 'Aprendizagem de Máquina'.\"\n2.  Verifique se o aluno possui o pré-requisito obrigatório \"Introdução à Programação\", que é necessário para AMBAS as disciplinas. Pergunte se ele já cursou e foi aprovado nesta disciplina. Respostas esperadas: \"sim\" ou \"não\".\n    * Exemplo de pergunta: \"Para cursar qualquer uma delas, é necessário ter sido aprovado em 'Introdução à Programação'. Você já cumpriu esse pré-requisito?\"\n3.  SE a resposta for \"não\", informe que ele não pode se matricular nas eletivas no momento e encerre a conversa.\n    * Exemplo de mensagem: \"Entendo. Infelizmente, sem o pré-requisito 'Introdução à Programação', não é possível se matricular nestas eletivas agora. Procure a coordenação para mais informações.\"\n4.  SE a resposta for \"sim\" (possui o pré-requisito):\n    a.  Pergunte em qual(is) das duas disciplinas ele deseja se matricular. Ele pode escolher uma ou ambas.\n        * Exemplo de pergunta: \"Ótimo! Em qual(is) disciplina(s) você gostaria de se matricular: 'Inteligência Artificial Avançado', 'Aprendizagem de Máquina' ou ambas?\"\n    b.  APENAS SE o aluno escolher \"Inteligência Artificial Avançado\" (seja sozinha ou junto com a outra), pergunte se ele já cursou a disciplina \"Inteligência Artificial\". Respostas esperadas: \"sim\" ou \"não\".\n        * Exemplo de pergunta (se escolheu IA Avançado): \"Para cursar 'Inteligência Artificial Avançado', é recomendado ter cursado 'Inteligência Artificial' anteriormente. Você já cursou essa disciplina?\"\n        * (Nota: O prompt original não especifica o que fazer se ele NÃO cursou IA. Vamos assumir que ele ainda pode se matricular, mas a pergunta serve como um aviso ou coleta de dados).\n    c.  Após coletar as escolhas e a informação sobre IA (se aplicável), informe as disciplinas em que o aluno foi efetivamente matriculado. Liste apenas as disciplinas que ele escolheu E para as quais ele confirmou ter os pré-requisitos verificados neste fluxo (no caso, 'Introdução à Programação').\n        * Exemplo de finalização (matriculado em ambas, confirmou IA): \"Matrícula realizada com sucesso! Você está matriculado em: Inteligência Artificial Avançado e Aprendizagem de Máquina.\"\n        * Exemplo de finalização (matriculado apenas em Aprendizagem de Máquina): \"Matrícula realizada com sucesso! Você está matriculado em: Aprendizagem de Máquina.\"\n        * Exemplo de finalização (matriculado em IA Avançado, mesmo sem ter cursado IA antes): \"Matrícula realizada com sucesso! Você está matriculado em: Inteligência Artificial Avançado.\"\nInicie a conversa agora seguindo o passo 1.\n\nVeja na Figura 4.11 um exemplo de implementação e diálogo quando utilizado o ChatGPT.\n\n\n \n\n\nFigure 28: Chatbot criado com LLM (via ChatGPT).\n\n\nA qualidade da resposta de um LLM depende muito da clareza e do detalhamento do prompt. Quanto mais específicas forem as instruções, maior a probabilidade de o chatbot se comportar exatamente como desejado. Entretanto, no período em que este livro foi escrito, os LLMs eram bons em detectar intenções, mas ainda não tão eficientes em seguir instruções complexas. Por isso, frameworks de orquestração de agentes, como o mangaba.ia ou o crewAI, mostram-se úteis, pois agentes mais atômicos tendem a performar melhor. A utilização da orquestração — encadeando pequenos agentes com intenções bem definidas e transferindo parte das instruções para a comunicação entre eles, com o apoio desses frameworks — tem recebido boa aceitação pela comunidade.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#integração-de-técnicas",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#integração-de-técnicas",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.8 Integração de Técnicas",
    "text": "4.8 Integração de Técnicas\nNesta seção, vamos explorar como integrar as técnicas discutidas nas seções anteriores para construir chatbots eficientes. Abordaremos a utilização de Modelos de Linguagem Grande (LLMs), Fine-Tuning, Retrieval-Augmented Generation (RAG) e LLaMA em um único sistema, visando criar experiências de diálogo sofisticadas e personalizadas.\nDesenhando um Chatbot: A construção de um chatbot avançado requer a combinação de várias técnicas para garantir que ele seja capaz de entender, processar e responder a uma ampla gama de consultas de usuários. Vamos revisar os principais componentes:\n\nModelo de Linguagem Grande (LLM): A base para a compreensão e geração de linguagem natural.\nFine-Tuning: Adaptar o LLM a domínios ou tarefas específicas.\nRetrieval-Augmented Generation (RAG): Melhorar a relevância e precisão das respostas, combinando recuperação de informações com geração de texto.\nLLaMA: Utilizar um modelo mais eficiente para sistemas de produção que precisam balancear desempenho e custo.\n\nArquitetura de um Chatbot: A arquitetura de um chatbot pode ser desenhada de forma modular, combinando diferentes técnicas de acordo com a necessidade da aplicação. Primeiro, a entrada do usuário é capturada e tokenizada; em seguida, a entrada é analisada para determinar a intenção e extrair entidades importantes. Se necessário, o chatbot recupera informações relevantes de uma base de dados externa. Depois, utiliza-se o LLM, potencialmente ajustado com Fine-Tuning, para gerar uma resposta baseada na entrada e nas informações recuperadas. Por fim, a resposta gerada é enviada de volta ao usuário. Veja na Figura 4.12 um diagrama visual deste procedimento.\n\n\n \n\n\nFigure 29: Fluxo de dados em um chatbot.\n\n\nImplementação de um Chatbot com LLaMA e RAG: Vamos agora implementar um chatbot que utiliza LLaMA como o modelo principal para geração de respostas e RAG para recuperar informações adicionais, se necessário.\nConfiguração Inicial: Primeiro, configuramos os componentes principais, como o modelo LLaMA para geração de respostas e DPR (Dense Passage Retrieval) para recuperação de informações. Recomendamos utilizar o Google Colab com a configuração de Processador A100. Os 3 blocos de código a seguir devem ser executados sequencialmente, um após o outro.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DPRQuestionEncoder, DPRContextEncoder\n\n# Carregar o tokenizer e o modelo LLaMA\nllama_tokenizer = AutoTokenizer.from_pretrained(\n\"meta-llama/LLaMA-7B\")\nllama_model = AutoModelForCausalLM.from_pretrained(\n\"meta-llama/LLaMA-7B\")\n\n# Configurar DPR para recuperação de passagens\nquestion_encoder = DPRQuestionEncoder.from_pretrained( \"facebook/dpr-question_encoder-single-nq-base\")\ncontext_encoder = DPRContextEncoder.from_pretrained( \"facebook/dpr-ctx_encoder-single-nq-base\")\n\n\nSome weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\nProcessamento da Entrada e Recuperação de Informações: Em seguida, implementamos a lógica para processar a entrada do usuário e, se necessário, recuperar informações relevantes de uma base de dados. O bloco de código abaixo deve ser executado no Google Colab, em outra célula, depois do código anterior.\n\n# Este código deve ser executado na sequencia, depois do código anterior.\nimport torch\n\ndef retrieve_relevant_passage(query, passages):\n    query_input = question_tokenizer(query, return_tensors=\"pt\")\n    query_embedding = question_encoder(\n       **query_input).pooler_output\n\n    passage_inputs = context_tokenizer(passages, padding=True, truncation=True, return_tensors=\"pt\")\n    passage_embeddings = context_encoder(\n      **passage_inputs).pooler_output\n\n    similarity_scores = torch.matmul(query_embedding, passage_embeddings.T)\n    best_passage_index = torch.argmax(similarity_scores, dim=1).item()\n\n    return passages[best_passage_index]\n\n# Exemplo de passagens\npassages = [\n    \"LLaMA é um modelo de linguagem desenvolvido pela Meta AI.\",\n    \"RAG combina recuperação de informações com geração de texto.\",\n    \"GPT-3 é um dos maiores modelos de linguagem disponíveis.\"\n]\n\n# Entrada do usuário\nuser_input = \"O que é LLaMA?\"\n\n# Recuperar a passagem mais relevante\nrelevant_passage = retrieve_relevant_passage(user_input, passages)\n\nprint(f\"Passagem mais relevante: {relevant_passage}\")\n\n\nPassagem mais relevante: LLaMA é um modelo de linguagem desenvolvido pela Meta AI.\n\nGeração de Resposta com LLaMA: Finalmente, usamos o modelo LLaMA para gerar uma resposta, utilizando tanto a entrada original do usuário quanto a passagem recuperada. O bloco de código abaixo deve ser executado no Google Colab, em outra célula, depois do código anterior.\n\n# Concatenar a consulta com a passagem relevante\ninput_text = user_input + \" \" + relevant_passage\n\n# Geração da resposta\ninput_ids = llama_tokenizer.encode(input_text, return_tensors=\"pt\")\ngenerated_ids = llama_model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\nresponse = llama_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"Resposta do chatbot:\", response)\n\n\nResposta do chatbot: O que é LLaMA? LLaMA é um modelo de linguagem desenvolvido pela Meta AI. LLaMA é um acrônimo para Large Language Model, que significa modelo de linguagem grande. LLaMA é\n\nNeste exemplo, o chatbot gera uma resposta baseada na combinação da entrada do usuário e na informação relevante recuperada, criando uma resposta informativa e contextualizada.\nDesafios na Implementação de Chatbots: Implementar chatbots com múltiplas técnicas apresenta vários desafios, tais como: combinar modelos como LLaMA e DPR pode ser computacionalmente intensivo, especialmente em aplicações em tempo real; o Fine-Tuning e a configuração de modelos precisam ser bem ajustados para garantir que o chatbot seja eficaz e relevante em suas respostas; além disso, garantir que os diferentes componentes (recuperação, geração, etc.) funcionem de maneira coesa pode ser desafiador. Mesmo assim, os chatbots podem ser aplicados em uma variedade de cenários, como: fornecer suporte automatizado, personalizado, em tempo real; ajudar na triagem de sintomas ou fornecer informações médicas básicas; ou mesmo criar tutores virtuais que podem responder a perguntas de estudantes de maneira precisa e contextualizada.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#api-e-playground",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#api-e-playground",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.9 API e Playground",
    "text": "4.9 API e Playground\nGeralmente, a desenvolvedora de um modelo LLM fornece uma interface web para o usuário conversar com o seu modelo em forma de chatbot e fornece uma API (com um playground) para quem deseja integrar a solução em suas aplicações. Assim é o ChatGPT, da empresa OpenAI. Ela cobra um valor fixo para disponibilizar aos assinantes acesso completo aos recursos do seu chatbot, inclusive com o recurso de SearchGPT (uma busca na web), lousa, geração de vídeos, imagens etc. Já para acesso à API, a modalidade de cobrança é paga-pelo-uso. Veja na Figura 4.13 uma imagem do chatbot da OpenAI e na Figura 4.14 o playground da OpenAI.\n\n\n \n\n\nFigure 30: Chatbot da OpenAI.\n\n\n\n\n \n\n\nFigure 31: Playground da openai.\n\n\nExiste uma provedora de inferência chamada GROQ https://groq.com/. O GROQ (com Q) é uma fornecedora de infraestrutura para inferência em modelos de linguagem de grande porte (LLM). Esta empresa vende um serviço de inferência mais rápido do que seus concorrentes. No GROQ, podemos utilizar vários modelos LLMs disponíveis abertamente, tais como o Llama da Meta, o Gemma do Google ou o Mixtral da Mixtral AI. O ChatGPT não está disponível no GROQ pois ele não é open source.\nEntre outros modelos, utilizam uma versão do LLaMA em seu Chatbot GROQ, porém seu foco é a API, que pode ser testada no playground GROQ. O Playground funciona no formato pague-pelo-uso. O Chatbot GROQ também consome os mesmos créditos pague-pelo-uso, não tendo uma assinatura à parte, como no caso do chatbot do ChatGPT. Veja na Figura 4.15 o Chatbot do GROQ e na Figura 4.16 o Playground do GROQ. Note que no Chatbot do GROQ não existe o recurso de histórico como no ChatGPT.\n\n\n \n\n\nFigure 32: Chatbot do GROQ.\n\n\n\n\n \n\n\nFigure 33: Playground do GROQ.\n\n\nJá o Grok (com K) é utilizado em referência ao chatbot da empresa xAI. Ele é um chatbot e está disponível para quem tem o acesso premium ao X (ex-twitter), enquanto a API Grok pode ser acessada na forma pague-pelo-uso. Inclusive, o Grok está oferecendo este ano 25 R$ para quem deseja testar o serviço, ou seja, é possível utilizar a API do Grok sem desembolsar nada neste momento. Porém, o Grok não tem um playground avançado, logo, para usar a API do Grok é necessário utilizar uma ferramenta gráfica ou código externo, por exemplo: LLM Studio, JAN AI, GPT4ALL ou o excelente Msty app. Veja na Figura 4.17 o chatbot do Grok e na Figura 4.18 o playground do Grok.\n\n\n \n\n\nFigure 34: Interface do Grok.\n\n\n\n\n \n\n\nFigure 35: Playground do Grok.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#resumo",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#resumo",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.10 Resumo",
    "text": "4.10 Resumo\nOs Modelos de Linguagem Grande (LLMs) têm desempenhado um papel central na revolução do Processamento de Linguagem Natural (PLN). Esses modelos, que incluem variantes como GPT, BERT, e seus sucessores, são capazes de realizar uma ampla gama de tarefas, desde geração de texto até compreensão profunda de linguagem, graças ao seu treinamento em grandes volumes de dados textuais.\nEsses modelos são alimentados por vastos conjuntos de dados textuais e utilizam técnicas de aprendizado profundo, particularmente redes neurais, para aprender padrões, contextos e nuances da linguagem. Eles são capazes de realizar uma variedade de tarefas linguísticas, incluindo tradução automática, geração de texto, resumo de informações, resposta a perguntas e até mesmo a criação de diálogos interativos. Funcionam com base em arquiteturas complexas, como a Transformer, que permite que o modelo preste atenção a diferentes partes de um texto simultaneamente, facilitando a compreensão do contexto e das relações semânticas entre palavras e frases.\nO treinamento desses modelos envolve a exposição a enormes quantidades de texto, o que lhes permite desenvolver uma compreensão profunda da gramática, do vocabulário e dos estilos de comunicação. No entanto, essa capacidade de gerar texto coerente e relevante também levanta questões éticas e de responsabilidade, especialmente em relação à desinformação, viés algorítmico e privacidade.\nEles hoje representam um marco significativo na evolução da inteligência artificial, assim como foi o Eliza em sua época, porém com um impacto maior, oferecendo ferramentas para a interação humano-computador e abrindo novas possibilidades para aplicações em diversas áreas, como educação [@Neo2020Engajando], atendimento ao cliente ou criação de conteúdo.\nNeste capítulo, exploramos a arquitetura Transformer, que revolucionou o Processamento de Linguagem Natural ao introduzir o mecanismo de atenção. Discutimos como os Transformers funcionam e como eles são aplicados em modelos populares como BERT e GPT. Com exemplos em Python, vimos como utilizar esses modelos para tarefas de PLN.\nTambém, exploramos a técnica de Retrieval-Augmented Generation (RAG), uma abordagem que combina a recuperação de informações com a geração de texto. Vimos como implementar RAG em Python usando modelos como DPR e BART, e discutimos as aplicações e desafios dessa técnica.\nAdicionalmente, exploramos os LLMs incluindo suas arquiteturas, técnicas de treinamento e principais aplicações. Modelos como GPT, BERT, T5 e XLNet exemplificam como os LLMs estão redefinindo o campo do PLN. Com exemplos práticos, demonstramos como esses modelos podem ser aplicados em uma variedade de tarefas.\nOutro tópico explorado neste capítulo foi o processo de Fine-Tuning de modelos pré-treinados, discutindo sua importância, desafios e aplicações práticas. O Fine-Tuning permite que Modelos de Linguagem Grande sejam adaptados para tarefas específicas com eficiência, tornando-os extremamente versáteis em diversas aplicações de PLN.\nTambém, exploramos o LLaMA, um modelo de linguagem projetado para ser eficiente e acessível sem comprometer a capacidade de realizar tarefas complexas de PLN. Com uma arquitetura otimizada e foco em eficiência computacional, LLaMA oferece uma alternativa prática para modelos gigantescos como o GPT-3. Vimos também como implementar LLaMA em Python para tarefas de geração de texto e discutimos suas aplicações e limitações.\nAlém disso, integramos várias técnicas discutidas em capítulos anteriores para criar um chatbot capaz de fornecer respostas precisas e contextualizadas utilizando LLaMA e RAG. A implementação dessas técnicas oferece uma pequena base para o desenvolvimento de sistemas de diálogo eficientes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#exercícios",
    "href": "sections/7_modelos-de-linguagem-grande-llm-cap-llm.html#exercícios",
    "title": "4  Modelos de Linguagem Grande (LLM)",
    "section": "4.11 Exercícios",
    "text": "4.11 Exercícios\n\nQual é a principal inovação introduzida pelos Transformers em relação a modelos anteriores?\n\nA) O uso de redes neurais convolucionais para processamento de texto.\nB) A capacidade de processar sequências em paralelo, utilizando mecanismos de atenção.\nC) A utilização de redes neurais recorrentes para manter o contexto ao longo das sequências.\nD) A introdução de embeddings de palavras em redes neurais.\n\nO que é o \"mecanismo de atenção\" em Transformers?\n\nA) Um mecanismo que aumenta a frequência das palavras mais comuns.\nB) Um algoritmo que distribui o foco igualmente entre todas as palavras em uma sequência.\nC) Um método que permite ao modelo focar em partes específicas da entrada ao gerar uma saída, ponderando a importância das diferentes partes da sequência.\nD) Uma técnica para ignorar palavras irrelevantes em um texto.\n\nO que significa o termo \"self-attention\" no contexto dos Transformers?\n\nA) O modelo ajusta automaticamente seu aprendizado com base no erro de previsão.\nB) O modelo atribui pesos a diferentes partes da sequência de entrada para determinar quais partes são mais relevantes ao processar cada palavra.\nC) O modelo decide qual sequência de palavras é mais provável com base em exemplos anteriores.\nD) O modelo ignora todas as palavras, exceto a palavra alvo.\n\nQual é o papel do \"positional encoding\" nos Transformers?\n\nA) Ajudar o modelo a entender a ordem das palavras em uma sequência, já que o Transformer processa todas as palavras em paralelo.\nB) Substituir palavras desconhecidas por sinônimos.\nC) Compactar a representação de texto para economizar espaço de armazenamento.\nD) Permitir ao modelo focar em palavras específicas dentro de uma frase.\n\nQual das seguintes afirmações é verdadeira sobre a arquitetura Transformer?\n\nA) Os Transformers utilizam camadas convolucionais para processar texto.\nB) Os Transformers dependem exclusivamente de redes neurais recorrentes para manter o contexto.\nC) Os Transformers eliminam a necessidade de processar sequências em ordem, graças ao mecanismo de atenção e à paralelização.\nD) Os Transformers não são capazes de lidar com longas sequências de texto devido a limitações de memória.\n\nQual é a principal característica dos Modelos de Linguagem Grande (LLMs) como GPT e BERT?\n\nA) Eles são baseados exclusivamente em redes neurais convolucionais.\nB) Eles são treinados em grandes volumes de dados textuais e são capazes de realizar tarefas de PLN sem a necessidade de re-treinamento específico para cada tarefa.\nC) Eles dependem exclusivamente de dicionários pré-definidos para gerar respostas.\nD) Eles utilizam redes neurais recorrentes para prever a próxima palavra em uma sequência.\n\nQual é a diferença principal entre os modelos GPT e BERT?\n\nA) O GPT utiliza um mecanismo de atenção bidirecional, enquanto o BERT utiliza atenção unidirecional.\nB) O GPT é um modelo autoregressivo que gera texto palavra por palavra, enquanto o BERT é um modelo pré-treinado bidirecional para tarefas de preenchimento de máscara.\nC) O BERT é projetado apenas para geração de texto, enquanto o GPT é projetado apenas para compreensão de texto.\nD) O BERT utiliza aprendizado supervisionado, enquanto o GPT utiliza aprendizado não supervisionado.\n\nQual é uma das principais vantagens do ajuste fino de modelos pré-treinados?\n\nA) Reduz a necessidade de dados de treinamento específicos para uma nova tarefa.\nB) Garante que o modelo não precisará ser treinado novamente.\nC) Remove completamente a necessidade de validação cruzada.\nD) Evita qualquer risco de overfitting.\n\nEm que tipo de tarefa o ajuste fino é particularmente útil?\n\nTarefas que exigem uma grande quantidade de dados não rotulados.\nTarefas específicas que requerem adaptação de um modelo geral para um domínio particular.\nTarefas que não envolvem aprendizado de máquina.\nTarefas de compressão de dados para armazenamento eficiente.\n\nQual das seguintes abordagens é recomendada ao realizar ajuste fino para evitar overfitting?\n\nAumentar a taxa de aprendizado para forçar o modelo a aprender rapidamente.\nCongelar algumas camadas do modelo pré-treinado e treinar apenas as camadas superiores.\nReduzir drasticamente o tamanho do conjunto de dados de treinamento.\nUtilizar apenas um pequeno subconjunto do modelo pré-treinado.\n\nQual das seguintes técnicas pode ser usada para melhorar o ajuste fino em um modelo pré-treinado?\n\nAumentar o tamanho do lote para melhorar a estabilidade do treinamento.\nImplementar o decaimento da taxa de aprendizado ao longo do treinamento.\nTreinar o modelo apenas por uma época para evitar overfitting.\nUsar técnicas de normalização de batch para manter a média e variância constantes.\n\nO que é Retrieval-Augmented Generation (RAG)?\n\nUma técnica que combina a recuperação de informações com a geração de texto para criar respostas mais informadas e contextuais.\nUm método de compressão de texto que reduz o tamanho dos dados sem perda de informação.\nUma abordagem para treinar modelos de linguagem exclusivamente em dados não rotulados.\nUma técnica para traduzir textos entre diferentes idiomas.\n\nQual é a principal vantagem de usar RAG em chatbots?\n\nA capacidade de gerar respostas baseadas em dados estáticos sem necessidade de atualização.\nA habilidade de integrar informações externas e atualizadas, permitindo respostas mais precisas e relevantes.\nA eliminação da necessidade de modelos de linguagem grande.\nA redução dos custos de treinamento de modelos.\n\nNo processo de RAG, qual é o papel do componente de \"recuperação\"?\n\nGerar novas informações com base em uma sequência de texto fornecida.\nRecuperar documentos, passagens ou dados relevantes de uma base de conhecimento para serem usados na geração de uma resposta.\nExecutar a tradução de texto de um idioma para outro.\nClassificar textos em diferentes categorias.\n\nQual das seguintes estratégias é usada para melhorar a precisão das respostas em um sistema RAG?\n\nUtilizar um modelo de linguagem unidirecional.\nCombinar a recuperação de informações com a geração de texto, onde a informação recuperada guia a resposta gerada.\nImplementar somente a geração de texto sem recuperação de informações.\nUsar exclusivamente redes neurais convolucionais para processamento de texto.\n\nQual das seguintes afirmativas é verdadeira sobre a arquitetura RAG?\n\nRAG utiliza apenas modelos de recuperação e não depende de modelos de geração de texto.\nRAG é eficiente para gerar respostas em tempo real com base em grandes volumes de dados externos.\nRAG não é capaz de integrar informações externas ao contexto de uma conversa.\nRAG é uma abordagem exclusiva para análise de sentimentos em texto.\n\nQual é o principal objetivo do LLaMA (Large Language Model Meta AI)?\n\nCriar um modelo de linguagem massivo e pesado para tarefas específicas.\nOferecer um modelo de linguagem grande e eficiente que pode ser treinado e implantado com menor custo computacional.\nDesenvolver um modelo de linguagem exclusivamente para tarefas de tradução automática.\nImplementar um modelo de linguagem focado apenas em reconhecimento de voz.\n\nComo o LLaMA se diferencia de outros Modelos de Linguagem Grande (LLMs) como GPT-3?\n\nLLaMA é um modelo maior e mais caro para treinar do que GPT-3.\nLLaMA é projetado para ser mais leve e eficiente, com diferentes tamanhos de modelo, enquanto ainda oferece alto desempenho em tarefas de PLN.\nLLaMA só pode ser utilizado para tarefas de visão computacional.\nLLaMA depende de dados estruturados enquanto GPT-3 usa dados não estruturados.\n\nEm que contexto o LLaMA seria especialmente vantajoso para ser utilizado?\n\nEm dispositivos com recursos computacionais limitados, onde modelos grandes como GPT-3 não podem ser executados eficientemente.\nEm servidores de alto desempenho que exigem modelos extremamente grandes.\nEm ambientes que não necessitam de processamento de linguagem natural.\nPara operações que requerem apenas reconhecimento de fala em tempo real.\n\nQual das seguintes afirmações é verdadeira sobre a arquitetura do LLaMA?\n\nLLaMA é baseado em uma arquitetura de redes neurais convolucionais.\nLLaMA utiliza a arquitetura Transformer, otimizada para eficiência em termos de parâmetros e recursos computacionais.\nLLaMA não é capaz de realizar tarefas de compreensão de texto.\nLLaMA foi projetado exclusivamente para tarefas de visão computacional.\n\nQual é uma das principais aplicações do LLaMA em chatbots?\n\nTradução automática de textos literários complexos.\nImplementação de assistentes virtuais que precisam operar em dispositivos móveis com recursos limitados.\nAnálise de grandes volumes de imagens e vídeos.\nClassificação de sons em ambientes ruidosos.\n\nEm que tarefa o BERT se destaca em comparação ao GPT?\n\nTradução automática\nGeração de texto criativo\nPreenchimento de lacunas em uma frase (masked language modeling)\nCriação de imagens a partir de descrições textuais\n\nQual das seguintes afirmações é verdadeira sobre o modelo GPT?\n\nO GPT utiliza uma abordagem bidirecional para entender o contexto ao redor de uma palavra em uma frase.\nO GPT é um modelo autoregressivo que gera texto com base nas palavras anteriores da sequência.\nO GPT é incapaz de realizar tarefas de compreensão de texto.\nO GPT é treinado apenas em pequenas bases de dados altamente especializadas.\n\nQual é um dos principais usos de modelos como GPT e BERT em chatbots?\n\nA tradução automática de grandes textos literários.\nA geração de respostas naturais e coerentes em conversas com os usuários, simulando uma interação humana.\nA análise de imagens e vídeos para identificar objetos.\nA classificação de sons e ruídos em diferentes ambientes.\n\nQual é o principal objetivo ao integrar técnicas avançadas como RAG, Fine-Tuning e LLaMA em chatbots?\n\nCriar chatbots que operam exclusivamente em dispositivos móveis.\nMelhorar a precisão, relevância e capacidade de adaptação dos chatbots em diferentes cenários.\nReduzir o tamanho do modelo ao mínimo possível.\nEvitar o uso de inteligência artificial em chatbots.\n\nComo a técnica Retrieval-Augmented Generation (RAG) contribui para a eficiência de chatbots?\n\nReduzindo o tempo de treinamento dos modelos.\nPermitindo que o chatbot acesse e utilize informações externas para gerar respostas mais precisas e contextuais.\nSubstituindo completamente o processo de fine-tuning em modelos grandes.\nFacilitando a compressão de dados de texto em formato binário.\n\nQual é a vantagem de utilizar Fine-Tuning em um modelo como o LLaMA antes de integrá-lo em um chatbot?\n\nReduzir o custo de desenvolvimento do chatbot.\nAdaptar o modelo para responder de maneira mais eficaz a consultas específicas de um domínio particular.\nGarantir que o modelo funcione apenas em idiomas específicos.\nAumentar o número de parâmetros do modelo para melhorar a precisão.\n\nPor que é importante considerar a escalabilidade ao integrar técnicas avançadas em chatbots?\n\nPara garantir que o chatbot possa ser implantado em múltiplos idiomas sem qualquer modificação.\nara assegurar que o chatbot possa lidar com um grande volume de interações simultâneas sem perda de desempenho.\nPara eliminar a necessidade de armazenamento de dados.\nPara garantir que o chatbot possa operar sem qualquer conexão à internet.\n\nQual das seguintes estratégias pode ajudar a melhorar a personalização das respostas de um chatbot utilizando técnicas avançadas?\n\nImplementar caching de respostas comuns.\nUtilizar dados históricos de interações para ajustar as respostas às preferências do usuário.\nReduzir o número de camadas no modelo para melhorar a eficiência.\nEvitar o uso de técnicas de machine learning para gerar respostas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de Linguagem Grande (LLM)</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html",
    "href": "sections/8_conclus-o-conclusao.html",
    "title": "5  Conclusão",
    "section": "",
    "text": "5.1 Considerações Éticas\nO desenvolvimento de chatbots não envolve apenas desafios técnicos. À medida que esses sistemas se tornam cada vez mais sofisticados e amplamente utilizados, questões éticas ganham destaque. O uso de chatbots precisa ser entendido, experimentado e avaliado, para identificarmos os impactos de tal utilização, mediante experimentos, como por exemplo @Neo2020Engajando, que avaliaram seu uso em sala de aula com uma turma de alunos.\nOs impactos da Inteligência Artificial(IA) no bem-estar humano – positivo ou negativo – são mais complexos do que às vezes se supõe [@schiff2020ieee]. No campo da ética, algumas abordagens específicas, tal como a aristotélica, preconizam a noção de felicidade como cerne da conduta ética. Um representante da modernidade é a filosofia kantiana, que estabelece a distinção entre moralidade e civilidade. Kant traz à tona a noção de que o moralmente correto não é apenas a ação externa do indivíduo, pois o indicativo externo só pode ser mensurado pelas leis e normas. Uma moralidade verdadeiramente ética seria aquela que parte de uma convicção; para ele, “nós somos civilizados até a saturação por toda espécie de boas maneiras e decoro social. Mas ainda falta muito para nos considerarmos moralizados” [@kant2023fundamentaccao]. Nessa mesma perspectiva, Kant elabora seu imperativo categórico, uma máxima norteadora da ação ética: “age de tal modo que tua máxima possa ser universalizada” [@kant2017critica].\nHans Jonas, a partir de suas reflexões éticas acerca do meio ambiente, se opõe claramente à ética kantiana, Jonas assevera: “Age de tal forma que os efeitos da tua ação sejam compatíveis com a permanência de uma vida autenticamente humana sobre a terra.” [@jonas2006principio]. Jonas alarga a noção de ética, descentrando-a das ações temporais e projetando-as para o futuro; a ação do indivíduo é responsável pelos seus desdobramentos futuros.\nAo longo dos anos, importantes esforços de pesquisa têm sido dedicados à IA, e em especial aos chatbots. Tradicionalmente, um dos aspectos que mais chamou a atenção foram suas capacidades de interação destes programas com os seres humanos. Por exemplo, o Eliza foi um dos primeiros e mais influentes chatbots que se baseou em correspondência de padrões para criar esta ilusão de inteligência [@Weizenbaum1996].\nÉ sempre desafiador pensar na perspectiva moral e ética na relação tecnologia e usuário. Faz-se necessário refletir em torno da dicotomia moral e ética, pois, embora muitas vezes associados como elementos idênticos, possuem especificidades. São justamente essas especificidades que tornam candente o ensino da ética em sala de aula. Ora, podemos asseverar que ética e moral se coadunam quando pensadas como elementos do agir humano. Juízos de valor, noções de bem e mal, normas de conduta, são elementos comuns à ética e à moral. Entretanto, ética e moral separam-se quando entendemos que esta última se concentra nos valores vigentes de uma sociedade. Assim, tempo e espaço são elementos que alteram a moral, ou melhor, as morais. Dito de outra forma, a conduta valorada como correta em um período pode não ser em outro, bem como o comportamento criminoso de uma região pode, ao mesmo tempo, ser correto em outra. Esse veio que particulariza as ações dos indivíduos, de acordo com a sociedade em que vivemos, chamamos de moral.\nPor ética, em linhas gerais, consideramos as reflexões filosóficas que se desdobram sobre os diversos sistemas morais. O processo de interação homem-tecnologia deve sempre partir, em um movimento dialético, dos valores já sedimentados pelos usuários em questão. Não há grau zero do conhecimento. Ao tratarmos pessoas, deparamo-nos com seus sistemas morais, cristalizados desde a infância. O papel da filosofia, assim, não é de ser a demolidora dos valores. Entendemos que as diversas perspectivas filosóficas constituem-se essencialmente radicais, posto que se debruçam sobre a raiz e universalidade dos problemas.\nA filosofia pode ser utilizada primordialmente como elemento de crítica, sendo esta, em termos kantianos, uma “condição de possibilidade”. Intentando problematizar a dificuldade desta interação, compreendemos como a formação social da moral nos indivíduos necessita ser pensada à luz da crítica. Os usos da tecnologia, sobremaneira, impuseram uma dinâmica até então não empreendida antes do seu surgimento: a velocidade das mudanças, a internet e, recentemente, os chatbots aceleram o acesso à informação, tornando descompassada sua relação com o conhecimento. Porém, a filosofia evidencia a necessidade de ajuste do compasso entre tecnologia, conhecimento e seres humanos.\nA tecnologia pode ser aliada aos processos humanos e pode minimizar o cenário das dificuldades vividas. Porém, não basta simplesmente concluir que o benefício surge única e exclusivamente a partir da simples adoção dos chatbots. A simples incorporação dos chatbots ou uso deles não gera processos de inovação e melhoria; estes são determinados a partir dos usos específicos que parecem ter a capacidade de desencadear esses processos.\nTemos diversos desafios pela frente, entre eles: Como garantir um uso eficiente dos chatbots e qual o impacto social desta interação? É possível asseverar que há um impacto social em refletir sobre a ética no uso dos chatbots. É justamente pela falta de reflexão e crítica que temos perpetuado em nossa sociedade sistemas de dominação e corrupção. A naturalização acrítica dos valores vigentes permite que tais valores sejam transmitidos como verdades, ocultando muitas vezes os interesses individuais em detrimento dos coletivos.\nDesenvolvedores de chatbots têm a responsabilidade de garantir que seus sistemas sejam projetados e operados de maneira ética. Algumas recomendações podem incluir auditorias regulares para implementar revisões regulares de privacidade, segurança e viés para garantir a conformidade com os princípios éticos; o envio de notificações para informar os usuários de forma clara quando estão interagindo com um chatbot, e não com um humano, para evitar confusões. Além de processos para garantir que as empresas possam ser responsabilizadas por suas ações.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#privacidade-e-proteção-de-dados",
    "href": "sections/8_conclus-o-conclusao.html#privacidade-e-proteção-de-dados",
    "title": "5  Conclusão",
    "section": "5.2 Privacidade e Proteção de Dados",
    "text": "5.2 Privacidade e Proteção de Dados\nOs chatbots modernos, especialmente aqueles implantados em plataformas de atendimento ao cliente e assistentes pessoais, frequentemente processam informações pessoais e sensíveis. A coleta, armazenamento e uso de dados precisam estar em conformidade com regulamentos de proteção de dados, como o GDPR (Regulamento Geral de Proteção de Dados) na União Europeia.\nOs seguintes princípios são fundamentais para garantir a privacidade dos dados:\n\nMinimização de Dados: Coletar apenas os dados estritamente necessários para a tarefa em questão.\nTransparência: Informar os usuários sobre quais dados estão sendo coletados, como serão usados e por quanto tempo serão armazenados.\nConsentimento: Garantir que os usuários concordem explicitamente com o uso de seus dados.\nAnonimização: Sempre que possível, anonimizar os dados para evitar que informações pessoais possam ser associadas a um indivíduo específico.\n\nEste exemplo em python mostra como dados sensíveis, como informações pessoais, podem ser protegidos com criptografia em repouso e em trânsito, garantindo que apenas usuários autorizados possam acessá-los.\n\n# pip install cryptography  \nfrom cryptography.fernet import Fernet\n\n# Gerar uma chave de criptografia\nkey = Fernet.generate_key()\ncipher_suite = Fernet(key)\n\n# Criptografar dados sensíveis\ndata = \"Dados sensíveis do usuário\"\nencrypted_data = cipher_suite.encrypt(data.encode())\n\n# Descriptografar quando necessário\ndecrypted_data = cipher_suite.decrypt(encrypted_data).decode()\nprint(\"Dados descriptografados:\", decrypted_data)\n\n\nDados descriptografados: Dados sensíveis do usuário",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#viés-algorítmico",
    "href": "sections/8_conclus-o-conclusao.html#viés-algorítmico",
    "title": "5  Conclusão",
    "section": "5.3 Viés Algorítmico",
    "text": "5.3 Viés Algorítmico\nViés algorítmico ocorre quando um modelo de chatbot aprende padrões indesejáveis ou preconceituosos a partir dos dados de treinamento, levando a resultados injustos ou discriminatórios. Este problema pode se manifestar de várias formas, desde o uso de linguagem tendenciosa até a recomendação de serviços que favoreçam certos grupos em detrimento de outros.\nOs vieses nos chatbots podem surgir dos dados de treinamento, pois os dados usados para treinar o chatbot contêm preconceitos históricos; esses preconceitos podem ser perpetuados pelo modelo. Também pode existir viés nas iterações do Usuário, pois, caso o chatbot aprenda com as interações de usuários, pode absorver e amplificar vieses existentes nas entradas dos usuários. Além disso, alguns algoritmos podem inadvertidamente favorecer certos tipos de dados em detrimento de outros.\nMitigar o viés algorítmico requer abordagens proativas, tais como: a diversificação de dados de treinamento para garantir que o chatbot seja treinado em um conjunto de dados diversificado que represente várias culturas, gêneros, idades e contextos sociais; outra forma é regularmente auditar os modelos de chatbot para identificar e corrigir viés indesejado; e por fim, utilizar algoritmos que considerem equidade e justiça ao treinar o chatbot.\n\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n# Verificar o viés nas predições do chatbot\ndef evaluate_bias(true_labels, predictions):\n    cm = confusion_matrix(true_labels, predictions)\n    print(\"Matriz de confusão:\\n\", cm)\n\n# Exemplo de auditoria simples de viés\ntrue_labels = [0, 1, 1, 0, 1]  # Representa as classificações corretas\npredictions = [0, 1, 0, 0, 1]  # Predições do chatbot\n\nevaluate_bias(true_labels, predictions)\n\n\nMatriz de confusão:\n[[2 0]\n[1 2]]\n\nEste código simula uma auditoria básica para verificar a presença de viés em um modelo de chatbot, comparando predições com rótulos verdadeiros para identificar possíveis discrepâncias.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#segurança-de-chatbots",
    "href": "sections/8_conclus-o-conclusao.html#segurança-de-chatbots",
    "title": "5  Conclusão",
    "section": "5.4 Segurança de Chatbots",
    "text": "5.4 Segurança de Chatbots\nOs chatbots, especialmente aqueles integrados em sistemas críticos, são alvos potenciais para ataques cibernéticos. Garantir a segurança desses sistemas é essencial para proteger tanto os usuários quanto as empresas.\nAlgumas das principais vulnerabilidades de chatbots incluem:\n\nAtaques de Injeção de Código: Um atacante pode tentar injetar código malicioso nas entradas do chatbot para comprometer o sistema.\nAtaques de Engenharia Social: Os usuários podem ser enganados para compartilhar informações confidenciais, acreditando que estão falando com um agente confiável.\nAbuso de API: Se a API usada pelo chatbot não estiver adequadamente protegida, um atacante pode explorá-la para acessar dados ou realizar ações não autorizadas.\n\nMedidas de segurança para proteger chatbots incluem:\n\nValidação de Entrada: Implementar uma validação rigorosa das entradas do usuário para evitar ataques de injeção de código.\nAutenticação de Usuário: Exigir autenticação para acessar funcionalidades críticas do chatbot, como alteração de dados pessoais.\nLimitação de Taxa (Rate Limiting): Impedir o abuso da API limitando o número de solicitações que podem ser feitas por um usuário em um determinado período de tempo.\n\n\n# pip install flask_limiter\n# pip install flask\n\nfrom flask_limiter import Limiter\nfrom flask import Flask\n\napp = Flask(__name__)\nlimiter = Limiter(key_func=lambda: \"user_ip\")\nlimiter.init_app(app)\n\n@app.route(\"/secure-endpoint\")\n@limiter.limit(\"5 per minute\")\ndef secure_endpoint():\n    return \"Acesso seguro garantido!\"\n\n\nUserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. [...] documentation about configuring the storage backend. [...]\n\nNeste exemplo, utilizamos a limitação de taxa para proteger um endpoint sensível, garantindo que os usuários não possam abusar da API.\nImpacto Social dos Chatbots: Além das questões técnicas e de segurança, os chatbots podem ter um impacto social significativo. Eles podem influenciar o comportamento dos usuários, moldar interações sociais e até substituir trabalhos humanos em determinadas indústrias.\nSubstituição de Trabalho Humano: Embora os chatbots possam aumentar a eficiência e reduzir custos, sua implementação pode resultar na substituição de empregos, especialmente em setores de atendimento ao cliente. É importante considerar como a automação pode ser introduzida de forma ética, proporcionando requalificação e apoio a trabalhadores impactados.\nManipulação e Desinformação: Os chatbots também podem ser usados para manipulação e disseminação de desinformação. Modelos de Linguagem Grande podem ser explorados para criar bots maliciosos que disseminam fake news, discursos de ódio ou influenciam eleições. Uma possibilidade de mitigação seria implementar filtros de moderação e ferramentas de verificação de fatos em chatbots para impedir a disseminação de informações enganadoras ou prejudiciais.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#manutenção-e-atualização",
    "href": "sections/8_conclus-o-conclusao.html#manutenção-e-atualização",
    "title": "5  Conclusão",
    "section": "5.5 Manutenção e Atualização",
    "text": "5.5 Manutenção e Atualização\nImplantar um chatbot é apenas o início. Para garantir que ele continue a atender às necessidades dos usuários de forma eficaz e segura, é essencial implementar um processo contínuo de manutenção e atualização. Nesta seção, vamos explorar as melhores práticas para manter um chatbot em operação, incluindo monitoramento, atualização de modelos, coleta e análise de feedback dos usuários, e práticas para prevenir a deterioração do desempenho.\nUm chatbot que não é mantido adequadamente pode rapidamente se tornar obsoleto, irrelevante ou até prejudicial para a experiência do usuário. A manutenção contínua é necessária para:\n\nAtualizar Conhecimento: Adaptar o chatbot a novas informações, gírias ou mudanças de contexto.\nMelhorar o Desempenho: Ajustar o chatbot com base no feedback do usuário para aprimorar a precisão e a relevância das respostas.\nGarantir a Segurança: Aplicar patches de segurança e atualizações para proteger o chatbot de vulnerabilidades.\nAdaptar-se às Mudanças no Ambiente: Ajustar o chatbot às mudanças na infraestrutura técnica, como atualizações em APIs de integração.\n\nJá o monitoramento contínuo do chatbot em produção é fundamental para identificar problemas de desempenho, compreender como os usuários estão interagindo com o sistema e detectar comportamentos inesperados ou indesejados. Algumas métricas importantes para monitorar incluem:\nAtualização de Modelos: Com o tempo, os modelos de linguagem usados pelos chatbots podem se tornar desatualizados, especialmente se o domínio de aplicação estiver em constante evolução. A atualização dos modelos pode envolver realizar re-treinamento com dados novos, ou incorporar novos dados para que o modelo se adapte a mudanças de linguagem, gírias ou tópicos emergentes. Outra técnica é realizar Fine-Tuning periódico com base em novos exemplos de interações reais dos usuários.\nFeedback do Usuário e Melhorias Contínuas: O feedback dos usuários é uma fonte valiosa de informações para melhorar o chatbot. Processos automatizados e manuais de coleta e análise de feedback ajudam a identificar áreas para aprimoramento. É possível realizar a coleta de feedback via solicitação direta, perguntando diretamente aos usuários sobre sua satisfação com as respostas fornecidas, ou usar algoritmos para analisar o sentimento geral das interações dos usuários com o chatbot. De posse do feedback do chatbot, é possível refinar respostas problemáticas identificadas através do feedback para incluir novos dados ou ajustar o modelo para melhor atender às necessidades dos usuários.\nPrevenção de Deterioração de Desempenho: Com o tempo, chatbots podem sofrer deterioração de desempenho devido a mudanças no ambiente ou desgaste do modelo. Estratégias para prevenir isso incluem detectar e corrigir quando o desempenho do modelo começa a se degradar em relação a novas entradas de dados; além de avaliações regulares, realizando testes regulares para garantir que o chatbot continue operando conforme esperado; por fim, manter versões anteriores do chatbot para comparação e possível rollback em caso de problemas com novas versões.\nSegurança e Privacidade: Garantir a segurança contínua e a privacidade dos dados do usuário é uma prioridade. Isso envolve manter o ambiente e as dependências do chatbot atualizados com os patches de segurança mais recentes, o que chamamos de Aplicação de Patches de Segurança; além de implementar medidas para proteger dados sensíveis, incluindo criptografia de dados em repouso e em trânsito, e conformidade com regulamentos como o GDPR.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#perspectivas-futuras",
    "href": "sections/8_conclus-o-conclusao.html#perspectivas-futuras",
    "title": "5  Conclusão",
    "section": "5.6 Perspectivas Futuras",
    "text": "5.6 Perspectivas Futuras\nÀ medida que a tecnologia avança, os chatbots estão se tornando cada vez mais sofisticados e capazes de realizar tarefas complexas que antes eram consideradas impossíveis. A seguir, destacamos algumas das principais tendências e áreas de pesquisa que provavelmente moldarão o futuro dos chatbots:\nModelos de Linguagem Multimodais: Chatbots PLN não se limitam apenas ao texto. Modelos multimodais, que combinam texto com imagens, áudio e até mesmo vídeo, já são uma nova fronteira no desenvolvimento de chatbots. Esses modelos permitem que chatbots compreendam e respondam a entradas que combinam diferentes tipos de dados, oferecendo experiências mais ricas e interativas. Hoje, chatbots multimodais já analisam imagens enviadas por um usuário e fornecem diagnósticos ou recomendações, além de responderem a perguntas textuais relacionadas.\nPersonalização Avançada: À medida que a capacidade de coleta e análise de dados se expande, a personalização de chatbots se tornará mais sofisticada. Chatbots serão capazes de adaptar suas respostas com base no histórico de interações, preferências do usuário e até mesmo no contexto emocional, criando interações verdadeiramente personalizadas. Hoje, assistentes virtuais ajustam as recomendações de compras ou serviços com base nos hábitos de consumo e humor do usuário.\nChatbots Colaborativos e de Aprendizado Contínuo: Chatbots que podem aprender de forma contínua e colaborativa com outros sistemas e com os próprios usuários serão uma área chave de desenvolvimento. Isso permitirá que chatbots se adaptem rapidamente a novos domínios e que integrem informações em tempo real, melhorando sua eficácia e utilidade. Um exemplo de aplicação seria um chatbot que aprende novos termos e conceitos diretamente das interações com usuários e os compartilha com outros chatbots, criando uma rede de conhecimento distribuída. Outro exemplo é um chatbot que discute Ecologia com estudantes do ensino médio [@Neo2023Ecowe].\nSegurança e Privacidade Melhoradas: Com a crescente sofisticação dos chatbots, as questões de segurança e privacidade se tornarão ainda mais críticas. Avanços em criptografia, anonimização de dados e conformidade com regulamentos serão essenciais para garantir que chatbots possam ser usados com confiança em setores sensíveis, como saúde e finanças. Um exemplo seriam chatbots médicos que garantem a privacidade total dos dados do paciente enquanto oferecem diagnósticos e recomendações de tratamento, por exemplo @pires2024conversational.\nExplicabilidade e Transparência dos Modelos: Com o uso crescente de Modelos de Linguagem Grande e complexos, a explicabilidade e a transparência se tornarão áreas críticas de pesquisa. Ferramentas e técnicas para explicar como um chatbot chegou a uma determinada resposta serão cada vez mais demandadas, especialmente em setores regulamentados.Uma solução pode ser fornecer uma explicação detalhada de como chegaram a uma decisão ou recomendação, aumentando a confiança dos usuários.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#oportunidades-de-inovação",
    "href": "sections/8_conclus-o-conclusao.html#oportunidades-de-inovação",
    "title": "5  Conclusão",
    "section": "5.7 Oportunidades de Inovação",
    "text": "5.7 Oportunidades de Inovação\nÀ medida que o campo dos chatbots continua a evoluir, novas oportunidades para inovação e pesquisa emergem constantemente. Nesta seção, vamos explorar as áreas emergentes. Também discutiremos os desafios abertos que podem ser abordados e como os desenvolvedores e pesquisadores podem contribuir para o avanço deste campo, explorando as tendências tecnológicas e áreas promissoras de estudo.\n\nTendências Tecnológicas Emergentes\nAs tecnologias de Processamento de Linguagem Natural (PLN) e Inteligência Artificial (IA) estão em rápida evolução. Algumas das principais tendências que moldarão o futuro dos chatbots incluem:\n\nVibe Coding\nEm sua essência, vibe coding é uma abordagem ao desenvolvimento de software que se apoia fortemente na IA para gerar código. Em vez de escrever meticulosamente cada linha de código você mesmo, você descreve o que quer alcançar em linguagem natural, e a IA faz o trabalho pesado. É como ter um par de programação de IA que compreende suas vibrações (daí o nome) e as traduz em código funcional.\nEste conceito ganhou força com os avanços recentes em Modelos de Linguagem Grande (LLMs) como o GPT-4. Essas ferramentas de IA se tornaram cada vez mais hábeis em compreender a intenção humana e gerar código correspondente, tornando o processo de desenvolvimento mais rápido e acessível.\nCaracterísticas do Vibe Coding:\n\nAbstração: Vibe coding abstrai as complexidades da escrita de código tradicional, permitindo que os programadores se concentrem na lógica de nível superior e na criatividade.\nEficiência: Ao automatizar tarefas repetitivas e gerar código boilerplate, o vibe coding pode acelerar significativamente o processo de desenvolvimento.\nAcessibilidade: O vibe coding pode potencialmente democratizar o desenvolvimento de software, tornando-o mais acessível a pessoas com menos experiência em programação.\nColaboração: O vibe coding pode facilitar novas formas de colaboração entre humanos e IA, com os programadores trabalhando em conjunto com as ferramentas de IA para construir software.\n\nEmbora o vibe coding ainda esteja em seus estágios iniciais, tem o potencial de revolucionar o desenvolvimento de software. Imagine você poder construir um aplicativo simplesmente descrevendo-o em uma conversa com um assistente de IA. Este cenário pode estar mais perto do que pensamos.\nNo entanto, o vibe coding também levanta questões importantes. Irá levar a uma perda de controle sobre o processo de desenvolvimento? Quais são as implicações para a qualidade do código, segurança e manutenção a longo prazo? À medida que o vibe coding continua a evoluir, os programadores terão de navegar por estas questões e encontrar um equilíbrio entre aproveitar o poder da IA.\n\n\nModel Contex Protocol\nO Model Context Protocol (MCP) é um padrão aberto e um framework de código aberto introduzido pela Anthropic em 2024 para padronizar a forma como sistemas de IA, especialmente grandes modelos de linguagem (LLMs), se conectam a ferramentas e fontes de dados externas [@anthropic2024mcp]. Seu objetivo principal é unificar o modo como modelos acessam arquivos, executam funções externas e utilizam prompts contextuais, substituindo integrações ad hoc por uma interface consistente. Dessa forma, o MCP resolve o problema de integração \\(N \\times M\\), em que cada agente precisava de conectores específicos para cada ferramenta, tornando o desenvolvimento mais complexo e difícil de escalar [@anthropic2024mcp].\nDo ponto de vista conceitual, o MCP utiliza uma arquitetura cliente-servidor em que o agente de IA atua como cliente MCP e se comunica com um ou mais servidores MCP que expõem ferramentas, recursos e prompts [@jsonrpcSpec]. Toda a comunicação é realizada usando o protocolo JSON-RPC 2.0, que padroniza as mensagens de requisição e resposta em formato JSON e é independente de linguagem de programação. O MCP suporta transporte por stdio para execução local com baixa latência e transporte via HTTP com suporte a Server-Sent Events para cenários distribuídos ou remotos [@jsonrpcSpec]. Essa padronização garante interoperabilidade e permite que novos servidores ou ferramentas possam ser descobertos dinamicamente pelo cliente, que pode então invocá-los com parâmetros estruturados e receber resultados também em formato JSON.\nA adoção do MCP traz vantagens significativas para o ecossistema de agentes de IA, pois elimina a dependência de soluções proprietárias e facilita a criação de ambientes híbridos, onde modelos locais ou em nuvem podem acessar os mesmos recursos de maneira uniforme. Em aplicações corporativas ou privadas, o MCP permite que servidores sejam executados localmente, garantindo que dados sensíveis não precisem sair do ambiente da organização, atendendo a requisitos de segurança e conformidade. Além disso, ao ser um protocolo aberto, incentiva a colaboração e a criação de um ecossistema de conectores reutilizáveis, reduzindo o custo de desenvolvimento e favorecendo a interoperabilidade entre diferentes plataformas de IA [@anthropic2024mcp].\n\n\nOutras tendências\nInteligência Artificial Explicável (XAI): Com o aumento da complexidade dos modelos de linguagem, a necessidade de Inteligência Artificial Explicável (XAI) se torna mais premente. XAI visa tornar os sistemas de IA mais transparentes e compreensíveis para os seres humanos, o que é importante para garantir a confiança do usuário em decisões automatizadas. Um exemplo de aplicação é desenvolver chatbots que possam explicar como chegaram a uma determinada resposta, oferecendo aos usuários uma visão dos processos de tomada de decisão do modelo.\nModelos de Linguagem Contínuos e Atualizáveis: À medida que o conhecimento e a linguagem evoluem, há uma crescente necessidade de modelos de linguagem que possam ser continuamente atualizados sem a necessidade de re-treinamento completo. A pesquisa em aprendizado contínuo e incremental está ganhando força, com o objetivo de criar chatbots que possam se adaptar a novas informações em tempo real. Um exemplo de aplicação é implementar chatbots em ambientes corporativos que possam incorporar novas políticas ou informações à medida que são introduzidas, mantendo a precisão e relevância das respostas.\nIntegração Multimodal Avançada: Os chatbots do futuro não se limitarão apenas ao texto. A integração de dados multimodais – combinando texto, áudio, vídeo e outras formas de dados sensoriais – permitirá que os chatbots ofereçam interações muito mais ricas e naturais. Um exemplo de aplicação seria desenvolver assistentes virtuais que possam interpretar e responder a comandos de voz, reconhecer expressões faciais em vídeo, e até mesmo responder a estímulos táteis em interfaces especializadas.\n\n\n\nDesafios em abertos\nApesar dos avanços significativos, vários desafios permanecem no desenvolvimento de chatbots. Esses desafios representam oportunidades para pesquisa e inovação.\nCompreensão de Contexto Profundo: Embora os modelos atuais sejam capazes de manter o contexto em conversas curtas, a compreensão de contexto em conversas longas e complexas ainda é um desafio. Isso inclui a capacidade de lembrar detalhes ao longo de várias interações e responder de maneira coerente em tópicos que evoluem com o tempo. Uma área de pesquisa seria investigar novas arquiteturas de memória e mecanismos de atenção que possam melhorar a retenção e o uso de contexto em conversas prolongadas.\nInteração Emocional e Comportamental: Os chatbots ainda carecem de habilidades avançadas para interpretar e responder a sinais emocionais e comportamentais dos usuários. A capacidade de um chatbot de ajustar seu tom, estilo de resposta e sugestões com base no estado emocional do usuário pode melhorar significativamente a qualidade da interação. Uma área de pesquisa seria desenvolver modelos de linguagem que incorporem reconhecimento e resposta emocional, utilizando técnicas de aprendizado profundo e processamento de sinais.\nRedução de Viés e Aumento da Inclusividade: Como discutido na seção sobre considerações éticas, a mitigação de viés é um desafio contínuo. Garantir que os chatbots sejam inclusivos e justos em suas interações é essencial para evitar a perpetuação de preconceitos e discriminações. Uma área de pesquisa é criar métodos para detectar e corrigir viés em modelos de linguagem e explorar novos conjuntos de dados que representem uma diversidade maior de culturas e contextos.\nAutomação da Criação e Treinamento de Chatbots: O desenvolvimento e o treinamento de chatbots ainda são processos intensivos em tempo e recursos. Automação avançada nesses processos pode acelerar o desenvolvimento e permitir a criação de chatbots mais personalizados e especializados. Uma área de pesquisa é investigar o uso de técnicas de AutoML (Machine Learning Automático) para automatizar a seleção de modelos, ajuste de hiperparâmetros e treinamento de chatbots para diferentes domínios.\n\n\nOportunidades de P&D\nPara desenvolvedores e pesquisadores interessados em contribuir para a pesquisa e desenvolvimento dos chatbots, as seguintes áreas podem representar algumas oportunidades:\nColaboração entre Humanos e Chatbots: A pesquisa em sistemas colaborativos, onde humanos e chatbots trabalham juntos para resolver problemas complexos, está crescendo. Explorar como chatbots podem complementar e aprimorar as capacidades humanas em ambientes colaborativos é uma área rica para inovação. Um exemplo de aplicação seriam chatbots assistentes que ajudam equipes a coordenar projetos, fornecendo sugestões baseadas em análises de dados em tempo real e facilitando a comunicação entre os membros da equipe.\nChatbots para a Inclusão Digital: Com bilhões de pessoas ainda desconectadas do mundo digital, chatbots podem realizar inclusão digital. Desenvolver chatbots que funcionem em ambientes de baixa conectividade e que sejam acessíveis para pessoas com baixa alfabetização digital é uma área importante de pesquisa. Um exemplo seriam chatbots que operam em redes de baixa largura de banda e que utilizam interfaces de voz para alcançar comunidades rurais ou sub-representadas.\nSegurança e Privacidade em Chatbots Autônomos: À medida que os chatbots se tornam mais autônomos, a segurança e a privacidade se tornam preocupações ainda maiores. A pesquisa em criptografia, autenticação e anonimização de dados em chatbots autônomos é essencial para proteger os usuários. Um exemplo de aplicação seria um chatbot financeiro que realiza transações automaticamente, protegendo as informações do usuário com criptografia avançada e garantindo a conformidade com regulamentos como o GDPR.\nColaboração Interdisciplinar: O futuro do desenvolvimento de chatbots será moldado pela colaboração interdisciplinar. As áreas de IA, psicologia, linguística, direito e ética precisarão trabalhar juntas para criar sistemas que sejam não apenas tecnicamente avançados, mas também socialmente responsáveis. Uma proposta pode ser desenvolver chatbots educacionais que não apenas ensinem, mas também sejam capazes de adaptar seus métodos pedagógicos com base em princípios psicológicos e educativos, colaborando com educadores e psicólogos.\n\n\nRecomendações para Futuros Desenvolvedores\nPara aqueles que desejam continuar explorando e inovando no campo dos chatbots, algumas recomendações:\n\nExplore Novas Ferramentas e Tecnologias: Mantenha-se atualizado sobre os últimos desenvolvimentos em PLN e IA, e não tenha medo de experimentar novas ferramentas e frameworks.\nConcentre-se na Experiência do Usuário: Ao desenvolver chatbots, sempre coloque a experiência do usuário em primeiro lugar. Um chatbot útil e intuitivo será muito mais bem-sucedido.\nMantenha a Ética em Mente: Com o poder dos chatbots vem a responsabilidade. Certifique-se de que seu chatbot é ético, respeita a privacidade do usuário e está em conformidade com as regulamentações.\nParticipe da Comunidade: Envolva-se com a comunidade de PLN e IA. Contribua com projetos de código aberto, participe de conferências e workshops, e colabore com outros pesquisadores e desenvolvedores.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#considerações-finais",
    "href": "sections/8_conclus-o-conclusao.html#considerações-finais",
    "title": "5  Conclusão",
    "section": "5.8 Considerações Finais",
    "text": "5.8 Considerações Finais\nAs considerações éticas discutidas nesta seção são essenciais para o desenvolvimento de chatbots que respeitem os direitos e a dignidade dos usuários. Ao abordar questões de privacidade, viés algorítmico, segurança e impacto social, os desenvolvedores podem garantir que seus chatbots não apenas funcionem de maneira eficaz, mas também contribuam positivamente para a sociedade. Com um foco em práticas éticas, os chatbots têm o potencial de transformar interações humanas de forma significativa e benéfica.\nTambém revisamos as técnicas essenciais discutidas ao longo do livro e exploramos as tendências emergentes que estão moldando o futuro dos chatbots. À medida que continuamos a avançar no campo do Processamento de Linguagem Natural e Inteligência Artificial, as possibilidades para chatbots se expandem exponencialmente. Com as ferramentas e conhecimentos adquiridos neste livro, você está preparado para enfrentar esses desafios e contribuir para a próxima geração de sistemas de diálogo inteligentes. Além disso, destacamos as oportunidades de inovação e pesquisa futura no campo dos chatbots, apontando as tendências emergentes e os desafios que ainda precisam ser abordados.\nEste livro explorou uma ampla gama de técnicas e conceitos essenciais para o desenvolvimento de chatbots modernos e eficazes. À medida que avançamos, a responsabilidade recai sobre os desenvolvedores, pesquisadores e inovadores para continuar explorando, questionando e expandindo os limites do que os chatbots podem alcançar.\nO campo está repleto de oportunidades para aqueles que estão dispostos a enfrentar os desafios técnicos e éticos na construção dos chatbots. Com a combinação de técnica e responsabilidade social, o seu uso pode beneficiar a sociedade.\nConvidamos os pesquisadores a continuar sua jornada no desenvolvimento de chatbots, contribuindo para a criação de sistemas de diálogo que não apenas resolvam problemas técnicos, mas também ajudem a construir um mundo melhor, explorando novas fronteiras, garantindo que os chatbots do futuro sejam mais capazes, inclusivos e responsáveis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/8_conclus-o-conclusao.html#exercícios",
    "href": "sections/8_conclus-o-conclusao.html#exercícios",
    "title": "5  Conclusão",
    "section": "5.9 Exercícios",
    "text": "5.9 Exercícios\n\nPor que é importante considerar a ética no desenvolvimento de chatbots?\n\nPara garantir que o chatbot opere de forma eficiente em todos os idiomas.\nPara evitar que o chatbot cause danos sociais, respeitando a privacidade e evitando vieses.\nPara aumentar a taxa de resposta do chatbot.\nPara garantir que o chatbot possa operar em ambientes sem conexão com a internet.\n\nQual é uma prática recomendada para proteger a privacidade dos usuários ao interagir com um chatbot?\n\nColetar todos os dados possíveis do usuário, independentemente de seu consentimento.\nImplementar criptografia de ponta a ponta e obter consentimento explícito antes de coletar informações pessoais.\nEvitar qualquer tipo de monitoramento de interações do usuário.\nLimitar o uso do chatbot a conversas triviais para evitar questões de privacidade.\n\nComo o viés algorítmico pode afetar a interação de um chatbot com os usuários?\n\nO viés pode levar a respostas imprecisas, discriminatórias ou injustas, prejudicando a experiência do usuário.\nO viés melhora a capacidade do chatbot de aprender rapidamente.\nO viés reduz o tempo de resposta do chatbot, tornando-o mais eficiente.\nO viés ajuda a personalizar as respostas do chatbot para cada usuário.\n\nQual das seguintes práticas pode ajudar a mitigar o viés em modelos de chatbot?\n\nTreinar o modelo exclusivamente com dados provenientes de uma única fonte.\nDiversificar os dados de treinamento e realizar auditorias regulares para identificar e corrigir vieses.\nAumentar o tamanho do modelo para incluir mais parâmetros.\nManter o modelo desconectado de qualquer base de dados externa.\n\nPor que é essencial garantir a transparência e a explicabilidade em chatbots avançados?\n\nPara permitir que os desenvolvedores controlem todas as interações do chatbot manualmente.\nPara que os usuários compreendam como o chatbot toma decisões, aumentando a confiança e a aceitação.\nPara reduzir o custo de desenvolvimento do chatbot.\nPara eliminar a necessidade de manutenção contínua do chatbot.\n\nQual é o principal objetivo de analisar casos de estudo em chatbots avançados?\n\nDemonstrar como as técnicas teóricas discutidas podem ser aplicadas na prática para resolver problemas reais.\nDescrever os processos de instalação de chatbots em dispositivos móveis.\nExplicar como criar um chatbot sem conhecimento técnico prévio.\nComparar o desempenho de diferentes assistentes virtuais.\n\nEm um caso de estudo, qual foi a principal vantagem observada ao integrar um chatbot em uma plataforma de e-learning para o aprendizado de idiomas?\n\nRedução dos custos operacionais da plataforma.\nelhoria na fluência e confiança dos alunos ao praticar conversação em tempo real com correções imediatas.\nSubstituição completa dos professores humanos por chatbots.\nAumento do tempo de carga das páginas da plataforma.\n\nQual foi um dos desafios enfrentados ao implementar um chatbot de suporte ao cliente para uma empresa de telecomunicações?\n\nManter o contexto em longas conversas para fornecer respostas coerentes.\nReduzir o tempo de resposta para menos de 1 milissegundo.\nTreinar o chatbot para realizar diagnósticos médicos.\nImplementar o chatbot em dispositivos sem conexão à internet.\n\nPor que a empatia é uma característica importante em chatbots voltados para a saúde mental, conforme discutido em um dos casos de estudo?\n\nPorque ela substitui completamente a necessidade de intervenção humana.\nPorque ela ajuda a criar uma conexão mais forte com os usuários, oferecendo suporte emocional adequado.\nPorque ela permite que o chatbot responda mais rapidamente às consultas dos usuários.\nPorque ela elimina a necessidade de coleta de dados dos usuários.\n\nQual foi uma das lições aprendidas ao implementar chatbots avançados em diferentes indústrias, conforme os casos de estudo discutidos?\n\nA importância de personalizar o chatbot para o contexto específico da aplicação, garantindo relevância e eficácia.\nA necessidade de substituir completamente os sistemas de suporte tradicionais por chatbots.\nA impossibilidade de usar chatbots em plataformas online devido à falta de tecnologia.\nA dificuldade de integrar chatbots em plataformas de redes sociais.\n\nPor que é importante realizar a manutenção contínua de um chatbot em produção?\n\nPara garantir que o chatbot permaneça atualizado, relevante e capaz de lidar com mudanças no ambiente ou nas necessidades dos usuários.\nPara reduzir o número de interações que o chatbot pode realizar por dia.\nPara garantir que o chatbot funcione apenas em horários comerciais.\nPara desativar temporariamente o chatbot e evitar o desgaste de hardware.\n\nQual das seguintes métricas é essencial para monitorar o desempenho de um chatbot em produção?\n\nTaxa de compressão de dados.\nTempo de resposta do chatbot.\nTamanho do código-fonte do chatbot.\nNúmero de desenvolvedores envolvidos no projeto.\n\nComo o feedback dos usuários pode ser utilizado para melhorar um chatbot?\n\nIgnorando o feedback dos usuários para manter a consistência das respostas.\nAjustando as respostas do chatbot e atualizando o modelo com base nas sugestões e críticas dos usuários.\nReduzindo o tempo de atividade do chatbot para evitar problemas relatados.\nMantendo as respostas do chatbot sem alterações, independentemente do feedback.\n\nQual das seguintes práticas ajuda a prevenir a deterioração do desempenho de um chatbot ao longo do tempo?\n\nRealizar avaliações regulares do modelo e implementar monitoramento de drift para detectar quando o desempenho começa a decair.\nAumentar a complexidade do modelo constantemente sem revisões periódicas.\nDesativar o chatbot durante horários de pico para evitar sobrecarga.\nEvitar atualizações para manter a estabilidade do sistema.\n\nQual das seguintes abordagens é crucial para garantir a segurança contínua dos chatbots em produção?\n\nAplicar patches de segurança regularmente e garantir a conformidade com regulamentos de proteção de dados.\nReduzir o uso de autenticação para facilitar o acesso dos usuários.\nManter o código do chatbot desatualizado para evitar incompatibilidades.\nEvitar o uso de criptografia para melhorar o desempenho.\n\nPor que é importante considerar as implicações éticas ao desenvolver chatbots?\n\nPara garantir que o chatbot funcione corretamente em diferentes dispositivos.\nPara evitar consequências negativas, como viés, discriminação e invasão de privacidade.\nPara reduzir o custo de desenvolvimento do chatbot.\nPara garantir que o chatbot responda mais rapidamente às consultas dos usuários.\n\nQual das seguintes práticas ajuda a garantir a transparência em um chatbot?\n\nManter o código do chatbot fechado e inacessível ao público.\nFornecer explicações claras sobre como o chatbot toma decisões e responde às consultas dos usuários.\nReduzir o número de interações com o usuário para evitar mal-entendidos.\nImplementar o chatbot apenas em plataformas limitadas.\n\nComo o viés algorítmico pode afetar negativamente as interações com um chatbot?\n\nO viés algorítmico pode levar o chatbot a fornecer respostas que são injustas ou discriminatórias, afetando negativamente a experiência do usuário.\nO viés algorítmico ajuda a personalizar a experiência do usuário, garantindo respostas mais precisas.\nO viés algorítmico melhora a velocidade do chatbot em responder a perguntas.\nO viés algorítmico reduz a necessidade de treinamento contínuo do chatbot.\n\nQual das seguintes abordagens pode ajudar a mitigar o viés nos modelos de chatbot?\n\nUsar apenas dados históricos, sem realizar atualizações nos modelos.\nDiversificar os dados de treinamento e implementar auditorias de viés regularmente.\nReduzir a quantidade de dados usados no treinamento para evitar sobrecarga de informações.\nConfiar em uma única fonte de dados para garantir consistência.\n\nPor que a proteção de dados é crucial ao desenvolver e implantar chatbots?\n\nPara garantir que o chatbot funcione corretamente em diferentes idiomas.\nPara proteger a privacidade dos usuários e garantir a conformidade com regulamentações como o GDPR.\nPara reduzir o custo de desenvolvimento e manutenção do chatbot.\nPara aumentar a velocidade das respostas do chatbot.\n\nQual é uma das principais áreas de pesquisa futura mencionada no Capítulo 17 para o avanço dos chatbots?\n\nA substituição completa dos assistentes virtuais por agentes humanos.\nO desenvolvimento de chatbots capazes de integrar multimodalidade, como texto, áudio e vídeo em interações.\nfoco exclusivo em interações de texto simples e baseadas em regras.\nA eliminação de machine learning em favor de processamento de linguagem baseado em regras.\n\nPor que a personalização avançada é uma tendência importante para o futuro dos chatbots?\n\nPorque elimina a necessidade de treinamento adicional dos modelos.\nPorque permite que os chatbots adaptem suas respostas com base no histórico e preferências dos usuários, melhorando a experiência de interação.\nPorque acelera o tempo de resposta do chatbot para qualquer consulta.\nPorque substitui completamente os modelos de linguagem por modelos baseados em lógica.\n\nComo os modelos de linguagem contínuos podem contribuir para a inovação no desenvolvimento de chatbots?\n\nPermitindo que os chatbots realizem tarefas sem a necessidade de dados de entrada.\nPermitindo que os chatbots se adaptem em tempo real a novas informações, sem a necessidade de re-treinamento completo.\nReduzindo a necessidade de machine learning em chatbots.\nSubstituindo a compreensão de linguagem natural por regras fixas.\n\nPor que a integração de ética na pesquisa e inovação de chatbots é essencial?\n\nPara garantir que os chatbots sejam eficientes no uso de dados de treinamento.\nPara garantir que os chatbots operem de forma justa e responsável, respeitando a privacidade e evitando discriminação.\nPara garantir que os chatbots possam ser usados em todos os idiomas.\nPara melhorar a velocidade de resposta dos chatbots.\n\nQual é o papel da colaboração interdisciplinar no futuro dos chatbots?\n\nGarantir que os chatbots possam operar sem a necessidade de manutenção contínua.\nFacilitar a integração de diferentes áreas de conhecimento, como IA, psicologia, linguística e ética, para criar chatbots mais eficazes e responsáveis.\nReduzir o custo de desenvolvimento de chatbots em larga escala.\nEliminar a necessidade de monitoramento e atualização de chatbots.\n\nQual é a principal consideração ao escolher uma plataforma de implantação para um chatbot?\n\nO tamanho do modelo de linguagem usado.\nO público-alvo e os requisitos específicos de interação do chatbot.\nA capacidade do chatbot de realizar tarefas em tempo real.\nO idioma principal em que o chatbot foi treinado.\n\nPor que é importante realizar a otimização do modelo antes da implantação de um chatbot em produção?\n\nPara garantir que o chatbot funcione sem a necessidade de atualizações futuras.\nPara reduzir o uso de recursos computacionais e melhorar o tempo de resposta em produção.\nPara eliminar a necessidade de monitoramento contínuo.\nPara permitir que o chatbot funcione apenas em um único idioma.\n\nQual das seguintes práticas pode ajudar a garantir a segurança de um chatbot implantado?\n\nDesativar todos os logs de interação do usuário.\nImplementar criptografia de dados e controles de acesso rigorosos.\nEvitar o uso de autenticação para acelerar as interações.\nManter o código-fonte do chatbot em plataformas abertas.\n\nQual é a vantagem de usar containers e orquestração na implantação de chatbots?\n\nPermitir que o chatbot seja executado exclusivamente em dispositivos móveis.\nFacilitar o gerenciamento, escalabilidade e atualização do chatbot em ambientes de produção.\nAumentar a complexidade do processo de implantação.\narantir que o chatbot opere apenas em redes locais.\n\nComo a integração com aplicativos de mensagens, como WhatsApp ou Slack, pode beneficiar um chatbot?\n\nPermite que o chatbot interaja diretamente com os usuários em plataformas onde eles já estão ativos, melhorando o alcance e a conveniência.\nGarante que o chatbot funcione apenas em horários comerciais.\nElimina a necessidade de monitoramento e manutenção do chatbot.\nLimita as funcionalidades do chatbot para apenas responder a perguntas básicas.\n\nPor que é importante realizar a manutenção contínua de um chatbot em produção?\n\nara garantir que o chatbot permaneça atualizado, relevante e capaz de lidar com mudanças no ambiente ou nas necessidades dos usuários.\nPara reduzir o número de interações que o chatbot pode realizar por dia.\nPara garantir que o chatbot funcione apenas em horários comerciais.\nPara desativar temporariamente o chatbot e evitar o desgaste de hardware.\n\nQual das seguintes métricas é essencial para monitorar o desempenho de um chatbot em produção?\n\nTaxa de compressão de dados.\nTempo de resposta do chatbot.\nTamanho do código-fonte do chatbot.\nNúmero de desenvolvedores envolvidos no projeto.\n\nComo o feedback dos usuários pode ser utilizado para melhorar um chatbot?\n\nIgnorando o feedback dos usuários para manter a consistência das respostas.\nAjustando as respostas do chatbot e atualizando o modelo com base nas sugestões e críticas dos usuários.\nReduzindo o tempo de atividade do chatbot para evitar problemas relatados.\nMantendo as respostas do chatbot sem alterações, independentemente do feedback.\n\nQual das seguintes práticas ajuda a prevenir a deterioração do desempenho de um chatbot ao longo do tempo?\n\nRealizar avaliações regulares do modelo e implementar monitoramento de drift para detectar quando o desempenho começa a decair.\nAumentar a complexidade do modelo constantemente sem revisões periódicas.\nDesativar o chatbot durante horários de pico para evitar sobrecarga.\nEvitar atualizações para manter a estabilidade do sistema.\n\nQual das seguintes abordagens é crucial para garantir a segurança contínua dos chatbots em produção?\n\nAplicar patches de segurança regularmente e garantir a conformidade com regulamentos de proteção de dados.\nReduzir o uso de autenticação para facilitar o acesso dos usuários.\nManter o código do chatbot desatualizado para evitar incompatibilidades.\nEvitar o uso de criptografia para melhorar o desempenho.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusão</span>"
    ]
  },
  {
    "objectID": "sections/9_agradecimentos-agradecimentos-unnumbered.html",
    "href": "sections/9_agradecimentos-agradecimentos-unnumbered.html",
    "title": "Agradecimentos",
    "section": "",
    "text": "Expresso minha gratidão ao Professor Dr. Olival de Gusmão Freitas Júnior (UFAL) pela cuidadosa revisão deste livro, cujo olhar crítico contribuiu para elevar a qualidade do trabalho. Agradeço especialmente ao Pesquisador Dr. Joaquim José Cintra Maia Honório pelo auxílio no capítulo dos LLMs e Fine-Tuning; essas contribuições foram importantes e melhoraram muito o texto a partir do rascunho original. Agradeço ao Professor Dr. Evandro de Barros Costa (UFAL) pela apresentação do ELIZA; ao Professor Dr. José Antão Beltrão Moura (UFCG) pela inspiração e dedicação ao ensino; e agradeço ao Professor Msc. Otávio Monteiro Pereira (IFAL) pelo apoio relacionado à ética dos chatbots. Agradeço também ao Instituto Federal de Alagoas e a seus servidores. Estendo meus agradecimentos a todos que, de forma direta ou indireta, colaboraram com ideias, discussões e sugestões que enriqueceram o conteúdo apresentado.\n\nGiseldo Neo\n\nAgradeço à minha família, que sempre esteve ao meu lado em cada etapa desta jornada. Aos meus pais, pelo amor, paciência e incentivo incondicional. Aos parentes, pelo apoio e pela compreensão das ausências nos momentos de dedicação intensa. Agradeço também aos amigos e colegas que contribuíram com ideias, sugestões e palavras de motivação nos dias desafiadores. Este livro é fruto não apenas do meu esforço, mas também da força coletiva daqueles que acreditaram e apoiaram para que esta obra se tornasse realidade.\n\nAlana Neo",
    "crumbs": [
      "Agradecimentos"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html",
    "title": "Sobre os autores",
    "section": "",
    "text": "Giseldo da Silva Neo\nGISELDO DA SILVA NEO é Professor de Informática no Instituto Federal de Alagoas (IFAL) e desenvolve pesquisas na área de IA. Doutorado em Ciência da Computação na Universidade Federal de Campina Grande (UFCG). Possui Mestrado em Modelagem Computacional do Conhecimento (UFAL) e Mestrado em Contabilidade (FUCAPE). Possui MBA em Gestão e Estratégia Empresarial , Especialização em Arquitetura e Engenharia de Software, MBA em Gestão de Projetos. Graduação em Análise e Desenvolvimento de Sistemas e Graduação em Processos Gerenciais e possui nível Técnico em Informática (ETFSE - Escola Técnica Federal de Sergipe).",
    "crumbs": [
      "Sobre os autores"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#giseldo-da-silva-neo",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#giseldo-da-silva-neo",
    "title": "Sobre os autores",
    "section": "",
    "text": "Giseldo Neo",
    "crumbs": [
      "Sobre os autores"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#alana-viana-borges-da-silva-neo",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#alana-viana-borges-da-silva-neo",
    "title": "Sobre os autores",
    "section": "Alana Viana Borges da Silva Neo",
    "text": "Alana Viana Borges da Silva Neo\n\n\n\nAlana Neo\n\n\nALANA VIANA BORGES DA SILVA NEO é Professora de Informática no Instituto Federal do Mato Grosso do Sul (IFMS) e desenvolve pesquisas na área de Informática na Educação. Doutoranda em Ciência da Computação na Universidade Federal de Campina Grande (UFCG), Mestra em Modelagem Computacional do Conhecimento na Universidade Federal de Alagoas (UFAL), Especialista em Estratégias Didáticas para a Educação Básica com Uso de TIC na Universidade Federal de Alagoas (UFAL), Especialista em Desenvolvimento de Software, Especialista em Segurança da Informação, Graduada em Análise e Desenvolvimento de Sistemas e Bacharel em Sistemas de Informação pela Universidade Estácio de Sá (ESTÁCIO) e Licenciatura em Computação pelo Claretiano Centro Universitário.",
    "crumbs": [
      "Sobre os autores"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#contato",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#contato",
    "title": "Sobre os autores",
    "section": "Contato",
    "text": "Contato\nCaso deseje entrar em contato com os autores para reportar algum erro, crítica ou sugestão, envie e-mail para giseldo@gmail.com ou acesse o site https://giseldo.github.io/",
    "crumbs": [
      "Sobre os autores"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#aviso-legal",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#aviso-legal",
    "title": "Sobre os autores",
    "section": "Aviso Legal",
    "text": "Aviso Legal\nAs informações fornecidas neste trabalho são apenas para fins educacionais e informativos. Embora todos os esforços tenham sido feitos para garantir a precisão e a integridade, o autor e o editor não fazem declarações ou garantias de qualquer tipo, expressas ou implícitas, em relação à precisão, confiabilidade ou completude do conteúdo.\nO autor e o editor não poderão ser responsabilizados por quaisquer danos ou perdas decorrentes do uso deste material. As opiniões expressas são de responsabilidade exclusiva do autor e não refletem necessariamente as opiniões de qualquer instituição ou organização com a qual o autor possa estar vinculado.",
    "crumbs": [
      "Sobre os autores"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#uso-da-ia-generativa",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#uso-da-ia-generativa",
    "title": "Sobre os autores",
    "section": "Uso da IA Generativa",
    "text": "Uso da IA Generativa\nAlgumas partes textuais e algumas imagens foram criadas ou alteradas com várias das IAs Generativas disponíveis no momento da escrita. Porém, todo o texto foi revisado pelos autores e revisores.",
    "crumbs": [
      "Sobre os autores"
    ]
  },
  {
    "objectID": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#respostas-dos-exercícios",
    "href": "sections/10_sobre-os-autores-sobre-os-autores-unnumbered.html#respostas-dos-exercícios",
    "title": "Sobre os autores",
    "section": "Respostas dos exercícios",
    "text": "Respostas dos exercícios\n1.1. B 1.2. B 1.3. B 1.4. A 1.5. C\n2.1. B 2.2. C 2.3. B 2.4. A 2.5. B\n3.1. B 3.2. B 3.3. B 3.4. A 3.5. B 3.6. A 3.7. A 3.8. A 3.9. C 3.10. B 3.11. B\n4.1. B 4.2. C 4.3. B 4.4. A 4.5. C 4.6. B 4.7. B 4.8. A 4.9. B 4.10. B 4.11. B 4.12. A 4.13. B 4.14. B 4.15. B 4.16. B 4.17. B 4.18. B 4.19. A 4.20. B 4.21. B 4.22. C 4.23. B 4.24. B 4.25. B 4.26. B 4.27. B 4.28. B 4.29. B\n5.1. B 5.2. B 5.3. A 5.4. B 5.5. B 5.6. A 5.7. B 5.8. A 5.9. B 5.10. A 5.11. A 5.12. B 5.13. B 5.14. A 5.15. A 5.16. B 5.17. B 5.18. A 5.19. B 5.20. B 5.21. B 5.22. B 5.23. B 5.24. B 5.25. B 5.26. B 5.27. B 5.28. B 5.29. B 5.30. A 5.31. A 5.32. B 5.33. B 5.34. A 5.35. A",
    "crumbs": [
      "Sobre os autores"
    ]
  }
]