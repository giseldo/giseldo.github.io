<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Modelos de Linguagem Grande (LLM) – Construindo Chatbots</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./cap5.html" rel="next">
<link href="./cap3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e165a794f04406688339ef99d26b34d0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./cap4.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modelos de Linguagem Grande (LLM)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Construindo Chatbots</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefácio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cap1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cap2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ELIZA e AIML</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cap3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Processamento de Linguagem Natural (PLN)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cap4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modelos de Linguagem Grande (LLM)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cap5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusão</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introdução" id="toc-introdução" class="nav-link active" data-scroll-target="#introdução"><span class="header-section-number">4.1</span> Introdução</a></li>
  <li><a href="#arquitetura-geral-do-transformer" id="toc-arquitetura-geral-do-transformer" class="nav-link" data-scroll-target="#arquitetura-geral-do-transformer"><span class="header-section-number">4.2</span> Arquitetura Geral do Transformer</a>
  <ul class="collapse">
  <li><a href="#bert-bidirectional-encoder-representations-from-transformers" id="toc-bert-bidirectional-encoder-representations-from-transformers" class="nav-link" data-scroll-target="#bert-bidirectional-encoder-representations-from-transformers"><span class="header-section-number">4.2.1</span> BERT (<em>Bidirectional Encoder Representations from Transformers</em>)</a></li>
  <li><a href="#gpt-generative-pre-trained-transformer" id="toc-gpt-generative-pre-trained-transformer" class="nav-link" data-scroll-target="#gpt-generative-pre-trained-transformer"><span class="header-section-number">4.2.2</span> GPT (<em>Generative Pre-trained Transformer</em>)</a></li>
  <li><a href="#distilbert-base-uncased" id="toc-distilbert-base-uncased" class="nav-link" data-scroll-target="#distilbert-base-uncased"><span class="header-section-number">4.2.3</span> distilbert-base-uncased</a></li>
  </ul></li>
  <li><a href="#fine-tuning-de-modelos-pré-treinados" id="toc-fine-tuning-de-modelos-pré-treinados" class="nav-link" data-scroll-target="#fine-tuning-de-modelos-pré-treinados"><span class="header-section-number">4.3</span> Fine-Tuning de Modelos Pré-Treinados</a></li>
  <li><a href="#few-shot-e-zero-shot-learning" id="toc-few-shot-e-zero-shot-learning" class="nav-link" data-scroll-target="#few-shot-e-zero-shot-learning"><span class="header-section-number">4.4</span> Few Shot e Zero Shot Learning</a></li>
  <li><a href="#retrieval-augmented-generation-rag" id="toc-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag"><span class="header-section-number">4.5</span> Retrieval-Augmented Generation (RAG)</a></li>
  <li><a href="#llama" id="toc-llama" class="nav-link" data-scroll-target="#llama"><span class="header-section-number">4.6</span> LLaMA</a>
  <ul class="collapse">
  <li><a href="#arquitetura-do-llama" id="toc-arquitetura-do-llama" class="nav-link" data-scroll-target="#arquitetura-do-llama"><span class="header-section-number">4.6.1</span> Arquitetura do LLaMA</a></li>
  <li><a href="#exemplo-de-uso-do-llama" id="toc-exemplo-de-uso-do-llama" class="nav-link" data-scroll-target="#exemplo-de-uso-do-llama"><span class="header-section-number">4.6.2</span> Exemplo de uso do LLaMA</a></li>
  <li><a href="#aplicações-de-llama" id="toc-aplicações-de-llama" class="nav-link" data-scroll-target="#aplicações-de-llama"><span class="header-section-number">4.6.3</span> Aplicações de LLaMA</a></li>
  <li><a href="#comparação-com-outros-modelos" id="toc-comparação-com-outros-modelos" class="nav-link" data-scroll-target="#comparação-com-outros-modelos"><span class="header-section-number">4.6.4</span> Comparação com Outros Modelos</a></li>
  </ul></li>
  <li><a href="#llm-na-prática" id="toc-llm-na-prática" class="nav-link" data-scroll-target="#llm-na-prática"><span class="header-section-number">4.7</span> LLM na prática</a>
  <ul class="collapse">
  <li><a href="#hugging-face-pipeline" id="toc-hugging-face-pipeline" class="nav-link" data-scroll-target="#hugging-face-pipeline"><span class="header-section-number">4.7.1</span> Hugging Face Pipeline</a></li>
  <li><a href="#llm-local-com-ollama" id="toc-llm-local-com-ollama" class="nav-link" data-scroll-target="#llm-local-com-ollama"><span class="header-section-number">4.7.2</span> LLM Local com Ollama</a></li>
  <li><a href="#tokenizador-no-llm" id="toc-tokenizador-no-llm" class="nav-link" data-scroll-target="#tokenizador-no-llm"><span class="header-section-number">4.7.3</span> Tokenizador no LLM</a></li>
  <li><a href="#langchain" id="toc-langchain" class="nav-link" data-scroll-target="#langchain"><span class="header-section-number">4.7.4</span> LangChain</a></li>
  <li><a href="#mangaba.ai" id="toc-mangaba.ai" class="nav-link" data-scroll-target="#mangaba.ai"><span class="header-section-number">4.7.5</span> Mangaba.AI</a></li>
  <li><a href="#fluxos-em-llm-ou-engenharia-de-prompts" id="toc-fluxos-em-llm-ou-engenharia-de-prompts" class="nav-link" data-scroll-target="#fluxos-em-llm-ou-engenharia-de-prompts"><span class="header-section-number">4.7.6</span> Fluxos em LLM (ou Engenharia de Prompts)</a></li>
  <li><a href="#como-funciona" id="toc-como-funciona" class="nav-link" data-scroll-target="#como-funciona">Como Funciona?</a></li>
  <li><a href="#exemplos-de-requisitos" id="toc-exemplos-de-requisitos" class="nav-link" data-scroll-target="#exemplos-de-requisitos">Exemplos de requisitos</a></li>
  </ul></li>
  <li><a href="#integração-de-técnicas" id="toc-integração-de-técnicas" class="nav-link" data-scroll-target="#integração-de-técnicas"><span class="header-section-number">4.8</span> Integração de Técnicas</a></li>
  <li><a href="#api-e-playground" id="toc-api-e-playground" class="nav-link" data-scroll-target="#api-e-playground"><span class="header-section-number">4.9</span> API e Playground</a></li>
  <li><a href="#resumo" id="toc-resumo" class="nav-link" data-scroll-target="#resumo"><span class="header-section-number">4.10</span> Resumo</a></li>
  <li><a href="#exercícios" id="toc-exercícios" class="nav-link" data-scroll-target="#exercícios"><span class="header-section-number">4.11</span> Exercícios</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="cap:LLM" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modelos de Linguagem Grande (LLM)</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="epigraph">
<p>“A inteligência desses sistemas é uma miragem — o que se vê não é compreensão, mas um ajuste estatístico de padrões.”</p>
<p>Emily Bender</p>
</div>
<div class="myboxobj">
<p>Objetivo Discutir a arquitetura de <em>Transformers</em>, modelos como BERT, GPT e LLaMA, <em>Fine-Tuning</em> e RAG, mostrando como essas técnicas são aplicadas na construção de chatbots modernos.</p>
</div>
<section id="introdução" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introdução"><span class="header-section-number">4.1</span> Introdução</h2>
<p>Um <em>Large Language Model</em> (LLM) - em inglês, Large Language Models - pode ser definido como um sistema computacional fundamentado em técnicas de aprendizado de máquina, cuja finalidade consiste em gerar texto a partir de uma sequência textual fornecida como entrada. Nesse contexto, o texto de entrada é denominado <em>prompt</em>, também referido em inglês como <em>input</em>, enquanto o texto produzido pelo modelo é denominado resposta, ou <em>output</em>. A figura a seguir apresenta uma representação esquemática do processo de entrada e saída de texto em um LLM.</p>
<figure id="fig:placeholder2" class="figure">
<p>
<img src="fig/llm1.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:placeholder2" data-label="fig:placeholder2"></span>
</p>
<figcaption>
Figure 18: Fluxo entrada e saída do LLM.
</figcaption>
</figure>
<p>Um LLM nada mais é do que um tipo específico de rede neural artificial (a rede neural profunda) treinada com dados textuais. Em outras palavras, uma grande rede neural treinada com muitos dados. O conceito de “grande rede neural” está relacionado à quantidade de parâmetros que compõem a rede, na casa de milhões em diante; já em relação a “muitos dados” refere-se a dezenas de gigabytes em diante, abrangendo conjuntos de dados brutos que podem incluir livros, artigos, páginas da web, documentos técnicos e outros materiais escritos.</p>
<p>O processo de treino envolve duas etapas (veja Figura 4): (i) um treinamento com dados textuais brutos e (ii) um treinamento com dados textuais anotados (ou seja, mais ajustados) às necessidades de um LLM (Raschka 2024).</p>
<figure id="fig:placeholder" class="figure">
<p>
<img src="fig/llm.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:placeholder" data-label="fig:placeholder"></span>
</p>
<figcaption>
Figure 19: Alto nível do processo de treino de um modelo de LLM genérico.
</figcaption>
</figure>
<p>Quando o LLM é treinado com dados brutos, ele é denominado modelo “base” e tem como objetivo prever a próxima sequência de palavras a partir do <em>prompt</em> de entrada. Por exemplo, para o <em>prompt</em>: O “livro está”, a resposta de um modelo <em>base</em> poderia ser: “O livro está em cima da mesa.”. A seguir, no Exemplo 1, apresenta-se um conjunto ilustrativo de dados textuais de treino.</p>
<div class="listing">
<p>Exemplo 1 - Dados textuais brutos para a primeira etapa do treino, omitindo alguns detalhes.</p>
<pre class="text"><code>"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco."
Trecho de Assis, Machado. Todos os Romances: Machado de Assis (Portuguese Edition) (p. 226). Edição do Kindle.</code></pre>
</div>
<p>Na segunda etapa, o modelo <em>base</em> é treinado novamente, mas agora com dados de texto anotados (por humanos, muitas vezes), o que permite ao modelo aprender não apenas padrões linguísticos gerais, mas também instruções explícitas de interação. Veja o exemplo de dados textuais de treino no Exemplo 2.</p>
<div class="listing">
<p>Exemplo 2 - Dados textuais anotados, omitindo alguns detalhes.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>user: <span class="st">"Quem descobriu o Brasil?"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>bot: <span class="st">"Que boa pergunta. Quem descobriu o Brasil foi Pedro Álvares Cabral. Você quer saber mais sobre o Brasil?"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>user: <span class="st">"traduza o trecho The book is on the table"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>bot: <span class="st">"A tradução é: O livro está na mesa. Gostaria de saber mais alguma coisa?"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Os Exemplos 1 e 2 foram construídos apenas para fins didáticos, com o objetivo de ilustrar, de maneira simplificada, como trechos textuais podem ser utilizados no treinamento de um modelo de linguagem. Em aplicações reais, contudo, o processo é muito mais complexo: envolve conjuntos massivos de dados textuais, frequentemente com bilhões de palavras provenientes de fontes diversas e organizados com formatações específicas. A execução desse treinamento em larga escala só se tornou possível a partir da arquitetura <em>Transformer</em>, que introduziu o mecanismo de atenção capaz de lidar, de forma eficiente e paralela, com dependências de longo alcance em sequências de texto.</p>
<p>Modelos de linguagem de larga escala são treinados justamente para prever a próxima palavra em uma sequência, habilidade que serve de base para a construção de representações linguísticas sofisticadas. Esse processo permite que os modelos compreendam contextos extensos e realizem tarefas complexas de PLN, como tradução automática, sumarização de documentos e resposta a perguntas. A capacidade de capturar relações de longo alcance nos textos é um dos diferenciais desses modelos em relação a arquiteturas anteriores.</p>
<p>A introdução dos <em>Transformers</em>, proposta por Vaswani et al.&nbsp;(2017) no artigo seminal <em>“Attention is All You Need”</em> (Vaswani et al.&nbsp;2017), representou uma mudança fundamental no campo. A principal inovação foi o mecanismo de atenção, que atribui pesos diferentes às palavras de entrada de acordo com seu contexto, permitindo ao modelo identificar quais termos são mais relevantes em cada situação. Essa característica torna possível o processamento paralelo de sequências longas, acelerando substancialmente o treinamento e aumentando a eficácia na modelagem de dependências distantes.</p>
<p>Antes dessa arquitetura, os modelos de PLN eram dominados por RNNs (<em>Recurrent Neural Networks</em>) e LSTMs (<em>Long Short-Term Memory Networks</em>). Embora funcionais, esses métodos apresentavam limitações significativas: dificuldade em capturar dependências de longo alcance devido ao problema do <em>vanishing gradient</em> e restrições de desempenho, já que processavam o texto de maneira estritamente sequencial, reduzindo o potencial de paralelização. O <em>Transformer</em> superou essas limitações ao considerar todas as palavras de entrada simultaneamente, ponderando sua importância relativa por meio da atenção.</p>
<p>Com isso, os LLMs não apenas aumentaram a precisão em tarefas tradicionais de PLN, como também ampliaram o escopo de aplicações, abrangendo geração criativa de texto, tradução contextualizada e correção gramatical automática. Por outro lado, o avanço desses modelos também trouxe questionamentos relacionados à ética, viés e uso responsável da inteligência artificial, aspectos que vêm sendo discutidos intensamente pela comunidade científica e que serão abordados no Capítulo&nbsp;<a href="cap5.html" data-reference-type="ref" data-reference="conclusao">6</a>.</p>
</section>
<section id="arquitetura-geral-do-transformer" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="arquitetura-geral-do-transformer"><span class="header-section-number">4.2</span> Arquitetura Geral do Transformer</h2>
<p>Os LLMs modernos são em grande parte fundamentados na arquitetura <em>Transformer</em>, proposta originalmente por Vaswani et al.&nbsp;(2017). Essa arquitetura se tornou a base de inúmeros avanços no processamento de linguagem natural porque oferece uma forma eficiente e escalável de lidar com sequências de texto. Diferente de abordagens anteriores, que processavam as palavras uma a uma em ordem, os <em>Transformers</em> permitem que todo o contexto seja considerado em paralelo, o que torna o treinamento e a inferência mais rápidos e flexíveis.</p>
<p>A estrutura geral de um <em>Transformer</em> é composta por blocos que se repetem, cada um deles combinando mecanismos de atenção e redes neurais totalmente conectadas (<em>feedforward</em>). Em sua formulação original, a arquitetura possui dois grandes componentes: o <strong>encoder</strong>, responsável por transformar a sequência de entrada em uma representação interna, e o <strong>decoder</strong>, que utiliza essa representação para gerar uma saída, como no caso da tradução automática. Embora alguns modelos atuais utilizem apenas a parte do encoder (como o BERT) ou apenas o decoder (como o GPT), o princípio fundamental continua o mesmo: ambos dependem intensamente do mecanismo de atenção.</p>
<p>Cada camada do encoder e do decoder contém três elementos centrais:</p>
<ul>
<li><p><strong>Mecanismo de atenção múltipla (Multi-Head Attention)</strong>: avalia, em paralelo, diferentes formas de relacionar as palavras entre si.</p></li>
<li><p><strong>Redes <em>feedforward</em></strong>: transformam as representações intermediárias, permitindo maior capacidade de modelagem.</p></li>
<li><p><strong>Normalização e conexões residuais</strong>: estabilizam o treinamento e ajudam a preservar informações ao longo das camadas.</p></li>
</ul>
<p>O conceito de <strong>self-attention</strong> é um dos pontos mais inovadores. Nesse mecanismo, cada palavra da sequência não é processada isoladamente, mas em comparação com todas as outras palavras da mesma sequência. Isso permite que o modelo entenda relações de dependência de longo alcance, como entre o sujeito no início de uma frase e o verbo que aparece muito depois. O <strong>multi-head attention</strong> amplia essa ideia ao aplicar várias atenções em paralelo, cada uma aprendendo a capturar um tipo de relação — algumas cabeças podem identificar proximidade sintática, enquanto outras captam conexões semânticas mais distantes.</p>
<p>Um aspecto central da arquitetura é que todo esse processamento ocorre de forma paralela. Diferente das redes recorrentes (<em>RNNs</em>) ou das LSTMs, que analisavam o texto palavra por palavra em sequência, o <em>Transformer</em> avalia a frase inteira de uma só vez. Essa mudança de paradigma permitiu que modelos fossem treinados em coleções massivas de texto, chegando a bilhões de parâmetros e escalando para tamanhos sem precedentes.</p>
<p>Além de sua eficiência estrutural, o <em>Transformer</em> também é flexível: pode ser adaptado a diferentes tarefas com pequenas modificações. Modelos <strong>autoregressivos</strong>, como a família GPT, são baseados apenas no decoder e são treinados para prever a próxima palavra em uma sequência de forma unidirecional. Já os modelos <strong>bidirecionais</strong>, como o BERT, exploram tanto o contexto à esquerda quanto o à direita, o que os torna particularmente eficazes em tarefas de compreensão textual.</p>
<p>O impacto da arquitetura <em>Transformer</em> é amplo e vai além do processamento de linguagem natural em sua forma mais básica. A combinação entre paralelismo, capacidade de capturar dependências de longo alcance e flexibilidade estrutural abriu caminho para a criação de famílias inteiras de modelos com diferentes finalidades. Alguns deles utilizam apenas o encoder, outros apenas o decoder, e há ainda os que combinam os dois de formas distintas. Essa diversidade deu origem a arquiteturas conhecidas, como BERT, GPT, T5 e LLaMA, que se tornaram referências na área e serão discutidas a seguir em maior detalhe.</p>
<section id="bert-bidirectional-encoder-representations-from-transformers" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="bert-bidirectional-encoder-representations-from-transformers"><span class="header-section-number">4.2.1</span> BERT (<em>Bidirectional Encoder Representations from Transformers</em>)</h3>
<p>O BERT (<em>Bidirectional Encoder Representations from Transformers</em>) introduziu uma nova forma de treinamento em larga escala, na qual o modelo considera tanto o contexto à esquerda quanto o contexto à direita de uma palavra-alvo. Essa abordagem permitiu uma compreensão mais rica da linguagem em comparação com modelos anteriores, que eram predominantemente unidirecionais.</p>
<p>O BERT é construído sobre a pilha de <em>encoders</em> do <em>Transformer</em>. Isso significa que ele não é projetado para gerar texto de forma autoregressiva, mas sim para produzir representações contextuais profundas de palavras e frases, que podem depois ser utilizadas em diversas tarefas de PLN. Essa característica torna o BERT especialmente adequado para problemas de compreensão textual, como classificação, resposta a perguntas e extração de informações.</p>
<p>O modelo é pré-treinado em uma grande quantidade de texto utilizando duas tarefas principais:</p>
<ul>
<li><p><strong>Masked Language Modeling (MLM)</strong>: Palavras aleatórias em uma sequência são substituídas por um token especial de máscara, e o modelo é treinado para prever as palavras originais com base no restante do contexto. Esse procedimento permite que o BERT aprenda relações bidirecionais, uma vez que a previsão de uma palavra depende de termos anteriores e posteriores na frase.</p></li>
<li><p><strong>Next Sentence Prediction (NSP)</strong>: O modelo recebe pares de frases e deve identificar se a segunda frase segue logicamente a primeira. Essa tarefa ajuda o BERT a capturar relações discursivas entre sentenças, algo essencial em aplicações como resposta a perguntas e inferência textual.</p></li>
</ul>
<p>Após o pré-treinamento, o BERT pode ser ajustado finamente (<em>Fine-Tuning</em>) para tarefas específicas, muitas vezes com datasets muito menores que aqueles usados no pré-treinamento. Esse processo de adaptação é o que permite que um único modelo, previamente treinado de maneira genérica, seja aplicado em uma ampla gama de problemas.</p>
<div class="listing">
<p>Uso do BERT.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> softmax</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BERT</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained( <span class="st">'bert-base-uncased'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained( <span class="st">'bert-base-uncased'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"Chatbots são muito úteis para automação."</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predição</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>input_ids)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> softmax(outputs.logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Probabilidades de classe:"</span>, probs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Probabilidades de classe: tensor([[0.7097, 0.2903]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
<p>O BERT rapidamente estabeleceu novos patamares em benchmarks de PLN, como GLUE (General Language Understanding Evaluation) e SQuAD (Stanford Question Answering Dataset). Sua eficácia impulsionou o desenvolvimento de uma série de variações e aprimoramentos, como RoBERTa, DistilBERT e ALBERT, que buscaram melhorar desempenho, eficiência ou reduzir custos de treinamento.</p>
<p>O BERT é amplamente utilizado em:</p>
<ul>
<li><p><strong>Classificação de Texto</strong>: Análise de sentimento, detecção de spam, categorização de documentos.</p></li>
<li><p><strong>Respostas a Perguntas</strong>: Modelos que identificam trechos relevantes em um texto para responder perguntas formuladas em linguagem natural.</p></li>
<li><p><strong>Extração de Informações</strong>: Identificação de entidades nomeadas (pessoas, lugares, organizações) e relações entre elas.</p></li>
</ul>
<p>Apesar de seu impacto, o BERT apresenta limitações relevantes. O modelo é pesado em termos computacionais, exigindo grande capacidade de processamento para treinamento e mesmo para inferência em aplicações práticas. Além disso, sua janela de contexto é limitada, o que dificulta o processamento de documentos muito extensos sem estratégias adicionais de segmentação.</p>
<div class="listing">
<p>Exemplo das representações do BERT.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BERT pré-treinado</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained( <span class="st">'bert-base-uncased'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained( <span class="st">'bert-base-uncased'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">"Machine learning is fascinating."</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(texto, return_tensors<span class="op">=</span><span class="st">'pt'</span>)[<span class="st">'input_ids'</span>]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenção das representações do modelo BERT</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(input_ids)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>last_hidden_states <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Representações BERT:"</span>, last_hidden_states)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Representações BERT: tensor([[[ <span class="fl">0.0836</span>,  <span class="fl">0.0931</span>, <span class="op">-</span><span class="fl">0.2452</span>,  ..., <span class="op">-</span><span class="fl">0.3743</span>,  <span class="fl">0.0502</span>,  <span class="fl">0.5606</span>],</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>[ <span class="fl">0.2351</span>,  <span class="fl">0.0642</span>, <span class="op">-</span><span class="fl">0.1859</span>,  ..., <span class="op">-</span><span class="fl">0.1147</span>,  <span class="fl">0.5381</span>,  <span class="fl">0.5704</span>],</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>...,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>[ <span class="fl">0.8129</span>,  <span class="fl">0.1013</span>, <span class="op">-</span><span class="fl">0.2131</span>,  ...,  <span class="fl">0.0133</span>, <span class="op">-</span><span class="fl">0.6965</span>, <span class="op">-</span><span class="fl">0.0085</span>]]],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>grad_fn<span class="op">=&lt;</span>NativeLayerNormBackward0<span class="op">&gt;</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="gpt-generative-pre-trained-transformer" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="gpt-generative-pre-trained-transformer"><span class="header-section-number">4.2.2</span> GPT (<em>Generative Pre-trained Transformer</em>)</h3>
<p>O <em>Generative Pre-trained Transformer</em> (GPT) é um dos LLMs mais conhecidos. Ele é treinado de forma autoregressiva, o que significa que prediz a próxima palavra em uma sequência, dada a entrada anterior. Isso o torna excelente para tarefas de geração de texto. O GPT 2 foi uma versão inicial do GPT, contendo 1,5 bilhões de parâmetros. Ele mostrou que, ao ser treinado em grandes quantidades de texto, poderia gerar conteúdo coerente e complexo. GPT é uma evolução ainda maior, com 175 bilhões de parâmetros. Esse modelo pode realizar uma ampla gama de tarefas de PLN sem a necessidade de ajustes finos específicos, simplesmente recebendo exemplos de como a tarefa deve ser executada (aprendizado por poucos exemplos, ou few-shot learning).</p>
<div class="listing">
<p>Geração de texto com GPT-2.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo GPT-2</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"Chatbots modernos podem"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Texto gerado:"</span>, generated_text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Texto gerado: Chatbots modernos podemodel.com The following is a list of all the bots that have been added to the podemodel.com community. Bot Name Description bot_bot_name_id bot_</code></pre>
</div>
<p>O GPT tem sido aplicado em diversas áreas, incluindo:</p>
<ul>
<li><p><strong>Geração de Texto</strong>: Criação de conteúdo, histórias, artigos, etc.</p></li>
<li><p><strong>Assistentes Virtuais</strong>: Implementação de sistemas de diálogo baseados em IA.</p></li>
<li><p><strong>Tradução Automática</strong>: Utilização de contexto amplo para melhorar a tradução entre idiomas.</p></li>
</ul>
<p>O GPT é um modelo autoregressivo que se concentra na geração de texto. É treinado para prever a próxima palavra em uma sequência, o que o torna excelente para tarefas de geração de texto, como chatbots.</p>
<div class="listing">
<p>Outro exemplo de geração de texto com GPT-2.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo GPT-2 pré-treinado</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">'gpt2'</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"In the future, artificial intelligence will"</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Texto gerado:"</span>, generated_text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Texto gerado: In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people. We're going to see a lot of things that are going to be very interesting in</code></pre>
</div>
<p>Além de BERT e GPT, há muitos outros modelos baseados em <em>Transformers</em> projetados para tarefas específicas. Alguns exemplos incluem o T5 (<em>Text-To-Text Transfer Transformer</em>) que converte qualquer tarefa de PLN em um problema de tradução; o XLNet que combina ideias de BERT e <em>Transformers</em> autoregressivos para melhorar a modelagem de dependências de longo alcance; o RoBERTa (A Robustly Optimized BERT Pretraining Approach) que é uma variação do BERT com treinamento aprimorado, além do DistilBERT.</p>
<p>O T5 transforma qualquer tarefa de PLN em um problema de tradução, onde a entrada e a saída são tratadas como texto. Isso simplifica o Fine-Tuning para diferentes tarefas.</p>
<div class="listing">
<p>Realiza uma tradução automática de uma frase usando o T5.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install sentencepiece</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> T5Tokenizer, T5ForConditionalGeneration</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo T5</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained( <span class="st">'t5-small'</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained( <span class="st">'t5-small'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"translate English to German: The weather is nice today."</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tradução gerada:"</span>, generated_text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Tradução gerada: Das Wetter ist heute schon.</code></pre>
</div>
<p>Cabe ressaltar ainda o XLNet e o DistilBERT. O XLNet combina vantagens dos modelos autoregressivos e bidirecionais, como o GPT e BERT, para capturar dependências de longo alcance de forma mais eficiente. Já o DistilBERT é uma versão reduzida do BERT, com menos parâmetros, mas mantendo uma alta performance, o que o torna mais eficiente para uso em produção.</p>
<div class="listing">
<p>Um exemplo de geração de texto com IA usando XLNet.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> XLNetTokenizer, XLNetLMHeadModel</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo XLNet</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> XLNetTokenizer.from_pretrained( <span class="st">'xlnet-base-cased'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> XLNetLMHeadModel.from_pretrained( <span class="st">'xlnet-base-cased'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"Natural Language Processing is"</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Texto gerado com XLNet:"</span>, generated_text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Texto gerado com XLNet: Natural Language Processing is and in and in [...]</code></pre>
</div>
</section>
<section id="distilbert-base-uncased" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="distilbert-base-uncased"><span class="header-section-number">4.2.3</span> distilbert-base-uncased</h3>
<p>O modelo distilbert-base-uncased (Akhila et al.&nbsp;2023) foi lançado em 2019, sendo menor e mais rápido que o BERT e otimizado para tarefas que processam frases ou sentenças. Sua versão <em>uncased</em> é útil para descrições de código e problemas, já que não considera diferenças de maiúsculas/minúsculas. Ele é pré-treinado em um grande corpus, o que ajuda na generalização, e tem uma boa arquitetura para esse tipo de tarefa, além de exigir hardware acessível.</p>
<p>O <em>distilbert</em> resulta de um processo de knowledge distillation que reduz em aproximadamente 40% o número de parâmetros e acelera a inferência em cerca de 60%, preservando 95–97% da acurácia do BERT-base (original) em benchmarks de compreensão de linguagem natural (Sanh et al.&nbsp;2019). Essa compacidade de fornecer representações ricas com custo computacional inferior é particularmente vantajosa em ambientes de hardware moderado. Com apenas 67M de parâmetros (Sanh et al.&nbsp;2019), a variante uncased cabe em uma GPU modesta (por exemplo, 6 GB de RAM da placa de vídeo), possibilitando fine-tuning e inferência dentro de recursos computacionais restritos. Assim, torna-se viável re-treinar o modelo à medida que novos dados de projeto se acumulam, mantendo a acurácia sem investir em infraestruturas onerosas.</p>
<p>Por fim, o amplo suporte no ecossistema <em>Hugging Face</em> para os modelos do tipo BERT e para outros modelos simplifica a reprodutibilidade e a integração em pipelines de tarefas de processamento de linguagem natural.</p>
</section>
</section>
<section id="fine-tuning-de-modelos-pré-treinados" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="fine-tuning-de-modelos-pré-treinados"><span class="header-section-number">4.3</span> Fine-Tuning de Modelos Pré-Treinados</h2>
<p>Modelos pré-treinados como BERT e GPT demonstraram capacidade de compreender nuances semânticas em texto e transferir esse conhecimento para diversas tarefas específicas através de Fine-Tuning. (Minaee et al.&nbsp;2025).</p>
<p>O <em>Fine-Tuning</em> de modelos pré-treinados é uma técnica fundamental no Processamento de Linguagem Natural (PLN) moderno, especialmente ao trabalhar com Modelos de Linguagem Grande (LLMs). <em>Fine-Tuning</em> permite adaptar um modelo geral para tarefas específicas, como classificação de texto, análise de sentimentos ou geração de linguagem, utilizando um conjunto de dados menor e específico.</p>
<p><em>Fine-Tuning</em> é o processo de tomar um modelo pré-treinado em uma grande quantidade de dados gerais e adaptá-lo para uma tarefa específica. Este processo envolve ajustar os pesos do modelo, mas com uma taxa de aprendizado menor para não “desaprender” o que foi aprendido durante o pré-treinamento. Modelos como BERT (Devlin et al.&nbsp;2019), GPT (Radford et al.&nbsp;2019), e T5 (Raffel et al.&nbsp;2020) são comumente fine-tuned para tarefas específicas.</p>
<p>Uma alternativa eficiente ao <em>Fine-Tuning</em> completo é o uso de métodos de adaptação com baixo número de parâmetros, como o LoRA (<em>Low-Rank Adaptation</em>). Em vez de atualizar todos os pesos do modelo pré-treinado, o LoRA introduz pequenas matrizes adicionais de baixa dimensão que são ajustadas durante o treinamento, enquanto os pesos originais permanecem congelados. Essa técnica reduz drasticamente o número de parâmetros que precisam ser treinados, diminuindo o custo computacional e de armazenamento. Na prática, isso torna viável aplicar <em>Fine-Tuning</em> em LLMs muito grandes, mesmo em ambientes com recursos limitados, preservando boa parte do desempenho obtido pelo ajuste completo.</p>
<p>A principal vantagem do <em>Fine-Tuning</em> é a eficiência, pois permite que os modelos aprendam rapidamente uma nova tarefa, utilizando relativamente poucos dados. Além disso, modelos pré-treinados já capturam padrões linguísticos gerais, o que torna o Fine-Tuning uma abordagem útil para resolver problemas específicos sem precisar treinar um modelo do zero. O processo de <em>Fine-Tuning</em> geralmente envolve os seguintes passos:</p>
<ul>
<li><p><strong>Escolha do Modelo</strong>: Selecionar um modelo pré-treinado adequado para a tarefa. Modelos como BERT e GPT são populares devido à sua versatilidade.</p></li>
<li><p><strong>Preparação dos Dados</strong>: Os dados precisam estar formatados de maneira que sejam compatíveis com a tarefa específica, como classificação de texto ou resposta a perguntas.</p></li>
<li><p><strong>Configuração do Treinamento</strong>: Ajuste de hiperparâmetros como a taxa de aprendizado, número de épocas e tamanho do lote.</p></li>
<li><p><strong>Treinamento</strong>: Executar o treinamento do modelo no conjunto de dados específico.</p></li>
<li><p><strong>Avaliação</strong>: Avaliar o desempenho do modelo ajustado em um conjunto de validação ou teste.</p></li>
</ul>
<p>Vamos realizar o <em>Fine-Tuning</em> de um modelo BERT para uma tarefa de classificação de sentimentos usando o conjunto de dados IMDb.</p>
<p>Cabe ressaltar que a biblioteca <em>transformers</em> da Hugging Face, utilizada neste exemplo, tornou-se a ferramenta de referência para trabalhar com modelos baseados em <em>Transformers</em>. Ela oferece uma ampla gama de modelos pré-treinados que podem ser facilmente integrados em pipelines de PLN.</p>
<p>O código a seguir pode levar de 30 minutos a 2 horas para finalizar em um computador com uma boa GPU. Em CPU, pode demorar várias horas ou até mais de um dia. Uma sugestão é utilizar o Google Colab.</p>
<div class="listing">
<p>O código treina e avalia um modelo BERT para classificar sentimentos em textos do IMDb.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install datasets</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"WANDB_DISABLED"</span>] <span class="op">=</span> <span class="st">"true"</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o dataset IMDb</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BERT</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained( <span class="st">'bert-base-uncased'</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained( <span class="st">'bert-base-uncased'</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenizar os dados</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> tokenizer(examples[<span class="st">'text'</span>], padding<span class="op">=</span><span class="st">'max_length'</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir argumentos de treinamento</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">'./results'</span>,</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Criar o Trainer</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>model<span class="op">=</span>model,</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>args<span class="op">=</span>training_args,</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>train_dataset<span class="op">=</span>tokenized_datasets[<span class="st">'train'</span>],</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>eval_dataset<span class="op">=</span>tokenized_datasets[<span class="st">'test'</span>],</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinar o modelo</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Avaliar o modelo</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>eval_result <span class="op">=</span> trainer.evaluate()</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Resultado da Avaliação: </span><span class="sc">{</span>eval_result<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Resultado da Avaliação: {'eval_loss': 0.2594565749168396, 'eval_runtime': 172.544, 'eval_samples_per_second': 144.891, 'eval_steps_per_second': 9.059, 'epoch': 3.0}</code></pre>
</div>
<p>Neste exemplo, utilizamos o modelo BERT e a biblioteca <code>datasets</code> da Hugging Face para carregar o conjunto de dados IMDb, que é usado para tarefas de classificação de sentimentos. O processo de tokenização é realizado com o <code>BertTokenizer</code>, seguido pelo treinamento do modelo usando o <code>Trainer</code>, que automatiza o processo de <em>Fine-Tuning</em>.</p>
<p>Embora o <em>Fine-Tuning</em> seja uma técnica com potencial, é importante considerar alguns desafios:</p>
<ul>
<li><p><strong>Overfitting</strong>: Ajustar demais o modelo para os dados de treinamento específicos pode reduzir a generalização para novos dados.</p></li>
<li><p><strong>Biases Inerentes</strong>: Se o modelo pré-treinado já contém vieses, o <em>Fine-Tuning</em> pode reforçá-los, especialmente se os dados de treinamento forem limitados ou enviesados.</p></li>
<li><p><strong>Requisitos Computacionais</strong>: <em>Fine-Tuning</em> de LLMs pode ser computacionalmente intensivo, especialmente para modelos maiores como GPT-3.</p></li>
</ul>
<p>Além disso, algumas abordagens avançadas para melhorar o processo de <em>Fine-Tuning</em> incluem:</p>
<ul>
<li><p><strong>Learning Rate Warmup</strong>: Aumentar gradualmente a taxa de aprendizado no início do treinamento para evitar grandes atualizações de peso que poderiam desestabilizar o modelo.</p></li>
<li><p><strong>Layer-Wise Learning Rate Decay</strong>: Aplicar diferentes taxas de aprendizado para diferentes camadas do modelo, com camadas inferiores aprendendo mais lentamente.</p></li>
<li><p><strong>Data Augmentation</strong>: Aumentar a diversidade do conjunto de dados de treinamento para melhorar a robustez do modelo.</p></li>
</ul>
<p>O <em>Fine-Tuning</em> tem uma vasta gama de aplicações em PLN, incluindo:</p>
<ul>
<li><p><strong>Classificação de Texto</strong>: Análise de sentimentos, detecção de spam, categorização de notícias.</p></li>
<li><p><strong>Respostas a Perguntas</strong>: Modelos que respondem a perguntas baseadas em um contexto textual específico.</p></li>
<li><p><strong>Geração de Texto</strong>: <em>Fine-Tuning</em> de modelos como GPT para gerar textos específicos de um domínio, como redação de artigos científicos.</p></li>
<li><p><strong>Tradução Automática</strong>: Adaptação de modelos de tradução para dialetos ou linguagens específicas.</p></li>
</ul>
</section>
<section id="few-shot-e-zero-shot-learning" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="few-shot-e-zero-shot-learning"><span class="header-section-number">4.4</span> Few Shot e Zero Shot Learning</h2>
<p><strong>Zero Shot</strong>: A abordagem LLM zero-shot learning refere-se à capacidade dos LLMs de resolver tarefas sem a necessidade de exemplos explícitos fornecidos durante a inferência. Nesse contexto, a tarefa de processamento de linguagem natural <em>text classification</em> é especificada unicamente por meio de uma instrução textual (prompt), e o modelo deve inferir a ação esperada com base em seu conhecimento prévio adquirido durante o pré-treinamento (Radford et al.&nbsp;2019).</p>
<p><strong>Few Shot</strong>: Já o few-shot learning caracteriza-se pela inclusão de um pequeno conjunto de exemplos da tarefa no próprio prompt, com o objetivo de guiar a geração do modelo durante a inferência. Essa técnica permite ao modelo identificar padrões desejados com base nos exemplos fornecidos e aplicá-los a novos casos, mesmo sem reconfiguração ou ajuste de parâmetros. Trata-se de uma abordagem intermediária entre o zero-shot e o treinamento supervisionado tradicional, sendo especialmente eficaz em tarefas de classificação com variações contextuais (Raschka 2024). Sua principal vantagem está na adaptação rápida a novas tarefas com custo computacional reduzido.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="retrieval-augmented-generation-rag"><span class="header-section-number">4.5</span> Retrieval-Augmented Generation (RAG)</h2>
<p>O <em>Retrieval-Augmented Generation</em> (RAG) é uma abordagem que combina duas técnicas na área de processamento de linguagem natural: recuperação de informações e geração de texto. A ideia central do RAG é aprimorar a capacidade de um modelo de linguagem ao integrá-lo com um sistema de recuperação que busca informações relevantes de uma base de dados ou de um conjunto de documentos.</p>
<p>Na prática, o RAG opera em duas etapas. Primeiro, quando uma consulta ou pergunta é feita, um mecanismo de recuperação é acionado para identificar e extrair informações pertinentes de um repositório de dados. Isso pode incluir documentos, artigos ou qualquer outro tipo de conteúdo textual que possa fornecer contexto e detalhes adicionais sobre o tema em questão. Essa fase garante que o modelo de linguagem tenha acesso a informações atualizadas e específicas, em vez de depender apenas do conhecimento prévio que foi incorporado durante seu treinamento.</p>
<p>Em seguida, na segunda etapa, o modelo de linguagem utiliza as informações recuperadas para gerar uma resposta mais rica e contextualizada. Essa geração não se limita a reproduzir o conteúdo recuperado, mas sim a integrar esses dados de forma coesa, criando uma resposta que não apenas responde à pergunta, mas também fornece uma narrativa mais completa e informativa. Isso resulta em respostas que são mais precisas e relevantes, pois são fundamentadas em dados concretos e atualizados.</p>
<p>A combinação dessas duas etapas permite que o RAG supere algumas limitações dos modelos de linguagem tradicionais, que podem falhar em fornecer informações precisas ou atualizadas, especialmente em domínios que evoluem rapidamente. Além disso, essa abordagem é particularmente útil em aplicações como assistentes virtuais, chatbots e sistemas de perguntas e respostas, onde a precisão e a relevância da informação são fatores chaves na experiência do usuário.</p>
<p>Portanto, ele é uma técnica que não apenas melhora a qualidade das respostas geradas por modelos de linguagem, mas também amplia o alcance e a aplicabilidade desses modelos em cenários do mundo real, onde a informação é dinâmica e em constante evolução.</p>
<p>O RAG une dois componentes principais, a Recuperação de Informação (<em>Retrieval</em>): envolve buscar documentos, parágrafos ou passagens relevantes a partir de uma grande coleção de dados; e a Geração de Texto (<em>Generation</em>): uma vez que a informação relevante é recuperada, um modelo de linguagem, como GPT ou BART, é utilizado para gerar uma resposta coerente e informativa baseada nas informações recuperadas. Dessa forma, RAG é capaz de responder a perguntas e gerar conteúdo que não apenas utiliza o contexto imediato, mas também consulta uma base de conhecimento externa, aumentando a precisão e a relevância das respostas.</p>
<p>Vamos implementar um exemplo simples de RAG usando as bibliotecas da Hugging Face, incluindo o modelo DPR para recuperação e o modelo BART para geração de texto.</p>
<p>Recuperação de Passagens com DPR: Primeiro, precisamos carregar e configurar o modelo DPR para recuperar passagens relevantes a partir de uma base de dados. Sugerimos a execução do código abaixo no Google Colab.</p>
<div class="listing">
<p>O código identifica qual passagem é mais relevante para responder à consulta fornecida, usando embeddings densos e busca vetorial.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRContextEncoder, DPRContextEncoderTokenizer</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para as consultas (questions)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>question_tokenizer <span class="op">=</span> DPRQuestionEncoderTokenizer.from_pretrained(</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="st">"facebook/dpr-question_encoder-single-nq-base"</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>question_encoder <span class="op">=</span> DPRQuestionEncoder.from_pretrained(</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="st">"facebook/dpr-question_encoder-single-nq-base"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para os contextos (passages)</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>context_tokenizer <span class="op">=</span> DPRContextEncoderTokenizer.from_pretrained( <span class="st">"facebook/dpr-ctx_encoder-single-nq-base"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>context_encoder <span class="op">=</span> DPRContextEncoder.from_pretrained( <span class="st">"facebook/dpr-ctx_encoder-single-nq-base"</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de consulta</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">"What is Retrieval-Augmented Generation?"</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar a consulta</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>query_input <span class="op">=</span> question_tokenizer(query, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> question_encoder(<span class="op">**</span>query_input).pooler_output</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de passagens</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> [</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation."</span>,</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"It allows for more accurate and contextually relevant answers by consulting external knowledge bases."</span>,</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model."</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar as passagens</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>passage_inputs <span class="op">=</span> context_tokenizer(passages, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>passage_embeddings <span class="op">=</span> context_encoder(</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>passage_inputs).pooler_output</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular similaridade e selecionar a passagem mais relevante</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> torch.matmul(query_embedding, passage_embeddings.T)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>best_passage_index <span class="op">=</span> torch.argmax(similarity_scores, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>best_passage <span class="op">=</span> passages[best_passage_index]</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Passagem mais relevante:"</span>, best_passage)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Passagem mais relevante: Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation </code></pre>
</div>
<p>Neste exemplo, utilizamos o modelo DPR para codificar uma consulta e várias passagens, e então calculamos a similaridade entre a consulta e as passagens para recuperar a mais relevante.</p>
<p>Geração de Texto com BART: Uma vez que a passagem mais relevante foi recuperada, utilizamos o modelo BART para gerar uma resposta coerente.</p>
<div class="listing">
<p>O código busca identificar, entre vários textos, qual é o mais relevante para uma pergunta, usando embeddings e similaridade vetorial. Depois, prepara um modelo de geração para responder à consulta usando o contexto selecionado.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRContextEncoder, DPRContextEncoderTokenizer</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BartTokenizer, BartForConditionalGeneration</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para as consultas (questions)</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>question_tokenizer <span class="op">=</span> DPRQuestionEncoderTokenizer.from_pretrained(</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="st">"facebook/dpr-question_encoder-single-nq-base"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>question_encoder <span class="op">=</span> DPRQuestionEncoder.from_pretrained(</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="st">"facebook/dpr-question_encoder-single-nq-base"</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para os contextos (passages)</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>context_tokenizer <span class="op">=</span> DPRContextEncoderTokenizer.from_pretrained( <span class="st">"facebook/dpr-ctx_encoder-single-nq-base"</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>context_encoder <span class="op">=</span> DPRContextEncoder.from_pretrained( <span class="st">"facebook/dpr-ctx_encoder-single-nq-base"</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de consulta</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">"What is Retrieval-Augmented Generation?"</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar a consulta</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>query_input <span class="op">=</span> question_tokenizer(query, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> question_encoder(<span class="op">**</span>query_input).pooler_output</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de passagens</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> [</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation."</span>,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"It allows for more accurate and contextually relevant answers by consulting external knowledge bases."</span>,</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model."</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar as passagens</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>passage_inputs <span class="op">=</span> context_tokenizer(passages, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>passage_embeddings <span class="op">=</span> context_encoder</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>   <span class="op">**</span>passage_inputs).pooler_output</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular similaridade e selecionar a passagem mais relevante</span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> torch.matmul(query_embedding, passage_embeddings.T)</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>best_passage_index <span class="op">=</span> torch.argmax(similarity_scores, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>best_passage <span class="op">=</span> passages[best_passage_index]</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BART</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>bart_tokenizer <span class="op">=</span> BartTokenizer.from_pretrained(</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="st">"facebook/bart-large"</span>)</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>bart_model <span class="op">=</span> BartForConditionalGeneration.from_pretrained(</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="st">"facebook/bart-large"</span>)</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenar a consulta com a passagem relevante</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> query <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> best_passage</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar e gerar resposta</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> bart_tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> bart_model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">4</span>, early_stopping<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> bart_tokenizer.decode(generated_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Resposta gerada:"</span>, generated_text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Resposta gerada: What is Retrieval-Augmented Generation?Retrieval and Augmented Generation is a technique that combines retrieval of relevant information with text generation.What is retrieval?</code></pre>
</div>
<p>Este código gera uma resposta baseada na passagem recuperada, criando uma resposta informativa que combina a informação relevante com a geração de texto fluida.</p>
<p>A técnica RAG tem várias aplicações práticas, incluindo:</p>
<ul>
<li><p><strong>Sistemas de Resposta a Perguntas</strong>: Sistemas que precisam consultar bases de conhecimento extensivas para responder a perguntas de forma precisa.</p></li>
<li><p><strong>Assistentes Virtuais</strong>: Assistentes que necessitam de acesso a informações específicas e detalhadas, além do treinamento inicial do modelo.</p></li>
<li><p><strong>Geração de Conteúdo</strong>: Criação de conteúdo especializado que requer consulta de fontes externas para garantir precisão e relevância.</p></li>
</ul>
<p>A técnica RAG também apresenta alguns desafios:</p>
<ul>
<li><p><strong>Escalabilidade</strong>: A recuperação de informações em bases de dados muito grandes pode ser computacionalmente intensiva.</p></li>
<li><p><strong>Relevância das Passagens</strong>: A qualidade das respostas geradas depende fortemente da relevância das passagens recuperadas.</p></li>
<li><p><strong>Treinamento Conjunto</strong>: Treinar os componentes de recuperação e geração de maneira conjunta pode ser complexo e requer grandes volumes de dados.</p></li>
</ul>
</section>
<section id="llama" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="llama"><span class="header-section-number">4.6</span> LLaMA</h2>
<p>LLaMA é uma família de modelos de linguagem grandes (LLMs) que são menores em tamanho, mas ainda mantêm a capacidade de realizar tarefas complexas de PLN. A abordagem de LLaMA é baseada em uma arquitetura de <em>Transformer</em>, semelhante a outros LLMs, mas otimizada para eficiência em termos de parâmetros e recursos computacionais.</p>
<section id="características-principais" class="level4" data-number="4.6.0.1">
<h4 data-number="4.6.0.1" class="anchored" data-anchor-id="características-principais"><span class="header-section-number">4.6.0.1</span> Características Principais</h4>
<ul>
<li><p><strong>Tamanho Reduzido</strong>: LLaMA é projetado para ser mais leve que os modelos gigantescos, com diferentes variantes que variam de 7B a 65B parâmetros.</p></li>
<li><p><strong>Eficiência Computacional</strong>: Devido ao seu design otimizado, o LLaMA pode ser treinado em menos tempo e com menos recursos, tornando-o acessível para organizações menores e pesquisadores.</p></li>
<li><p><strong>Versatilidade</strong>: Apesar de seu tamanho reduzido, o LLaMA é capaz de realizar uma ampla gama de tarefas de PLN, incluindo geração de texto, tradução, e compreensão de linguagem.</p></li>
</ul>
</section>
<section id="arquitetura-do-llama" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="arquitetura-do-llama"><span class="header-section-number">4.6.1</span> Arquitetura do LLaMA</h3>
<p>A arquitetura do LLaMA é baseada no <em>Transformer</em>, mas com várias otimizações que permitem que ele mantenha uma alta qualidade de predição, enquanto usa menos parâmetros e recursos computacionais.</p>
<section id="camadas-transformer-otimizadas" class="level4" data-number="4.6.1.1">
<h4 data-number="4.6.1.1" class="anchored" data-anchor-id="camadas-transformer-otimizadas"><span class="header-section-number">4.6.1.1</span> Camadas <em>Transformer</em> Otimizadas</h4>
<p>O LLaMA utiliza camadas <em>Transformer</em> com melhorias específicas para otimizar o uso de memória e tempo de processamento. As principais diferenças incluem:</p>
<ul>
<li><p><strong>Atenção Multi-Head Otimizada</strong>: Reduz a redundância ao calcular a atenção em múltiplas cabeças.</p></li>
<li><p><strong>Feedforward Otimizado</strong>: Utiliza técnicas de compressão para reduzir o número de operações necessárias.</p></li>
<li><p><strong>Parâmetros Compactos</strong>: Redução do número de parâmetros, mantendo a capacidade de capturar relações complexas na linguagem.</p></li>
</ul>
</section>
<section id="treinamento-e-escalabilidade" class="level4" data-number="4.6.1.2">
<h4 data-number="4.6.1.2" class="anchored" data-anchor-id="treinamento-e-escalabilidade"><span class="header-section-number">4.6.1.2</span> Treinamento e Escalabilidade</h4>
<p>O LLaMA foi treinado em grandes corpora de dados textuais, incluindo múltiplos idiomas e domínios. A arquitetura permite que o modelo seja escalado de maneira eficiente, com versões menores (7B parâmetros) adequadas para tarefas menos intensivas e versões maiores (65B parâmetros) competindo com modelos de ponta como GPT-3.</p>
</section>
</section>
<section id="exemplo-de-uso-do-llama" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="exemplo-de-uso-do-llama"><span class="header-section-number">4.6.2</span> Exemplo de uso do LLaMA</h3>
<p>Embora o LLaMA seja relativamente novo, é possível utilizar as ferramentas da Hugging Face para trabalhar com variantes do modelo ou implementações semelhantes.</p>
<p>Vamos utilizar uma versão de LLaMA para realizar uma tarefa simples de geração de texto. Assumiremos que a variante LLaMA foi integrada à <em>Hugging Face Transformers</em>.</p>
<p>Para executar o código abaixo, recomendamos o Google Colab. É necessário informar a sua chave do Hugging Face e solicitar acesso ao modelo LLaMa no próprio Hugging Face.</p>
<div class="listing">
<p>Cria uma frase continuando o texto inicial, simulando uma escrita criativa baseada em inteligência artificial.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> login</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> userdata</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> userdata</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>userdata.get(<span class="st">'HF_TOKEN'</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Log in to Hugging Face</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>login(token<span class="op">=</span>userdata.get(<span class="st">'HF_TOKEN'</span>))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo LLaMA</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="st">"meta-llama/Meta-Llama-3-8B"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="st">"meta-llama/Meta-Llama-3-8B"</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Texto de entrada</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"Artificial intelligence is transforming the world of"</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenizar e gerar texto</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">5</span>, early_stopping<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(generated_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Texto gerado:"</span>, generated_text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Texto gerado: Artificial intelligence is transforming the world of business. It has the potential to revolutionize the way we work, communicate, and interact with each other. AI is already being used in a variety of industries, from healthcare to finance, and it is</code></pre>
</div>
<p>Neste exemplo, carregamos o tokenizer e o modelo LLaMA e geramos um texto baseado em um prompt de entrada. O código pode ser ajustado para diferentes tamanhos de modelos e diferentes tarefas de geração de texto.</p>
</section>
<section id="aplicações-de-llama" class="level3" data-number="4.6.3">
<h3 data-number="4.6.3" class="anchored" data-anchor-id="aplicações-de-llama"><span class="header-section-number">4.6.3</span> Aplicações de LLaMA</h3>
<p>Devido a sua eficiência e versatilidade, LLaMA pode ser aplicado em uma variedade de cenários, incluindo:</p>
<ul>
<li><p><strong>Assistentes Virtuais</strong>: Implementação de assistentes que podem ser executados em dispositivos com recursos limitados.</p></li>
<li><p><strong>Geração de Conteúdo</strong>: Produção de artigos, histórias e outros conteúdos textuais de alta qualidade.</p></li>
<li><p><strong>Tradução Automática</strong>: Modelos LLaMA menores podem ser usados para traduções em tempo real em dispositivos móveis.</p></li>
<li><p><strong>Análise de Sentimentos</strong>: Aplicações que exigem processamento eficiente de grandes volumes de dados textuais.</p></li>
</ul>
</section>
<section id="comparação-com-outros-modelos" class="level3" data-number="4.6.4">
<h3 data-number="4.6.4" class="anchored" data-anchor-id="comparação-com-outros-modelos"><span class="header-section-number">4.6.4</span> Comparação com Outros Modelos</h3>
<p>Quando comparado com modelos maiores como GPT-3, o LLaMA oferece um excelente equilíbrio entre desempenho e eficiência. Ele é particularmente útil em cenários onde os recursos computacionais são limitados ou onde a implantação em escala é uma consideração chave.</p>
<p>Algumas Vantagens do LLaMA:</p>
<ul>
<li><p><strong>Redução de Custos</strong>: Menor demanda por recursos computacionais, resultando em custos reduzidos para treinamento e implantação.</p></li>
<li><p><strong>Escalabilidade</strong>: Pode ser facilmente adaptado para diferentes tarefas e ambientes.</p></li>
<li><p><strong>Rapidez</strong>: Menor latência em inferências devido ao menor tamanho do modelo.</p></li>
</ul>
<p>Algumas Limitações do LLaMA:</p>
<ul>
<li><p><strong>Capacidade Limitada</strong>: Embora eficiente, modelos menores podem não capturar todas as nuances de linguagem que modelos maiores conseguem.</p></li>
<li><p><strong>Menor Variedade de Tarefas</strong>: Pode ser menos adequado para tarefas extremamente complexas que exigem modelos com bilhões de parâmetros.</p></li>
</ul>
</section>
</section>
<section id="llm-na-prática" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="llm-na-prática"><span class="header-section-number">4.7</span> LLM na prática</h2>
<section id="hugging-face-pipeline" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="hugging-face-pipeline"><span class="header-section-number">4.7.1</span> Hugging Face Pipeline</h3>
<p>A biblioteca <em>Transformer</em> da <em>Hugging Face</em> torna muito mais fácil trabalhar com modelos pré-treinados como o GPT-2. Aqui está um exemplo de como gerar texto usando o GPT-2 pré-treinado:</p>
<div class="listing">
<p>Exemplo de uso do GPT-2 com a biblioteca Transformers.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span><span class="st">'gpt2'</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> <span class="st">'Olá, como vai você?'</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(<span class="bu">input</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>[{<span class="st">'generated_text'</span>: <span class="st">'The book is on one of the most exciting,'</span>},</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>{<span class="st">'generated_text'</span>: <span class="st">'The book is on sale via Amazon.com for'</span>}, </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>{<span class="st">'generated_text'</span>: <span class="st">'The book is on sale tomorrow for $2.'</span>}, </span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>{<span class="st">'generated_text'</span>: <span class="st">'The book is on sale now, read more at'</span>}, </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>{<span class="st">'generated_text'</span>: <span class="st">'The book is on the bookshelf in the'</span>}]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Este código é simples porque ele usa um modelo que já foi treinado em um grande dataset. Também é possível ajustar (fine-tune) um modelo pré-treinado em seus próprios dados para obter resultados melhores.</p>
</section>
<section id="llm-local-com-ollama" class="level3" data-number="4.7.2">
<h3 data-number="4.7.2" class="anchored" data-anchor-id="llm-local-com-ollama"><span class="header-section-number">4.7.2</span> LLM Local com Ollama</h3>
<p>Nesta seção, descreveremos detalhes da instalação do software Ollama, baixaremos o modelo LLM Qwen2 0.5B e exploraremos suas capacidades com algumas perguntas simples.</p>
<p>O Ollama é uma plataforma baseada em linha de comando que facilita o uso de modelos de IA localmente; existem outras, porém a ideia geral é bem semelhante. Com ele, você pode baixar modelos pré-treinados, gerar texto e fazer inferências sem precisar de uma placa de vídeo potente. Sua simplicidade e rapidez o tornam perfeito para quem quer experimentar LLMs em computadores mais modestos.</p>
<p>Baixando e Instalando o Ollama: Acesse o site oficial do Ollama e baixe o instalador para o seu sistema operacional. Os procedimentos descritos nesta seção são baseados no Windows, mas o Ollama também funciona no macOS e Linux.</p>
<p>Entre no site do Ollama e clique no botão de download para Windows. Depois, salve o instalador (um arquivo pequeno, com poucos megabytes) na sua pasta de downloads. Em seguida, execute o instalador e siga as instruções para concluir a instalação. Depois de instalado, o Ollama já está pronto para ser usado pelo terminal. Veja na Figura&nbsp;<a href="#fig:downloadollama" data-reference-type="ref" data-reference="fig:downloadollama">5.3</a> a página do site do Ollama para download.</p>
<figure id="fig:downloadollama" class="figure">
<p>
<img src="fig/ollama1.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:downloadollama" data-label="fig:downloadollama"></span>
</p>
<figcaption>
Figure 20: Site do Ollama par download.
</figcaption>
</figure>
<p>Conhecendo os Comandos do Ollama: Com o Ollama instalado, abra o terminal (Prompt de Comando no Windows) e digite “ollama” para confirmar que a instalação deu certo. Você verá uma lista de comandos disponíveis. Veja na Figura&nbsp;<a href="#fig:comandolistarollama" data-reference-type="ref" data-reference="fig:comandolistarollama">5.4</a> o resultado da saída do comando list no prompt de comando.</p>
<figure id="fig:comandolistarollama" class="figure">
<p>
<img src="fig/ollama2.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:comandolistarollama" data-label="fig:comandolistarollama"></span>
</p>
<figcaption>
Figure 21: Comando para listar os modelos LLM já instalado.
</figcaption>
</figure>
<p>Vamos detalhar os comandos run e show do Ollama. O comando “ollama run &lt;nome-do-modelo&gt;” baixa e roda um modelo específico. Já o comando “ollama show &lt;nome-do-modelo&gt;” exibe detalhes sobre um modelo, como janela de contexto e parâmetros. Como ainda não baixamos nenhum modelo, o comando “ollama list” vai mostrar uma lista vazia. Vamos resolver isso baixando um modelo!</p>
<p>Escolhendo e Baixando um Modelo: No site do Ollama, na seção “Models”, você encontra vários modelos disponíveis. Veja na Figura&nbsp;<a href="#fig:sitebuscaollamaprint" data-reference-type="ref" data-reference="fig:sitebuscaollamaprint">5.5</a> um print de tela do navegador do site do Ollama com o resultado da busca de um modelo LLM.</p>
<figure id="fig:sitebuscaollamaprint" class="figure">
<p>
<img src="fig/ollama4.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:sitebuscaollamaprint" data-label="fig:sitebuscaollamaprint"></span>
</p>
<figcaption>
Figure 22: Site do ollama, busca pelo nome do modelo.
</figcaption>
</figure>
<p>Vamos usar um modelo bem leve chamado “Qwen2 0.5B”, da Alibaba, que tem 0,5 bilhão de parâmetros e apenas 350 MB de tamanho. Ele é ideal para máquinas mais simples, como um computador com processador Pentium Gold sem GPU. Veja na Figura&nbsp;<a href="#fig:detalheswen" data-reference-type="ref" data-reference="fig:detalheswen">5.6</a> um print de tela do navegador do site do Ollama com o resultado do detalhamento do modelo Qwen2.5.</p>
<figure id="fig:detalheswen" class="figure">
<p>
<img src="fig/ollama5.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:detalheswen" data-label="fig:detalheswen"></span>
</p>
<figcaption>
Figure 23: Tela exibindo detalhes do modelo LLM Qwen2.5.
</figcaption>
</figure>
<p>Para baixar e rodar o modelo, acesse a página de modelos no site do Ollama. Localize o modelo Qwen2 e anote o comando para a versão de 0.5B: “ollama run qwen2:0.5b”. No terminal, digite:</p>
<div class="listing">
<p>Comando para executar (subir na memória) um modelo LLM no Ollama</p>
<pre class="text"><code>$ ollama run qwen2.5:0.5b</code></pre>
</div>
<p>Esse comando vai baixar o modelo e abrir uma interface de texto interativa para você começar a conversar com ele. Veja na Figura&nbsp;<a href="#fig:promptrunollama" data-reference-type="ref" data-reference="fig:promptrunollama">5.7</a> um print de tela do navegador com o resultado do prompt do comando “run”.</p>
<figure id="fig:promptrunollama" class="figure">
<p>
<img src="fig/ollama6.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:promptrunollama" data-label="fig:promptrunollama"></span>
</p>
<figcaption>
Figure 24: Prompt de comando demonstrando a execução do Ollama.
</figcaption>
</figure>
<p>Interagindo com o modelo: Com o modelo baixado, o Ollama inicia uma interface no terminal para conversas baseadas em texto. Vamos testar com algumas perguntas.</p>
<section id="exemplo-1-perguntando-sobre-aprendizado-de-máquina" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exemplo-1-perguntando-sobre-aprendizado-de-máquina">Exemplo 1 Perguntando Sobre Aprendizado de Máquina</h4>
<p>.</p>
<p>Digite a seguinte pergunta “O que é aprendizagem de máquina?” no console (ou prompt de comando) do Ollama instalado e com um modelo LLM carregado. Veja na Figura&nbsp;<a href="#fig:prompsaidaollamaaprendizagem" data-reference-type="ref" data-reference="fig:prompsaidaollamaaprendizagem">5.8</a> um print do prompt de comando com a resposta do LLM Qwen carregado em memória e respondendo ao prompt: o que é aprendizagem de máquina.</p>
<div class="listing">
<p>Prompt que será passado para o LLM.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>O que é aprendizagem de máquina?</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figure id="fig:prompsaidaollamaaprendizagem" class="figure">
<p>
<img src="fig/ollama7.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:prompsaidaollamaaprendizagem" data-label="fig:prompsaidaollamaaprendizagem"></span>
</p>
<figcaption>
Figure 25: Prompt de comando exibindo a saída do LLM em português.
</figcaption>
</figure>
<p>Por ser um modelo pequeno, o Qwen2 0.5B pode “alucinar” um pouco ou dar respostas meio vagas, especialmente em português. Suas capacidades de diálogo emergente são limitadas comparadas a modelos maiores, mas ainda assim são impressionantes para o tamanho dele.</p>
</section>
<section id="exemplo-2-perguntando-em-inglês" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exemplo-2-perguntando-em-inglês">Exemplo 2: Perguntando em Inglês</h4>
<p>.</p>
<p>Agora, vamos tentar a mesma pergunta em inglês: “What is machine learning?”. Perguntando em inglês, a resposta costuma ser mais precisa. Figura&nbsp;<a href="#fig:promptrunollamaingles" data-reference-type="ref" data-reference="fig:promptrunollamaingles">5.9</a> um resultado da resposta (output) do LLM em inglês.</p>
<div class="listing">
<p>Prompt que será enviado para o LLM em inglês</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>What <span class="kw">is</span> machine learning?</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figure id="fig:promptrunollamaingles" class="figure">
<p>
<img src="fig/ollama8.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:promptrunollamaingles" data-label="fig:promptrunollamaingles"></span>
</p>
<figcaption>
Figure 26: Prompt de comando exibindo a saída do LLM em inglês.
</figcaption>
</figure>
<p>O modelo tem uma resposta melhor em inglês, provavelmente porque foi treinado com mais dados nessa língua. Isso mostra como o desempenho de modelos de linguagem pode variar dependendo do idioma e dos dados de treinamento.</p>
</section>
<section id="exemplo-3-prompt-simples-pergunta-e-resposta" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exemplo-3-prompt-simples-pergunta-e-resposta">Exemplo 3: prompt simples pergunta e resposta</h4>
<p>Vamos facilitar, perguntando uma coisa simples, em que o modelo teria facilidade em perguntas: “O que é Ollama?”. Veja na Figura&nbsp;<a href="#fig:perguntasimples" data-reference-type="ref" data-reference="fig:perguntasimples">5.10</a> a saída desta pergunta.</p>
<div class="listing">
<p>Pergunta simples</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>O que é Ollama?</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figure id="fig:perguntasimples" class="figure">
<p>
<img src="fig/ollama9.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:perguntasimples" data-label="fig:perguntasimples"></span>
</p>
<figcaption>
Figure 27: Prompt de comando exibindo a saída do LLM em português.
</figcaption>
</figure>
<p>O modelo confundiu o “Ollama” a que eu me referia com outra coisa, mostrando as limitações de modelos menores. Perguntando em inglês (“What’s Ollama in language models?”), ele também não acerta, sugerindo que é um tipo de música latina.</p>
<p>A interface do Ollama tem alguns comandos úteis para gerenciar a sessão. Entre eles “/clear”: Limpa a sessão atual para começar do zero; “/help”: Mostra os comandos disponíveis; “/bye”: Sai do modo interativo do modelo. Por exemplo, depois de algumas perguntas, você pode digitar /clear para zerar o contexto ou /bye para encerrar o modelo.</p>
<p>Verificando detalhes do Modelo LLM: Para saber mais sobre o modelo que você está usando, execute o comando “ollama show &lt;nome modelo&gt;”.</p>
<div class="listing">
<p>Comando para exibir detalhes do modelo LLM desejado.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>$ ollama show qwen2<span class="fl">.5</span>:<span class="fl">0.5</span><span class="er">b</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Esse comando mostra informações, tais como a janela de contexto: 32k tokens (a quantidade de texto que o modelo consegue considerar de uma vez); a quantização: Q4 (um método para reduzir o tamanho do modelo e otimizar a performance). A janela de contexto é especialmente importante, pois ela acumula tanto o que você digita quanto as respostas do modelo, até o limite de 32.000 tokens. Um limite considerável para um modelo pequeno.</p>
<p>O Ollama funciona em computadores simples, tornando a IA acessível a todos; com ele é possível uma execução local pois não depende de serviços na nuvem, garantindo privacidade e controle. Sua configuração é rápida e sua interface de linha de comando é ideal para desenvolvedores que preferem um fluxo de trabalho minimalista e baseado em texto; por fim, embora existam plataformas mais visuais para rodar LLMs, a velocidade e o baixo consumo de recursos do Ollama o tornam uma ótima escolha para experimentos rápidos.</p>
</section>
</section>
<section id="tokenizador-no-llm" class="level3" data-number="4.7.3">
<h3 data-number="4.7.3" class="anchored" data-anchor-id="tokenizador-no-llm"><span class="header-section-number">4.7.3</span> Tokenizador no LLM</h3>
<p>O tokenizador é responsável por dividir o texto em partes menores (tokens) que o modelo pode entender. Depois, precisamos de um modelo. O modelo é a parte que realmente faz o trabalho de entender e gerar texto. Vamos usar um tokenizador já existente no Hugging Face.</p>
<div class="listing">
<p>Este código utiliza a biblioteca Transformers para trabalhar com o modelo de linguagem GPT-2.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">"gpt2"</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span><span class="op">=</span> <span class="st">"Olá, como vai você?"</span>    </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>token_id <span class="op">=</span> tokenizer(<span class="bu">input</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(token_id)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>{'input_ids': [30098, 6557, 11, 401, 78, 410, 1872, 12776, 25792, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</code></pre>
</div>
<p>A saída deste código será um dicionário com os ids dos tokens e a máscara de atenção. O id do token é o número que representa cada palavra ou parte da palavra no vocabulário do modelo. A máscara de atenção indica quais tokens devem ser considerados pelo modelo durante o processamento.</p>
<p>Attention mask é uma lista de 1s e 0s que indica quais tokens devem ser considerados pelo modelo durante o processamento. Um valor de 1 significa que o token correspondente deve ser considerado, enquanto um valor de 0 significa que ele deve ser ignorado.</p>
</section>
<section id="langchain" class="level3" data-number="4.7.4">
<h3 data-number="4.7.4" class="anchored" data-anchor-id="langchain"><span class="header-section-number">4.7.4</span> LangChain</h3>
<p>LangChain é uma biblioteca de software de código aberto projetada para simplificar a interação com Modelos de Linguagem Grande (LLMs) e construir aplicativos de processamento de linguagem natural robustos. Ele fornece uma camada de abstração de alto nível sobre as complexidades de trabalhar diretamente com modelos de linguagem, tornando mais acessível a criação de aplicativos de compreensão e geração de linguagem.</p>
<p>Trabalhar com LLMs pode ser complexo devido à sua natureza sofisticada e aos requisitos de recursos computacionais. LangChain lida com muitos detalhes complexos em segundo plano, permitindo que os desenvolvedores se concentrem na construção de aplicativos de linguagem eficazes. Aqui estão algumas vantagens do uso do LangChain:</p>
<ul>
<li><p>Simplicidade: LangChain oferece uma API simples e intuitiva, ocultando os detalhes complexos de interação com LLMs. Ele abstrai as nuances de carregar modelos, gerenciar recursos computacionais e executar previsões.</p></li>
<li><p>Flexibilidade: A biblioteca suporta vários frameworks de deep learning, como TensorFlow e PyTorch, e pode ser integrada a diferentes LLMs. Isso oferece aos desenvolvedores a flexibilidade de escolher as ferramentas e modelos que melhor atendem às suas necessidades.</p></li>
<li><p>Extensibilidade: LangChain é projetado para ser extensível, permitindo que os usuários criem seus próprios componentes personalizados. Você pode adicionar novos modelos, adaptar o processamento de texto ou desenvolver recursos específicos do domínio para atender aos requisitos exclusivos do seu aplicativo.</p></li>
<li><p>Comunidade e suporte: LangChain tem uma comunidade ativa de desenvolvedores e pesquisadores que contribuem para o projeto. A documentação abrangente, tutoriais e suporte da comunidade tornam mais fácil começar e navegar por quaisquer desafios que surgirem durante o desenvolvimento.</p></li>
</ul>
<p>A arquitetura do LangChain pode ser entendida em três componentes principais:</p>
<p>Camada de Abstração: Esta camada fornece uma interface simples e unificada para interagir com diferentes LLMs. Ela abstrai as complexidades de carregar, inicializar e executar previsões em modelos, oferecendo uma API consistente independentemente do modelo subjacente.</p>
<p>Camada de Processamento de Texto: O LangChain inclui ferramentas robustas para processamento de texto, incluindo tokenização, análise sintática, reconhecimento de entidades nomeadas (NER) e muito mais. Esta camada prepara os dados de entrada e saída para que possam ser processados de forma eficaz pelos modelos de linguagem.</p>
<p>Camada de Modelo: Aqui é onde os próprios LLMs residem. O LangChain suporta uma variedade de modelos de linguagem, desde modelos pré-treinados de uso geral até modelos personalizados específicos de domínio. Esta camada lida com a execução de previsões, gerenciamento de recursos computacionais e interação com as APIs dos modelos.</p>
<p>Vamos ver um exemplo simples de como usar o LangChain para consultar um LLM e obter uma resposta. Neste exemplo, usaremos o gpt-4o-mini da OpenAI para responder a uma pergunta.</p>
<p>Primeiro, importe as bibliotecas necessárias e configure o cliente LangChain. Em seguida, carregue o modelo de linguagem desejado. Agora, você pode usar o modelo para fazer uma consulta. Vamos perguntar quem é o presidente do Brasil.</p>
<div class="listing">
<p>Exemplo de uso do LangChain.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install langchain</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chat_models <span class="im">import</span> init_chat_model</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_core.messages <span class="im">import</span> HumanMessage</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>OPENAI_API_KEY <span class="op">=</span> os.environ.get(<span class="st">"OPENAI_API_KEY"</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> init_chat_model(<span class="st">"gpt-4o-mini"</span>, </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    model_provider<span class="op">=</span><span class="st">"openai"</span>,                        openai_api_key<span class="op">=</span>OPENAI_API_KEY)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>user_message <span class="op">=</span> HumanMessage(content<span class="op">=</span><span class="st">"Quem é o presidente do Brasil?"</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model.invoke([user_message])</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Até a minha última atualização em outubro de 2023, o presidente do Brasil é Luiz Inácio Lula da Silva. Ele assumiu o cargo em janeiro de 2023. Para informações mais atualizadas, recomendo verificar fontes de notícias recentes.</code></pre>
</div>
<p>Este exemplo básico demonstra a simplicidade de usar o LangChain para interagir com LLMs. No entanto, o LangChain oferece muito mais recursos e funcionalidades para construir aplicativos de chatbot mais robustos.</p>
</section>
<section id="mangaba.ai" class="level3" data-number="4.7.5">
<h3 data-number="4.7.5" class="anchored" data-anchor-id="mangaba.ai"><span class="header-section-number">4.7.5</span> Mangaba.AI</h3>
<p>O Mangaba.AI é um framework escrito em Python para a criação de agentes de IA autônomos que colaboram em equipe para resolver tarefas complexas. Ele permite montar equipes de agentes com funções especializadas — por exemplo, um agente pesquisador, outro analista, outro redator — que compartilham memória contextual (isto é, histórico de interações e resultados) para dar continuidade ao trabalho de forma inteligente. Além disso, o Mangaba integra modelos avançados (incluindo os modelos Gemini do Google), permite o uso de ferramentas externas via APIs e opera de forma assíncrona para executar múltiplas tarefas em paralelo, a que melhora a eficiência e a velocidade. Ele pode ser acessado em <a href="mangaba-ai.vercel.app" class="uri">mangaba-ai.vercel.app</a>.</p>
<p>Entre seus usos práticos, o Mangaba.AI pode ser aplicado para automação de processos repetitivos, geração de relatórios com base em dados complexos, análise e extração de informações de grandes volumes de documentos, construção de assistentes virtuais, e apoio à pesquisa e desenvolvimento. Ele procura facilitar a vida de desenvolvedores ao fornecer uma API simples e intuitiva para definir agentes, atribuir papéis, configurar equipes, e delegar tarefas inteiras para essa equipe de IA colaborativa.</p>
<div class="listing">
<p>Exemplo de uso do Mangaba.AI.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># para instalação acesse https://github.com/Mangaba-ai/mangaba_ai</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mangaba <span class="im">import</span> Team, Agent</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Criar uma equipe de agentes</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>monitor <span class="op">=</span> Team(<span class="st">"Autorregulação"</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Adicionar agentes especializados</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>planejador <span class="op">=</span> Agent(<span class="st">"Plano de Estudo"</span>, role<span class="op">=</span><span class="st">"Gera Plano de Estudo com cronograma baseado no tempo disponível do estudante"</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>apoio <span class="op">=</span> Agent(<span class="st">"Suporte Emocional"</span>, role<span class="op">=</span><span class="st">"Motiva o estudante"</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>pesquisador <span class="op">=</span> Agent(<span class="st">"Guia de Estudo"</span>, role<span class="op">=</span><span class="st">"Busca materiais didáticos para o estudante"</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Adicionar agentes à equipe</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>monitor.add_agents([pesquisador, apoio, planejador])</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir uma tarefa complexa</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> monitor.solve(</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Pesquise sobre Redes Neurais Artificiais"</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result.output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="fluxos-em-llm-ou-engenharia-de-prompts" class="level3" data-number="4.7.6">
<h3 data-number="4.7.6" class="anchored" data-anchor-id="fluxos-em-llm-ou-engenharia-de-prompts"><span class="header-section-number">4.7.6</span> Fluxos em LLM (ou Engenharia de Prompts)</h3>
<p>Modelos de Linguagem Grandes (LLMs), como a família GPT, são utilizados na compreensão e geração de texto. Uma maneira eficaz e relativamente rápida de criar um chatbot funcional é através da <strong>engenharia de prompts</strong>. Em vez de codificar regras complexas e árvores de decisão manualmente, você "programa" o LLM fornecendo-lhe um conjunto detalhado de instruções iniciais (o prompt).</p>
<p>O prompt é o texto inicial que você fornece ao LLM. Ele define:</p>
<ol type="1">
<li><p><strong>O Papel do Chatbot:</strong> Quem ele é (um atendente de pizzaria, um consultor de moda, etc.).</p></li>
<li><p><strong>O Objetivo da Conversa:</strong> O que ele precisa alcançar (vender uma pizza, ajudar a escolher uma roupa, abrir uma conta, etc.).</p></li>
<li><p><strong>As Regras da Conversa:</strong> A sequência exata de perguntas a fazer, as opções válidas para cada pergunta, e como lidar com diferentes respostas do usuário (lógica condicional).</p></li>
<li><p><strong>O Tom e Estilo:</strong> Se o chatbot deve ser formal, informal, amigável, etc. (embora não especificado nos exemplos, pode ser adicionado).</p></li>
<li><p><strong>O Formato da Saída Final:</strong> Como as informações coletadas devem ser apresentadas no final.</p></li>
</ol>
</section>
<section id="como-funciona" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="como-funciona">Como Funciona?</h3>
<ol type="1">
<li><p><strong>Definição:</strong> Você escreve um prompt detalhado que descreve o fluxo da conversa passo a passo.</p></li>
<li><p><strong>Instrução:</strong> Você alimenta este prompt no LLM.</p></li>
<li><p><strong>Execução:</strong> O LLM usa o prompt como seu guia mestre. Ele inicia a conversa com o usuário seguindo o primeiro passo definido no prompt, faz as perguntas na ordem especificada, valida as respostas (se instruído), segue os caminhos condicionais e, finalmente, gera a saída desejada.</p></li>
<li><p><strong>Iteração:</strong> Se o chatbot não se comportar exatamente como esperado, você ajusta e refina o prompt até que ele siga as regras perfeitamente.</p></li>
</ol>
<p>Algumas Vantagens do uso de Fluxos em Chatbots:</p>
<ul>
<li><p><strong>Rapidez:</strong> Muito mais rápido do que desenvolver um chatbot tradicional do zero.</p></li>
<li><p><strong>Flexibilidade:</strong> Fácil de modificar o comportamento alterando o prompt.</p></li>
<li><p><strong>Capacidade Conversacional:</strong> Aproveita a habilidade natural do LLM para conversas fluidas.</p></li>
</ul>
<p>Algumas Limitações do uso de Fluxos em Chatbots:</p>
<ul>
<li><p><strong>Controle Fino:</strong> Pode ser mais difícil garantir que sempre siga exatamente um caminho lógico muito complexo, embora prompts detalhados minimizem isso.</p></li>
<li><p><strong>Estado:</strong> Gerenciar estados complexos ao longo de conversas muito longas pode exigir técnicas de prompt mais avançadas.</p></li>
</ul>
</section>
<section id="exemplos-de-requisitos" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="exemplos-de-requisitos">Exemplos de requisitos</h3>
<p>Vamos simular uma necessidade real de alguns clientes em 5 exercícios. Dados os requisitos de negócio a seguir, iremos implementar os chatbots utilizando LLM. Portanto, primeiro vem o requisito de negócio e, depois de apresentados todos os requisitos, a solução de sua implantação somente utilizando prompts.<br>
</p>
<section id="requisitos" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="requisitos">Requisitos</h4>
<div class="listing">
<p>Requisito de negócio 1 Pizzaria.Requisito de negócio 1 Pizzaria.</p>
<pre class="text"><code>Construa um chatbot para uma pizzaria. O chatbot será responsável por vender uma pizza.
Verifique com o usuário qual o o tipo de massa desejado da pizza (pan ou fina).
Verifique o recheio (queijo, calabresa ou bacon)
Se o usuário escolheu massa pan verifique qual o recheio da borda (gorgonzola ou cheddar)
Ao final deve ser exibido as opções escolhidas.</code></pre>
</div>
<div class="listing">
<p>Requisito de negócio 2 Loja de RoupasRequisito de negócio 2 Loja de Roupas</p>
<pre class="text"><code>Construa um chatbot para uma loja de roupas, o chatbot será responsável por vender uma calça ou camisa.
Verifique se o usuário quer uma calça ou uma camisa.
Se o usuário quiser uma calça:
pergunte o tamanho da calça (34, 35 ou 36)
pergunte o tipo de fit da calça pode ser slim fit, regular fit, skinny fit.
Se ele quiser uma camisa:
verifique se a camisa é (P, M ou G)
verifique se ele deseja gola (v, redonda ou polo).
Ao final informe as opções escolhidas com uma mensagem informando que o pedido está sendo processado.</code></pre>
</div>
<div class="listing">
<p>Requisito de negócio 3 Empresa de Turismo.</p>
<pre class="text"><code>Este chatbot deve ser utilizado por uma empresa de turismo para vender um pacote turístico
Verifique com o usuário quais das cidades disponíveis ele quer viajar (maceio, aracaju ou fortaleza)
Se ele for para maceio:
verifique se ele já conhece as belezas naturais da cidade.
sugira os dois pacotes (nove ilhas e orla de alagoas)
Se ele for a aracaju:
verifique com o usuário quais dos dois passeios disponíveis serão escolhidos. existem duisponíveis um na passarela do carangueijo e outro na orla de aracaju.
informe que somente existe passagem de ônibus e verifique se mesmo assim ele quer continuar
Caso ele deseje ir a fortaleza:
informe que o único pacote são as falasias cearenses.
verifique se ele irá de ônibus ou de avião para o ceará
Verifique a forma de pagamento cartão ou débito em todas as opções.
Ao final informe as opções escolhidas com uma mensagem informando que o pedido está sendo processado.</code></pre>
</div>
<div class="listing">
<p>Requisito de negócio 4 Banco Financeiro.</p>
<pre class="text"><code>Crie uma aplicação para um banco que será responsável por abrir uma conta corrente para um usuário.
Verifique se o usuário já tem conta em outros bancos.
Caso o usuário tenha conta em outros bancos verifique se ele quer fazer portabilidade
Verifique o nome do correntista.
Verifique qual o saldo que será depositado, zero ou um outro valor inicial.
Verifique se o usuário quer um empréstimo.
Ao final informe o nome do correntista, se ele quis um empréstimo e se ele fez portabilidade e o valor inicial da conta.</code></pre>
</div>
<div class="listing">
<p>Requisito de negócio 5 Universidade.</p>
<pre class="text"><code>Desenvolver um chatbot para realização de matricula em duas disciplinas eletivas.
O chatbot apresenta as duas disciplinas eletivas (Inteligência artificial Avançado, Aprendizagem de Máquina)
Verificar se ele tem o pré-requisito introdução a programação para ambas as disciplinas.
Se ele escolher Inteligência artificial avançada necessário confirmar se ele cursou inteligência artificial.
Ao final informe qual o nome das disciplina em que ele se matriculou.</code></pre>
</div>
</section>
<section id="solução" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solução">Solução</h4>
<p>A seguir, mostramos como os fluxos de conversa do exercício anterior podem ser traduzidos em prompts para um LLM. Cada prompt instrui o modelo a agir como o chatbot específico e seguir as regras definidas.</p>
<div class="listing">
<p>Prompt para o LLM do Exemplo 1 Pizzaria.</p>
<pre class="text"><code>Você é um chatbot de atendimento de uma pizzaria. Sua tarefa é anotar o pedido de pizza de um cliente. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao cliente qual o tipo de massa desejado. As únicas opções válidas são "pan" ou "fina".
    * Exemplo de pergunta: "Olá! Qual tipo de massa você prefere para sua pizza: pan ou fina?"
2.  Depois que o cliente escolher a massa, pergunte qual o recheio desejado. As únicas opções válidas são "queijo", "calabresa" ou "bacon".
    * Exemplo de pergunta: "Ótima escolha! E qual recheio você gostaria: queijo, calabresa ou bacon?"
3.  APENAS SE o cliente escolheu a massa "pan" no passo 1, pergunte qual o recheio da borda. As únicas opções válidas são "gorgonzola" ou "cheddar".
    * Exemplo de pergunta (apenas para massa pan): "Para a massa pan, temos borda recheada! Você prefere com gorgonzola ou cheddar?"
4.  Após coletar todas as informações necessárias (massa, recheio e recheio da borda, se aplicável), exiba um resumo claro do pedido com todas as opções escolhidas pelo cliente.
    * Exemplo de resumo: "Perfeito! Seu pedido ficou assim: Pizza com massa [massa escolhida], recheio de [recheio escolhido] [se aplicável: e borda recheada com [recheio da borda escolhido]]."
Inicie a conversa agora seguindo o passo 1.</code></pre>
</div>
<div class="listing">
<p>Prompt para o LLM do Exemplo 2 Loja de roupas.</p>
<pre class="text"><code>Você é um chatbot de vendas de uma loja de roupas. Seu objetivo é ajudar o cliente a escolher uma calça ou uma camisa. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao cliente se ele está procurando por uma "calça" ou uma "camisa".
    * Exemplo de pergunta: "Bem-vindo(a) à nossa loja! Você está procurando por uma calça ou uma camisa hoje?"
2.  SE o cliente responder "calça":
    a.  Pergunte o tamanho da calça. As únicas opções válidas são "34", "35" ou "36".
        * Exemplo de pergunta: "Para calças, qual tamanho você usa: 34, 35 ou 36?"
    b.  Depois do tamanho, pergunte o tipo de fit da calça. As únicas opções válidas são "slim fit", "regular fit" ou "skinny fit".
        * Exemplo de pergunta: "E qual tipo de fit você prefere: slim fit, regular fit ou skinny fit?"
3.  SE o cliente responder "camisa":
    a.  Pergunte o tamanho da camisa. As únicas opções válidas são "P", "M" ou "G".
        * Exemplo de pergunta: "Para camisas, qual tamanho você prefere: P, M ou G?"
    b.  Depois do tamanho, pergunte o tipo de gola. As únicas opções válidas são "V", "redonda" ou "polo".
        * Exemplo de pergunta: "E qual tipo de gola você gostaria: V, redonda ou polo?"
4.  Após coletar todas as informações (tipo de peça e suas especificações), apresente um resumo das opções escolhidas e informe que o pedido está sendo processado.
    * Exemplo de resumo (Cal\c{c}a): "Entendido! Voc\^e escolheu uma cal\c{c}a tamanho [tamanho] com fit [fit]. Seu pedido est\'a sendo processado."
    * Exemplo de resumo (Camisa): "Entendido! Você escolheu uma camisa tamanho [tamanho] com gola [gola]. Seu pedido está sendo processado."
Inicie a conversa agora seguindo o passo 1.</code></pre>
</div>
<div class="listing">
<p>Prompt para o LLM do Exemplo 3: Empresa de Turismo.</p>
<pre class="text"><code>Você é um agente de viagens virtual de uma empresa de turismo. Sua tarefa é ajudar um cliente a escolher e configurar um pacote turístico. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao cliente para qual das cidades disponíveis ele gostaria de viajar. As únicas opções são "Maceió", "Aracaju" ou "Fortaleza".
    * Exemplo de pergunta: "Olá! Temos ótimos pacotes para Maceió, Aracaju e Fortaleza. Qual desses destinos te interessa mais?"
2.  SE o cliente escolher "Maceió":
    a.  Pergunte se ele já conhece as belezas naturais da cidade. (A resposta não altera o fluxo, é apenas conversacional).
        * Exemplo de pergunta: "Maceió é linda! Você já conhece as belezas naturais de lá?"
    b.  Sugira os dois pacotes disponíveis: "Nove Ilhas" e "Orla de Alagoas". Pergunte qual ele prefere.
        * Exemplo de pergunta: "Temos dois pacotes incríveis em Maceió: 'Nove Ilhas' e 'Orla de Alagoas'. Qual deles você prefere?"
    c.  Vá para o passo 5.
3.  SE o cliente escolher "Aracaju":
    a.  Pergunte qual dos dois passeios disponíveis ele prefere: "Passarela do Caranguejo" ou "Orla de Aracaju".
        * Exemplo de pergunta: "Em Aracaju, temos passeios pela 'Passarela do Caranguejo' e pela 'Orla de Aracaju'. Qual te atrai mais?"
    b.  Informe ao cliente que para Aracaju, no momento, só temos transporte via ônibus. Pergunte se ele deseja continuar mesmo assim.
        * Exemplo de pergunta: "Importante: para Aracaju, nosso transporte é apenas de ônibus. Podemos continuar com a reserva?"
    c.  Se ele confirmar, vá para o passo 5. Se não, agradeça e encerre.
4.  SE o cliente escolher "Fortaleza":
    a.  Informe que o pacote disponível é o "Falésias Cearenses".
        * Exemplo de informação: "Para Fortaleza, temos o pacote especial 'Falésias Cearenses'."
    b.  Pergunte se ele prefere ir de "ônibus" ou "avião" para o Ceará.
        * Exemplo de pergunta: "Como você prefere viajar para o Ceará: de ônibus ou avião?"
    c.  Vá para o passo 5.
5.  Depois de definir o destino, pacote/passeio e transporte (se aplicável), pergunte qual a forma de pagamento preferida. As únicas opções são "cartão" ou "débito".
    * Exemplo de pergunta: "Para finalizar, como você prefere pagar: cartão ou débito?"
6.  Ao final, apresente um resumo completo das opções escolhidas (destino, pacote/passeio, transporte se aplicável, forma de pagamento) e informe que o pedido está sendo processado.
    * Exemplo de resumo: "Confirmado! Seu pacote para [Destino] inclui [Pacote/Passeio], transporte por [Ônibus/Avião, se aplicável], com pagamento via [Forma de Pagamento]. Seu pedido está sendo processado!"

Inicie a conversa agora seguindo o passo 1.</code></pre>
</div>
<div class="listing">
<p>Prompt para o LLM do Exemplo 4: Banco Financeiro.</p>
<pre class="text"><code>Você é um assistente virtual de um banco e sua função é auxiliar usuários na abertura de uma conta corrente. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao usuário se ele já possui conta em outros bancos. Respostas esperadas: "sim" ou "não".
    * Exemplo de pergunta: "Bem-vindo(a) ao nosso banco! Para começar, você já possui conta corrente em alguma outra instituição bancária?"
2.  APENAS SE a resposta for "sim", pergunte se ele gostaria de fazer a portabilidade da conta para o nosso banco. Respostas esperadas: "sim" ou "não".
    * Exemplo de pergunta: "Entendido. Você gostaria de solicitar a portabilidade da sua conta existente para o nosso banco?"
3.  Pergunte o nome completo do futuro correntista.
    * Exemplo de pergunta: "Por favor, informe o seu nome completo para o cadastro."
4.  Pergunte qual será o valor do depósito inicial na conta. Informe que pode ser "zero" ou qualquer outro valor.
    * Exemplo de pergunta: "Qual valor você gostaria de depositar inicialmente? Pode ser R$ 0,00 ou outro valor à sua escolha."
5.  Pergunte se o usuário tem interesse em solicitar um empréstimo pré-aprovado junto com a abertura da conta. Respostas esperadas: "sim" ou "não".
    * Exemplo de pergunta: "Você teria interesse em verificar uma oferta de empréstimo pré-aprovado neste momento?"
6.  Ao final, apresente um resumo com as informações coletadas: nome do correntista, se solicitou portabilidade (sim/não), se solicitou empréstimo (sim/não) e o valor do depósito inicial.
    * Exemplo de resumo: "Perfeito! Finalizamos a solicitação. Resumo da abertura: Correntista: [Nome Completo], Portabilidade Solicitada: [Sim/Não], Empréstimo Solicitado: [Sim/Não], Depósito Inicial: R$ [Valor]."

Inicie a conversa agora seguindo o passo 1.</code></pre>
</div>
<div class="listing">
<p>Prompt para o LLM do Exemplo 5: Universidade.</p>
<pre class="text"><code>Você é um assistente de matrícula de uma universidade. Sua tarefa é ajudar um aluno a se matricular em até duas disciplinas eletivas. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Apresente as duas disciplinas eletivas disponíveis: "Inteligência Artificial Avançado" e "Aprendizagem de Máquina".
    * Exemplo de apresentação: "Olá! Temos duas disciplinas eletivas disponíveis para matrícula: 'Inteligência Artificial Avançado' e 'Aprendizagem de Máquina'."
2.  Verifique se o aluno possui o pré-requisito obrigatório "Introdução à Programação", que é necessário para AMBAS as disciplinas. Pergunte se ele já cursou e foi aprovado nesta disciplina. Respostas esperadas: "sim" ou "não".
    * Exemplo de pergunta: "Para cursar qualquer uma delas, é necessário ter sido aprovado em 'Introdução à Programação'. Você já cumpriu esse pré-requisito?"
3.  SE a resposta for "não", informe que ele não pode se matricular nas eletivas no momento e encerre a conversa.
    * Exemplo de mensagem: "Entendo. Infelizmente, sem o pré-requisito 'Introdução à Programação', não é possível se matricular nestas eletivas agora. Procure a coordenação para mais informações."
4.  SE a resposta for "sim" (possui o pré-requisito):
    a.  Pergunte em qual(is) das duas disciplinas ele deseja se matricular. Ele pode escolher uma ou ambas.
        * Exemplo de pergunta: "Ótimo! Em qual(is) disciplina(s) você gostaria de se matricular: 'Inteligência Artificial Avançado', 'Aprendizagem de Máquina' ou ambas?"
    b.  APENAS SE o aluno escolher "Inteligência Artificial Avançado" (seja sozinha ou junto com a outra), pergunte se ele já cursou a disciplina "Inteligência Artificial". Respostas esperadas: "sim" ou "não".
        * Exemplo de pergunta (se escolheu IA Avançado): "Para cursar 'Inteligência Artificial Avançado', é recomendado ter cursado 'Inteligência Artificial' anteriormente. Você já cursou essa disciplina?"
        * (Nota: O prompt original não especifica o que fazer se ele NÃO cursou IA. Vamos assumir que ele ainda pode se matricular, mas a pergunta serve como um aviso ou coleta de dados).
    c.  Após coletar as escolhas e a informação sobre IA (se aplicável), informe as disciplinas em que o aluno foi efetivamente matriculado. Liste apenas as disciplinas que ele escolheu E para as quais ele confirmou ter os pré-requisitos verificados neste fluxo (no caso, 'Introdução à Programação').
        * Exemplo de finalização (matriculado em ambas, confirmou IA): "Matrícula realizada com sucesso! Você está matriculado em: Inteligência Artificial Avançado e Aprendizagem de Máquina."
        * Exemplo de finalização (matriculado apenas em Aprendizagem de Máquina): "Matrícula realizada com sucesso! Você está matriculado em: Aprendizagem de Máquina."
        * Exemplo de finalização (matriculado em IA Avançado, mesmo sem ter cursado IA antes): "Matrícula realizada com sucesso! Você está matriculado em: Inteligência Artificial Avançado."
Inicie a conversa agora seguindo o passo 1.</code></pre>
</div>
<p>Veja na Figura&nbsp;<a href="#fig:chat_chatgpt_pizza" data-reference-type="ref" data-reference="fig:chat_chatgpt_pizza">5.11</a> um exemplo de implementação e diálogo quando utilizado o ChatGPT.</p>
<figure id="fig:chat_chatgpt_pizza" class="figure">
<p>
<img src="./fig/chat_chatgpt_pizza.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat_chatgpt_pizza" data-label="fig:chat_chatgpt_pizza"></span>
</p>
<figcaption>
Figure 28: Chatbot criado com LLM (via ChatGPT).
</figcaption>
</figure>
<p>A qualidade da resposta de um LLM depende muito da clareza e do detalhamento do prompt. Quanto mais específicas forem as instruções, maior a probabilidade de o chatbot se comportar exatamente como desejado. Entretanto, no período em que este livro foi escrito, os LLMs eram bons em detectar intenções, mas ainda não tão eficientes em seguir instruções complexas. Por isso, frameworks de orquestração de agentes, como o mangaba.ia ou o crewAI, mostram-se úteis, pois agentes mais atômicos tendem a performar melhor. A utilização da orquestração — encadeando pequenos agentes com intenções bem definidas e transferindo parte das instruções para a comunicação entre eles, com o apoio desses frameworks — tem recebido boa aceitação pela comunidade.</p>
</section>
</section>
</section>
<section id="integração-de-técnicas" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="integração-de-técnicas"><span class="header-section-number">4.8</span> Integração de Técnicas</h2>
<p>Nesta seção, vamos explorar como integrar as técnicas discutidas nas seções anteriores para construir chatbots eficientes. Abordaremos a utilização de Modelos de Linguagem Grande (LLMs), <em>Fine-Tuning</em>, Retrieval-Augmented Generation (RAG) e LLaMA em um único sistema, visando criar experiências de diálogo sofisticadas e personalizadas.</p>
<p>Desenhando um Chatbot: A construção de um chatbot avançado requer a combinação de várias técnicas para garantir que ele seja capaz de entender, processar e responder a uma ampla gama de consultas de usuários. Vamos revisar os principais componentes:</p>
<ul>
<li><p>Modelo de Linguagem Grande (LLM): A base para a compreensão e geração de linguagem natural.</p></li>
<li><p><em>Fine-Tuning</em>: Adaptar o LLM a domínios ou tarefas específicas.</p></li>
<li><p>Retrieval-Augmented Generation (RAG): Melhorar a relevância e precisão das respostas, combinando recuperação de informações com geração de texto.</p></li>
<li><p>LLaMA: Utilizar um modelo mais eficiente para sistemas de produção que precisam balancear desempenho e custo.</p></li>
</ul>
<p>Arquitetura de um Chatbot: A arquitetura de um chatbot pode ser desenhada de forma modular, combinando diferentes técnicas de acordo com a necessidade da aplicação. Primeiro, a entrada do usuário é capturada e tokenizada; em seguida, a entrada é analisada para determinar a intenção e extrair entidades importantes. Se necessário, o chatbot recupera informações relevantes de uma base de dados externa. Depois, utiliza-se o LLM, potencialmente ajustado com <em>Fine-Tuning</em>, para gerar uma resposta baseada na entrada e nas informações recuperadas. Por fim, a resposta gerada é enviada de volta ao usuário. Veja na Figura&nbsp;<a href="#fig:fluxodadoschatbot" data-reference-type="ref" data-reference="fig:fluxodadoschatbot">5.12</a> um diagrama visual deste procedimento.</p>
<figure id="fig:fluxodadoschatbot" class="figure">
<p>
<img src="fig/fluxo_llm_rag2.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:fluxodadoschatbot" data-label="fig:fluxodadoschatbot"></span>
</p>
<figcaption>
Figure 29: Fluxo de dados em um chatbot.
</figcaption>
</figure>
<p>Implementação de um Chatbot com LLaMA e RAG: Vamos agora implementar um chatbot que utiliza LLaMA como o modelo principal para geração de respostas e RAG para recuperar informações adicionais, se necessário.</p>
<p>Configuração Inicial: Primeiro, configuramos os componentes principais, como o modelo LLaMA para geração de respostas e DPR (Dense Passage Retrieval) para recuperação de informações. Recomendamos utilizar o Google Colab com a configuração de Processador A100. Os 3 blocos de código a seguir devem ser executados sequencialmente, um após o outro.</p>
<div class="listing">
<p>Este código prepara dois tipos de modelos: um para geração de texto (LLaMA) e outro para busca de passagens relevantes em textos (DPR), ambos usando modelos pré-treinados da biblioteca Hugging Face Transformers.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM, DPRQuestionEncoder, DPRContextEncoder</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo LLaMA</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>llama_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="st">"meta-llama/LLaMA-7B"</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>llama_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="st">"meta-llama/LLaMA-7B"</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Configurar DPR para recuperação de passagens</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>question_encoder <span class="op">=</span> DPRQuestionEncoder.from_pretrained( <span class="st">"facebook/dpr-question_encoder-single-nq-base"</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>context_encoder <span class="op">=</span> DPRContextEncoder.from_pretrained( <span class="st">"facebook/dpr-ctx_encoder-single-nq-base"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']
- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre>
</div>
<p>Processamento da Entrada e Recuperação de Informações: Em seguida, implementamos a lógica para processar a entrada do usuário e, se necessário, recuperar informações relevantes de uma base de dados. O bloco de código abaixo deve ser executado no Google Colab, em outra célula, depois do código anterior.</p>
<div class="listing">
<p>Implementa uma função de recuperação de passagens relevantes usando embeddings de linguagem.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Este código deve ser executado na sequencia, depois do código anterior.</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> retrieve_relevant_passage(query, passages):</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    query_input <span class="op">=</span> question_tokenizer(query, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    query_embedding <span class="op">=</span> question_encoder(</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>       <span class="op">**</span>query_input).pooler_output</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    passage_inputs <span class="op">=</span> context_tokenizer(passages, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    passage_embeddings <span class="op">=</span> context_encoder(</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>      <span class="op">**</span>passage_inputs).pooler_output</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    similarity_scores <span class="op">=</span> torch.matmul(query_embedding, passage_embeddings.T)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>    best_passage_index <span class="op">=</span> torch.argmax(similarity_scores, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> passages[best_passage_index]</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de passagens</span></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> [</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LLaMA é um modelo de linguagem desenvolvido pela Meta AI."</span>,</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RAG combina recuperação de informações com geração de texto."</span>,</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"GPT-3 é um dos maiores modelos de linguagem disponíveis."</span></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada do usuário</span></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>user_input <span class="op">=</span> <span class="st">"O que é LLaMA?"</span></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Recuperar a passagem mais relevante</span></span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>relevant_passage <span class="op">=</span> retrieve_relevant_passage(user_input, passages)</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Passagem mais relevante: </span><span class="sc">{</span>relevant_passage<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Passagem mais relevante: LLaMA é um modelo de linguagem desenvolvido pela Meta AI.</code></pre>
</div>
<p>Geração de Resposta com LLaMA: Finalmente, usamos o modelo LLaMA para gerar uma resposta, utilizando tanto a entrada original do usuário quanto a passagem recuperada. O bloco de código abaixo deve ser executado no Google Colab, em outra célula, depois do código anterior.</p>
<div class="listing">
<p>Realiza a geração de uma resposta de chatbot usando um modelo LLM carregado.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenar a consulta com a passagem relevante</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> user_input <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> relevant_passage</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração da resposta</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> llama_tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> llama_model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">5</span>, early_stopping<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> llama_tokenizer.decode(generated_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Resposta do chatbot:"</span>, response)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre class="text"><code>Resposta do chatbot: O que é LLaMA? LLaMA é um modelo de linguagem desenvolvido pela Meta AI. LLaMA é um acrônimo para Large Language Model, que significa modelo de linguagem grande. LLaMA é</code></pre>
</div>
<p>Neste exemplo, o chatbot gera uma resposta baseada na combinação da entrada do usuário e na informação relevante recuperada, criando uma resposta informativa e contextualizada.</p>
<p>Desafios na Implementação de Chatbots: Implementar chatbots com múltiplas técnicas apresenta vários desafios, tais como: combinar modelos como LLaMA e DPR pode ser computacionalmente intensivo, especialmente em aplicações em tempo real; o <em>Fine-Tuning</em> e a configuração de modelos precisam ser bem ajustados para garantir que o chatbot seja eficaz e relevante em suas respostas; além disso, garantir que os diferentes componentes (recuperação, geração, etc.) funcionem de maneira coesa pode ser desafiador. Mesmo assim, os chatbots podem ser aplicados em uma variedade de cenários, como: fornecer suporte automatizado, personalizado, em tempo real; ajudar na triagem de sintomas ou fornecer informações médicas básicas; ou mesmo criar tutores virtuais que podem responder a perguntas de estudantes de maneira precisa e contextualizada.</p>
</section>
<section id="api-e-playground" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="api-e-playground"><span class="header-section-number">4.9</span> API e Playground</h2>
<p>Geralmente, a desenvolvedora de um modelo LLM fornece uma interface web para o usuário conversar com o seu modelo em forma de chatbot e fornece uma API (com um playground) para quem deseja integrar a solução em suas aplicações. Assim é o ChatGPT, da empresa OpenAI. Ela cobra um valor fixo para disponibilizar aos assinantes acesso completo aos recursos do seu chatbot, inclusive com o recurso de SearchGPT (uma busca na web), lousa, geração de vídeos, imagens etc. Já para acesso à API, a modalidade de cobrança é paga-pelo-uso. Veja na Figura&nbsp;<a href="#fig:chat1" data-reference-type="ref" data-reference="fig:chat1">5.13</a> uma imagem do chatbot da OpenAI e na Figura&nbsp;<a href="#fig:chat2" data-reference-type="ref" data-reference="fig:chat2">5.14</a> o playground da OpenAI.</p>
<figure id="fig:chat1" class="figure">
<p>
<img src="fig/chatgpt1.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat1" data-label="fig:chat1"></span>
</p>
<figcaption>
Figure 30: Chatbot da OpenAI.
</figcaption>
</figure>
<figure id="fig:chat2" class="figure">
<p>
<img src="fig/chat2.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat2" data-label="fig:chat2"></span>
</p>
<figcaption>
Figure 31: Playground da openai.
</figcaption>
</figure>
<p>Existe uma provedora de inferência chamada GROQ <a href="https://groq.com/" class="uri">https://groq.com/</a>. O GROQ (com Q) é uma fornecedora de infraestrutura para inferência em modelos de linguagem de grande porte (LLM). Esta empresa vende um serviço de inferência mais rápido do que seus concorrentes. No GROQ, podemos utilizar vários modelos LLMs disponíveis abertamente, tais como o Llama da Meta, o Gemma do Google ou o Mixtral da Mixtral AI. O ChatGPT não está disponível no GROQ pois ele não é open source.</p>
<p>Entre outros modelos, utilizam uma versão do LLaMA em seu Chatbot GROQ, porém seu foco é a API, que pode ser testada no playground GROQ. O Playground funciona no formato pague-pelo-uso. O Chatbot GROQ também consome os mesmos créditos pague-pelo-uso, não tendo uma assinatura à parte, como no caso do chatbot do ChatGPT. Veja na Figura&nbsp;<a href="#fig:chat3" data-reference-type="ref" data-reference="fig:chat3">5.15</a> o Chatbot do GROQ e na Figura&nbsp;<a href="#fig:chat4" data-reference-type="ref" data-reference="fig:chat4">5.16</a> o Playground do GROQ. Note que no Chatbot do GROQ não existe o recurso de histórico como no ChatGPT.</p>
<figure id="fig:chat3" class="figure">
<p>
<img src="fig/groq3.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat3" data-label="fig:chat3"></span>
</p>
<figcaption>
Figure 32: Chatbot do GROQ.
</figcaption>
</figure>
<figure id="fig:chat4" class="figure">
<p>
<img src="fig/gro4.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat4" data-label="fig:chat4"></span>
</p>
<figcaption>
Figure 33: Playground do GROQ.
</figcaption>
</figure>
<p>Já o Grok (com K) é utilizado em referência ao chatbot da empresa xAI. Ele é um chatbot e está disponível para quem tem o acesso premium ao X (ex-twitter), enquanto a API Grok pode ser acessada na forma pague-pelo-uso. Inclusive, o Grok está oferecendo este ano 25 R$ para quem deseja testar o serviço, ou seja, é possível utilizar a API do Grok sem desembolsar nada neste momento. Porém, o Grok não tem um playground avançado, logo, para usar a API do Grok é necessário utilizar uma ferramenta gráfica ou código externo, por exemplo: LLM Studio, JAN AI, GPT4ALL ou o excelente Msty app. Veja na Figura&nbsp;<a href="#fig:chat5" data-reference-type="ref" data-reference="fig:chat5">5.17</a> o chatbot do Grok e na Figura&nbsp;<a href="#fig:chat6" data-reference-type="ref" data-reference="fig:chat6">5.18</a> o playground do Grok.</p>
<figure id="fig:chat5" class="figure">
<p>
<img src="fig/chatgpt5.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat5" data-label="fig:chat5"></span>
</p>
<figcaption>
Figure 34: Interface do Grok.
</figcaption>
</figure>
<figure id="fig:chat6" class="figure">
<p>
<img src="fig/chat7.png" style="width:90.0%" alt="image" class="figure-img"> <span id="fig:chat6" data-label="fig:chat6"></span>
</p>
<figcaption>
Figure 35: Playground do Grok.
</figcaption>
</figure>
</section>
<section id="resumo" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="resumo"><span class="header-section-number">4.10</span> Resumo</h2>
<p>Os Modelos de Linguagem Grande (LLMs) têm desempenhado um papel central na revolução do Processamento de Linguagem Natural (PLN). Esses modelos, que incluem variantes como GPT, BERT, e seus sucessores, são capazes de realizar uma ampla gama de tarefas, desde geração de texto até compreensão profunda de linguagem, graças ao seu treinamento em grandes volumes de dados textuais.</p>
<p>Esses modelos são alimentados por vastos conjuntos de dados textuais e utilizam técnicas de aprendizado profundo, particularmente redes neurais, para aprender padrões, contextos e nuances da linguagem. Eles são capazes de realizar uma variedade de tarefas linguísticas, incluindo tradução automática, geração de texto, resumo de informações, resposta a perguntas e até mesmo a criação de diálogos interativos. Funcionam com base em arquiteturas complexas, como a <em>Transformer</em>, que permite que o modelo preste atenção a diferentes partes de um texto simultaneamente, facilitando a compreensão do contexto e das relações semânticas entre palavras e frases.</p>
<p>O treinamento desses modelos envolve a exposição a enormes quantidades de texto, o que lhes permite desenvolver uma compreensão profunda da gramática, do vocabulário e dos estilos de comunicação. No entanto, essa capacidade de gerar texto coerente e relevante também levanta questões éticas e de responsabilidade, especialmente em relação à desinformação, viés algorítmico e privacidade.</p>
<p>Eles hoje representam um marco significativo na evolução da inteligência artificial, assim como foi o Eliza em sua época, porém com um impacto maior, oferecendo ferramentas para a interação humano-computador e abrindo novas possibilidades para aplicações em diversas áreas, como educação (N. G. da Silva, Pereira, and Silva Neo 2020), atendimento ao cliente ou criação de conteúdo.</p>
<p>Neste capítulo, exploramos a arquitetura <em>Transformer</em>, que revolucionou o Processamento de Linguagem Natural ao introduzir o mecanismo de atenção. Discutimos como os <em>Transformers</em> funcionam e como eles são aplicados em modelos populares como BERT e GPT. Com exemplos em Python, vimos como utilizar esses modelos para tarefas de PLN.</p>
<p>Também, exploramos a técnica de Retrieval-Augmented Generation (RAG), uma abordagem que combina a recuperação de informações com a geração de texto. Vimos como implementar RAG em Python usando modelos como DPR e BART, e discutimos as aplicações e desafios dessa técnica.</p>
<p>Adicionalmente, exploramos os LLMs incluindo suas arquiteturas, técnicas de treinamento e principais aplicações. Modelos como GPT, BERT, T5 e XLNet exemplificam como os LLMs estão redefinindo o campo do PLN. Com exemplos práticos, demonstramos como esses modelos podem ser aplicados em uma variedade de tarefas.</p>
<p>Outro tópico explorado neste capítulo foi o processo de <em>Fine-Tuning</em> de modelos pré-treinados, discutindo sua importância, desafios e aplicações práticas. O <em>Fine-Tuning</em> permite que Modelos de Linguagem Grande sejam adaptados para tarefas específicas com eficiência, tornando-os extremamente versáteis em diversas aplicações de PLN.</p>
<p>Também, exploramos o LLaMA, um modelo de linguagem projetado para ser eficiente e acessível sem comprometer a capacidade de realizar tarefas complexas de PLN. Com uma arquitetura otimizada e foco em eficiência computacional, LLaMA oferece uma alternativa prática para modelos gigantescos como o GPT-3. Vimos também como implementar LLaMA em Python para tarefas de geração de texto e discutimos suas aplicações e limitações.</p>
<p>Além disso, integramos várias técnicas discutidas em capítulos anteriores para criar um chatbot capaz de fornecer respostas precisas e contextualizadas utilizando LLaMA e RAG. A implementação dessas técnicas oferece uma pequena base para o desenvolvimento de sistemas de diálogo eficientes.</p>
</section>
<section id="exercícios" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="exercícios"><span class="header-section-number">4.11</span> Exercícios</h2>
<ol type="1">
<li><p><strong>Qual é a principal inovação introduzida pelos Transformers em relação a modelos anteriores?</strong></p>
<ol type="1">
<li><p>A) O uso de redes neurais convolucionais para processamento de texto.</p></li>
<li><p>B) A capacidade de processar sequências em paralelo, utilizando mecanismos de atenção.</p></li>
<li><p>C) A utilização de redes neurais recorrentes para manter o contexto ao longo das sequências.</p></li>
<li><p>D) A introdução de embeddings de palavras em redes neurais.</p></li>
</ol></li>
<li><p><strong>O que é o "mecanismo de atenção" em Transformers?</strong></p>
<ol type="1">
<li><p>A) Um mecanismo que aumenta a frequência das palavras mais comuns.</p></li>
<li><p>B) Um algoritmo que distribui o foco igualmente entre todas as palavras em uma sequência.</p></li>
<li><p>C) Um método que permite ao modelo focar em partes específicas da entrada ao gerar uma saída, ponderando a importância das diferentes partes da sequência.</p></li>
<li><p>D) Uma técnica para ignorar palavras irrelevantes em um texto.</p></li>
</ol></li>
<li><p><strong>O que significa o termo "self-attention" no contexto dos Transformers?</strong></p>
<ol type="1">
<li><p>A) O modelo ajusta automaticamente seu aprendizado com base no erro de previsão.</p></li>
<li><p>B) O modelo atribui pesos a diferentes partes da sequência de entrada para determinar quais partes são mais relevantes ao processar cada palavra.</p></li>
<li><p>C) O modelo decide qual sequência de palavras é mais provável com base em exemplos anteriores.</p></li>
<li><p>D) O modelo ignora todas as palavras, exceto a palavra alvo.</p></li>
</ol></li>
<li><p><strong>Qual é o papel do "positional encoding" nos Transformers?</strong></p>
<ol type="1">
<li><p>A) Ajudar o modelo a entender a ordem das palavras em uma sequência, já que o Transformer processa todas as palavras em paralelo.</p></li>
<li><p>B) Substituir palavras desconhecidas por sinônimos.</p></li>
<li><p>C) Compactar a representação de texto para economizar espaço de armazenamento.</p></li>
<li><p>D) Permitir ao modelo focar em palavras específicas dentro de uma frase.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes afirmações é verdadeira sobre a arquitetura Transformer?</strong></p>
<ol type="1">
<li><p>A) Os Transformers utilizam camadas convolucionais para processar texto.</p></li>
<li><p>B) Os Transformers dependem exclusivamente de redes neurais recorrentes para manter o contexto.</p></li>
<li><p>C) Os Transformers eliminam a necessidade de processar sequências em ordem, graças ao mecanismo de atenção e à paralelização.</p></li>
<li><p>D) Os Transformers não são capazes de lidar com longas sequências de texto devido a limitações de memória.</p></li>
</ol></li>
<li><p><strong>Qual é a principal característica dos Modelos de Linguagem Grande (LLMs) como GPT e BERT?</strong></p>
<ol type="1">
<li><p>A) Eles são baseados exclusivamente em redes neurais convolucionais.</p></li>
<li><p>B) Eles são treinados em grandes volumes de dados textuais e são capazes de realizar tarefas de PLN sem a necessidade de re-treinamento específico para cada tarefa.</p></li>
<li><p>C) Eles dependem exclusivamente de dicionários pré-definidos para gerar respostas.</p></li>
<li><p>D) Eles utilizam redes neurais recorrentes para prever a próxima palavra em uma sequência.</p></li>
</ol></li>
<li><p><strong>Qual é a diferença principal entre os modelos GPT e BERT?</strong></p>
<ol type="1">
<li><p>A) O GPT utiliza um mecanismo de atenção bidirecional, enquanto o BERT utiliza atenção unidirecional.</p></li>
<li><p>B) O GPT é um modelo autoregressivo que gera texto palavra por palavra, enquanto o BERT é um modelo pré-treinado bidirecional para tarefas de preenchimento de máscara.</p></li>
<li><p>C) O BERT é projetado apenas para geração de texto, enquanto o GPT é projetado apenas para compreensão de texto.</p></li>
<li><p>D) O BERT utiliza aprendizado supervisionado, enquanto o GPT utiliza aprendizado não supervisionado.</p></li>
</ol></li>
<li><p><strong>Qual é uma das principais vantagens do ajuste fino de modelos pré-treinados?</strong></p>
<ol type="1">
<li><p>A) Reduz a necessidade de dados de treinamento específicos para uma nova tarefa.</p></li>
<li><p>B) Garante que o modelo não precisará ser treinado novamente.</p></li>
<li><p>C) Remove completamente a necessidade de validação cruzada.</p></li>
<li><p>D) Evita qualquer risco de overfitting.</p></li>
</ol></li>
<li><p><strong>Em que tipo de tarefa o ajuste fino é particularmente útil?</strong></p>
<ol type="1">
<li><p>Tarefas que exigem uma grande quantidade de dados não rotulados.</p></li>
<li><p>Tarefas específicas que requerem adaptação de um modelo geral para um domínio particular.</p></li>
<li><p>Tarefas que não envolvem aprendizado de máquina.</p></li>
<li><p>Tarefas de compressão de dados para armazenamento eficiente.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes abordagens é recomendada ao realizar ajuste fino para evitar overfitting?</strong></p>
<ol type="1">
<li><p>Aumentar a taxa de aprendizado para forçar o modelo a aprender rapidamente.</p></li>
<li><p>Congelar algumas camadas do modelo pré-treinado e treinar apenas as camadas superiores.</p></li>
<li><p>Reduzir drasticamente o tamanho do conjunto de dados de treinamento.</p></li>
<li><p>Utilizar apenas um pequeno subconjunto do modelo pré-treinado.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes técnicas pode ser usada para melhorar o ajuste fino em um modelo pré-treinado?</strong></p>
<ol type="1">
<li><p>Aumentar o tamanho do lote para melhorar a estabilidade do treinamento.</p></li>
<li><p>Implementar o decaimento da taxa de aprendizado ao longo do treinamento.</p></li>
<li><p>Treinar o modelo apenas por uma época para evitar overfitting.</p></li>
<li><p>Usar técnicas de normalização de batch para manter a média e variância constantes.</p></li>
</ol></li>
<li><p><strong>O que é Retrieval-Augmented Generation (RAG)?</strong></p>
<ol type="1">
<li><p>Uma técnica que combina a recuperação de informações com a geração de texto para criar respostas mais informadas e contextuais.</p></li>
<li><p>Um método de compressão de texto que reduz o tamanho dos dados sem perda de informação.</p></li>
<li><p>Uma abordagem para treinar modelos de linguagem exclusivamente em dados não rotulados.</p></li>
<li><p>Uma técnica para traduzir textos entre diferentes idiomas.</p></li>
</ol></li>
<li><p><strong>Qual é a principal vantagem de usar RAG em chatbots?</strong></p>
<ol type="1">
<li><p>A capacidade de gerar respostas baseadas em dados estáticos sem necessidade de atualização.</p></li>
<li><p>A habilidade de integrar informações externas e atualizadas, permitindo respostas mais precisas e relevantes.</p></li>
<li><p>A eliminação da necessidade de modelos de linguagem grande.</p></li>
<li><p>A redução dos custos de treinamento de modelos.</p></li>
</ol></li>
<li><p><strong>No processo de RAG, qual é o papel do componente de "recuperação"?</strong></p>
<ol type="1">
<li><p>Gerar novas informações com base em uma sequência de texto fornecida.</p></li>
<li><p>Recuperar documentos, passagens ou dados relevantes de uma base de conhecimento para serem usados na geração de uma resposta.</p></li>
<li><p>Executar a tradução de texto de um idioma para outro.</p></li>
<li><p>Classificar textos em diferentes categorias.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes estratégias é usada para melhorar a precisão das respostas em um sistema RAG?</strong></p>
<ol type="1">
<li><p>Utilizar um modelo de linguagem unidirecional.</p></li>
<li><p>Combinar a recuperação de informações com a geração de texto, onde a informação recuperada guia a resposta gerada.</p></li>
<li><p>Implementar somente a geração de texto sem recuperação de informações.</p></li>
<li><p>Usar exclusivamente redes neurais convolucionais para processamento de texto.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes afirmativas é verdadeira sobre a arquitetura RAG?</strong></p>
<ol type="1">
<li><p>RAG utiliza apenas modelos de recuperação e não depende de modelos de geração de texto.</p></li>
<li><p>RAG é eficiente para gerar respostas em tempo real com base em grandes volumes de dados externos.</p></li>
<li><p>RAG não é capaz de integrar informações externas ao contexto de uma conversa.</p></li>
<li><p>RAG é uma abordagem exclusiva para análise de sentimentos em texto.</p></li>
</ol></li>
<li><p><strong>Qual é o principal objetivo do LLaMA (Large Language Model Meta AI)?</strong></p>
<ol type="1">
<li><p>Criar um modelo de linguagem massivo e pesado para tarefas específicas.</p></li>
<li><p>Oferecer um modelo de linguagem grande e eficiente que pode ser treinado e implantado com menor custo computacional.</p></li>
<li><p>Desenvolver um modelo de linguagem exclusivamente para tarefas de tradução automática.</p></li>
<li><p>Implementar um modelo de linguagem focado apenas em reconhecimento de voz.</p></li>
</ol></li>
<li><p><strong>Como o LLaMA se diferencia de outros Modelos de Linguagem Grande (LLMs) como GPT-3?</strong></p>
<ol type="1">
<li><p>LLaMA é um modelo maior e mais caro para treinar do que GPT-3.</p></li>
<li><p>LLaMA é projetado para ser mais leve e eficiente, com diferentes tamanhos de modelo, enquanto ainda oferece alto desempenho em tarefas de PLN.</p></li>
<li><p>LLaMA só pode ser utilizado para tarefas de visão computacional.</p></li>
<li><p>LLaMA depende de dados estruturados enquanto GPT-3 usa dados não estruturados.</p></li>
</ol></li>
<li><p><strong>Em que contexto o LLaMA seria especialmente vantajoso para ser utilizado?</strong></p>
<ol type="1">
<li><p>Em dispositivos com recursos computacionais limitados, onde modelos grandes como GPT-3 não podem ser executados eficientemente.</p></li>
<li><p>Em servidores de alto desempenho que exigem modelos extremamente grandes.</p></li>
<li><p>Em ambientes que não necessitam de processamento de linguagem natural.</p></li>
<li><p>Para operações que requerem apenas reconhecimento de fala em tempo real.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes afirmações é verdadeira sobre a arquitetura do LLaMA?</strong></p>
<ol type="1">
<li><p>LLaMA é baseado em uma arquitetura de redes neurais convolucionais.</p></li>
<li><p>LLaMA utiliza a arquitetura Transformer, otimizada para eficiência em termos de parâmetros e recursos computacionais.</p></li>
<li><p>LLaMA não é capaz de realizar tarefas de compreensão de texto.</p></li>
<li><p>LLaMA foi projetado exclusivamente para tarefas de visão computacional.</p></li>
</ol></li>
<li><p><strong>Qual é uma das principais aplicações do LLaMA em chatbots?</strong></p>
<ol type="1">
<li><p>Tradução automática de textos literários complexos.</p></li>
<li><p>Implementação de assistentes virtuais que precisam operar em dispositivos móveis com recursos limitados.</p></li>
<li><p>Análise de grandes volumes de imagens e vídeos.</p></li>
<li><p>Classificação de sons em ambientes ruidosos.</p></li>
</ol></li>
<li><p><strong>Em que tarefa o BERT se destaca em comparação ao GPT?</strong></p>
<ol type="1">
<li><p>Tradução automática</p></li>
<li><p>Geração de texto criativo</p></li>
<li><p>Preenchimento de lacunas em uma frase (masked language modeling)</p></li>
<li><p>Criação de imagens a partir de descrições textuais</p></li>
</ol></li>
<li><p><strong>Qual das seguintes afirmações é verdadeira sobre o modelo GPT?</strong></p>
<ol type="1">
<li><p>O GPT utiliza uma abordagem bidirecional para entender o contexto ao redor de uma palavra em uma frase.</p></li>
<li><p>O GPT é um modelo autoregressivo que gera texto com base nas palavras anteriores da sequência.</p></li>
<li><p>O GPT é incapaz de realizar tarefas de compreensão de texto.</p></li>
<li><p>O GPT é treinado apenas em pequenas bases de dados altamente especializadas.</p></li>
</ol></li>
<li><p><strong>Qual é um dos principais usos de modelos como GPT e BERT em chatbots?</strong></p>
<ol type="1">
<li><p>A tradução automática de grandes textos literários.</p></li>
<li><p>A geração de respostas naturais e coerentes em conversas com os usuários, simulando uma interação humana.</p></li>
<li><p>A análise de imagens e vídeos para identificar objetos.</p></li>
<li><p>A classificação de sons e ruídos em diferentes ambientes.</p></li>
</ol></li>
<li><p><strong>Qual é o principal objetivo ao integrar técnicas avançadas como RAG, Fine-Tuning e LLaMA em chatbots?</strong></p>
<ol type="1">
<li><p>Criar chatbots que operam exclusivamente em dispositivos móveis.</p></li>
<li><p>Melhorar a precisão, relevância e capacidade de adaptação dos chatbots em diferentes cenários.</p></li>
<li><p>Reduzir o tamanho do modelo ao mínimo possível.</p></li>
<li><p>Evitar o uso de inteligência artificial em chatbots.</p></li>
</ol></li>
<li><p><strong>Como a técnica Retrieval-Augmented Generation (RAG) contribui para a eficiência de chatbots?</strong></p>
<ol type="1">
<li><p>Reduzindo o tempo de treinamento dos modelos.</p></li>
<li><p>Permitindo que o chatbot acesse e utilize informações externas para gerar respostas mais precisas e contextuais.</p></li>
<li><p>Substituindo completamente o processo de fine-tuning em modelos grandes.</p></li>
<li><p>Facilitando a compressão de dados de texto em formato binário.</p></li>
</ol></li>
<li><p><strong>Qual é a vantagem de utilizar Fine-Tuning em um modelo como o LLaMA antes de integrá-lo em um chatbot?</strong></p>
<ol type="1">
<li><p>Reduzir o custo de desenvolvimento do chatbot.</p></li>
<li><p>Adaptar o modelo para responder de maneira mais eficaz a consultas específicas de um domínio particular.</p></li>
<li><p>Garantir que o modelo funcione apenas em idiomas específicos.</p></li>
<li><p>Aumentar o número de parâmetros do modelo para melhorar a precisão.</p></li>
</ol></li>
<li><p><strong>Por que é importante considerar a escalabilidade ao integrar técnicas avançadas em chatbots?</strong></p>
<ol type="1">
<li><p>Para garantir que o chatbot possa ser implantado em múltiplos idiomas sem qualquer modificação.</p></li>
<li><p>ara assegurar que o chatbot possa lidar com um grande volume de interações simultâneas sem perda de desempenho.</p></li>
<li><p>Para eliminar a necessidade de armazenamento de dados.</p></li>
<li><p>Para garantir que o chatbot possa operar sem qualquer conexão à internet.</p></li>
</ol></li>
<li><p><strong>Qual das seguintes estratégias pode ajudar a melhorar a personalização das respostas de um chatbot utilizando técnicas avançadas?</strong></p>
<ol type="1">
<li><p>Implementar caching de respostas comuns.</p></li>
<li><p>Utilizar dados históricos de interações para ajustar as respostas às preferências do usuário.</p></li>
<li><p>Reduzir o número de camadas no modelo para melhorar a eficiência.</p></li>
<li><p>Evitar o uso de técnicas de machine learning para gerar respostas.</p></li>
</ol></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./cap3.html" class="pagination-link" aria-label="Processamento de Linguagem Natural (PLN)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Processamento de Linguagem Natural (PLN)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./cap5.html" class="pagination-link" aria-label="Conclusão">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusão</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>