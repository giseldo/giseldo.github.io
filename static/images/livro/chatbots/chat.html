<!DOCTYPE html>
<html lang="pt-BR">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>Construindo Chatbots: Do AIML e PLN até os Modernos
LLMs</title>

    <meta name="author" content="Giseldo Neo<br />
Alana Neo" />
  
    <meta name="date" content="2025-12-16" />
  
    <link rel="stylesheet" href="styles.css" />
  
  
  </head>

<body>

  <!-- Botão hambúrguer para mobile -->
  <button class="menu-toggle" aria-label="Abrir menu" aria-expanded="false">
    <span></span>
    <span></span>
    <span></span>
  </button>

  <!-- Botão para ocultar/exibir menu lateral (desktop) -->
  <button class="sidebar-toggle" aria-label="Ocultar menu" aria-expanded="true" title="Ocultar menu lateral">
    <span class="sidebar-toggle-icon"></span>
  </button>

  <div class="layout">

    <!-- Menu lateral automático -->
    <nav class="menu">
      <div class="toc-controls">
        <button class="toc-expand-all" title="Expandir todos os menus">Expandir tudo</button>
        <button class="toc-collapse-all" title="Colapsar todos os menus">Colapsar tudo</button>
      </div>
            <ul>
            <li><a href="#agradecimentos"
            id="toc-agradecimentos">Agradecimentos</a></li>
            <li><a href="#prefácio" id="toc-prefácio">Prefácio</a></li>
            <li><a href="#informações"
            id="toc-informações">Informações</a></li>
            <li><a href="#resumo" id="toc-resumo">Resumo</a></li>
            <li><a href="#introdução" id="toc-introdução">Introdução</a>
            <ul>
            <li><a href="#contexto" id="toc-contexto">Contexto</a></li>
            <li><a href="#agentes" id="toc-agentes">Agentes</a></li>
            <li><a href="#fluxo-conversacional"
            id="toc-fluxo-conversacional">Fluxo Conversacional</a></li>
            <li><a href="#histórico"
            id="toc-histórico">Histórico</a></li>
            <li><a href="#abordagens"
            id="toc-abordagens">Abordagens</a></li>
            <li><a href="#chatbot-pensa" id="toc-chatbot-pensa">Chatbot
            pensa?</a></li>
            <li><a href="#exercícios"
            id="toc-exercícios">Exercícios</a></li>
            </ul></li>
            <li><a href="#cap:ELIZA" id="toc-cap:ELIZA">ELIZA e AIML</a>
            <ul>
            <li><a href="#introdução-1"
            id="toc-introdução-1">Introdução</a></li>
            <li><a href="#processamento"
            id="toc-processamento">Processamento</a></li>
            <li><a href="#regras-de-transformação"
            id="toc-regras-de-transformação">Regras de
            Transformação</a></li>
            <li><a href="#implementação-e-variações"
            id="toc-implementação-e-variações">Implementação e
            Variações</a></li>
            <li><a href="#mecanismo-de-pesos"
            id="toc-mecanismo-de-pesos">Mecanismo de Pesos</a></li>
            <li><a href="#geração-de-texto"
            id="toc-geração-de-texto">Geração de Texto</a></li>
            <li><a href="#eliza-em-python"
            id="toc-eliza-em-python">ELIZA em Python</a></li>
            <li><a href="#eliza-com-python-e-gradio"
            id="toc-eliza-com-python-e-gradio">ELIZA com Python e
            Gradio</a></li>
            <li><a href="#cap:AIML" id="toc-cap:AIML">Artificial
            Intelligence Markup Language (AIML)</a>
            <ul>
            <li><a href="#tags-do-aiml" id="toc-tags-do-aiml">Tags do
            AIML</a></li>
            <li><a href="#exemplo-em-python"
            id="toc-exemplo-em-python">Exemplo em Python</a></li>
            </ul></li>
            <li><a href="#exercícios-1"
            id="toc-exercícios-1">Exercícios</a></li>
            </ul></li>
            <li><a href="#cap:PLN" id="toc-cap:PLN">Processamento de
            Linguagem Natural (PLN)</a>
            <ul>
            <li><a href="#inteligência-artificial"
            id="toc-inteligência-artificial">Inteligência
            Artificial</a></li>
            <li><a href="#aprendizado-de-máquina"
            id="toc-aprendizado-de-máquina">Aprendizado de
            Máquina</a></li>
            <li><a href="#processamento-de-linguagem-natural"
            id="toc-processamento-de-linguagem-natural">Processamento de
            Linguagem Natural</a></li>
            <li><a href="#instalação-do-python"
            id="toc-instalação-do-python">Instalação do Python</a></li>
            <li><a href="#principais-técnicas-de-pln"
            id="toc-principais-técnicas-de-pln">Principais Técnicas de
            PLN</a>
            <ul>
            <li><a href="#tokenização"
            id="toc-tokenização">Tokenização</a></li>
            <li><a href="#lematização"
            id="toc-lematização">Lematização</a></li>
            <li><a href="#stemização"
            id="toc-stemização">Stemização</a></li>
            <li><a href="#stopwords"
            id="toc-stopwords">Stopwords</a></li>
            </ul></li>
            <li><a href="#outras-técnicas-de-pln"
            id="toc-outras-técnicas-de-pln">Outras técnicas de
            PLN</a></li>
            <li><a href="#expressões-regulares"
            id="toc-expressões-regulares">Expressões Regulares</a></li>
            <li><a href="#entendimento-de-linguagem-natural"
            id="toc-entendimento-de-linguagem-natural">Entendimento de
            Linguagem Natural</a>
            <ul>
            <li><a href="#intents" id="toc-intents">Intents</a></li>
            <li><a href="#sec:intents_utterances"
            id="toc-sec:intents_utterances">Utterances</a></li>
            <li><a href="#sec:intents_entities"
            id="toc-sec:intents_entities">Entities</a></li>
            <li><a href="#desafios" id="toc-desafios">Desafios</a></li>
            </ul></li>
            <li><a href="#vetorização-e-representação-de-texto"
            id="toc-vetorização-e-representação-de-texto">Vetorização e
            Representação de Texto</a>
            <ul>
            <li><a href="#one-hot-encoding"
            id="toc-one-hot-encoding">One-Hot Encoding</a></li>
            <li><a href="#bag-of-words-bow"
            id="toc-bag-of-words-bow">Bag of Words (BoW)</a></li>
            <li><a href="#tf-idf" id="toc-tf-idf">TF-IDF</a></li>
            <li><a href="#comparação-de-técnicas-de-vetorização"
            id="toc-comparação-de-técnicas-de-vetorização">Comparação de
            Técnicas de Vetorização</a></li>
            <li><a href="#implementação-em-projetos-reais"
            id="toc-implementação-em-projetos-reais">Implementação em
            Projetos Reais</a></li>
            <li><a href="#embeddings-de-palavras"
            id="toc-embeddings-de-palavras">Embeddings de Palavras
            </a></li>
            <li><a href="#word2vec" id="toc-word2vec">Word2Vec</a></li>
            <li><a href="#glove" id="toc-glove">GloVe</a></li>
            <li><a href="#fasttext" id="toc-fasttext">FastText</a></li>
            </ul></li>
            <li><a href="#resumo-1" id="toc-resumo-1">Resumo</a></li>
            <li><a href="#exercícios-2"
            id="toc-exercícios-2">Exercícios</a></li>
            </ul></li>
            <li><a href="#cap:LLM" id="toc-cap:LLM">Modelos de Linguagem
            Grande (LLM)</a>
            <ul>
            <li><a href="#introdução-2"
            id="toc-introdução-2">Introdução</a></li>
            <li><a href="#arquitetura-geral-do-transformer"
            id="toc-arquitetura-geral-do-transformer">Arquitetura Geral
            do Transformer</a>
            <ul>
            <li><a
            href="#bert-bidirectional-encoder-representations-from-transformers"
            id="toc-bert-bidirectional-encoder-representations-from-transformers">BERT
            (<em>Bidirectional Encoder Representations from
            Transformers</em>)</a></li>
            <li><a href="#gpt-generative-pre-trained-transformer"
            id="toc-gpt-generative-pre-trained-transformer">GPT
            (<em>Generative Pre-trained Transformer</em>)</a></li>
            <li><a href="#distilbert-base-uncased"
            id="toc-distilbert-base-uncased">distilbert-base-uncased</a></li>
            </ul></li>
            <li><a href="#fine-tuning-de-modelos-pré-treinados"
            id="toc-fine-tuning-de-modelos-pré-treinados">Fine-Tuning de
            Modelos Pré-Treinados</a></li>
            <li><a href="#few-shot-e-zero-shot-learning"
            id="toc-few-shot-e-zero-shot-learning">Few Shot e Zero Shot
            Learning</a></li>
            <li><a href="#retrieval-augmented-generation-rag"
            id="toc-retrieval-augmented-generation-rag">Retrieval-Augmented
            Generation (RAG)</a></li>
            <li><a href="#llama" id="toc-llama">LLaMA</a>
            <ul>
            <li><a href="#arquitetura-do-llama"
            id="toc-arquitetura-do-llama">Arquitetura do LLaMA</a></li>
            <li><a href="#exemplo-de-uso-do-llama"
            id="toc-exemplo-de-uso-do-llama">Exemplo de uso do
            LLaMA</a></li>
            <li><a href="#aplicações-de-llama"
            id="toc-aplicações-de-llama">Aplicações de LLaMA</a></li>
            <li><a href="#comparação-com-outros-modelos"
            id="toc-comparação-com-outros-modelos">Comparação com Outros
            Modelos</a></li>
            </ul></li>
            <li><a href="#llm-na-prática" id="toc-llm-na-prática">LLM na
            prática</a>
            <ul>
            <li><a href="#hugging-face-pipeline"
            id="toc-hugging-face-pipeline">Hugging Face
            Pipeline</a></li>
            <li><a href="#llm-local-com-ollama"
            id="toc-llm-local-com-ollama">LLM Local com Ollama</a></li>
            <li><a href="#tokenizador-no-llm"
            id="toc-tokenizador-no-llm">Tokenizador no LLM</a></li>
            <li><a href="#langchain"
            id="toc-langchain">LangChain</a></li>
            <li><a href="#mangaba.ai"
            id="toc-mangaba.ai">Mangaba.AI</a></li>
            <li><a href="#fluxos-em-llm-ou-engenharia-de-prompts"
            id="toc-fluxos-em-llm-ou-engenharia-de-prompts">Fluxos em
            LLM (ou Engenharia de Prompts)</a></li>
            <li><a href="#como-funciona" id="toc-como-funciona">Como
            Funciona?</a></li>
            <li><a href="#exemplos-de-requisitos"
            id="toc-exemplos-de-requisitos">Exemplos de
            requisitos</a></li>
            </ul></li>
            <li><a href="#integração-de-técnicas"
            id="toc-integração-de-técnicas">Integração de
            Técnicas</a></li>
            <li><a href="#api-e-playground"
            id="toc-api-e-playground">API e Playground</a></li>
            <li><a href="#resumo-2" id="toc-resumo-2">Resumo</a></li>
            <li><a href="#exercícios-3"
            id="toc-exercícios-3">Exercícios</a></li>
            </ul></li>
            <li><a href="#conclusao" id="toc-conclusao">Conclusão</a>
            <ul>
            <li><a href="#considerações-éticas"
            id="toc-considerações-éticas">Considerações Éticas</a></li>
            <li><a href="#privacidade-e-proteção-de-dados"
            id="toc-privacidade-e-proteção-de-dados">Privacidade e
            Proteção de Dados</a></li>
            <li><a href="#viés-algorítmico"
            id="toc-viés-algorítmico">Viés Algorítmico</a></li>
            <li><a href="#segurança-de-chatbots"
            id="toc-segurança-de-chatbots">Segurança de
            Chatbots</a></li>
            <li><a href="#manutenção-e-atualização"
            id="toc-manutenção-e-atualização">Manutenção e
            Atualização</a></li>
            <li><a href="#perspectivas-futuras"
            id="toc-perspectivas-futuras">Perspectivas Futuras</a></li>
            <li><a href="#oportunidades-de-inovação"
            id="toc-oportunidades-de-inovação">Oportunidades de
            Inovação</a>
            <ul>
            <li><a href="#tendências-tecnológicas-emergentes"
            id="toc-tendências-tecnológicas-emergentes">Tendências
            Tecnológicas Emergentes</a></li>
            <li><a href="#desafios-em-abertos"
            id="toc-desafios-em-abertos">Desafios em abertos</a></li>
            <li><a href="#oportunidades-de-pd"
            id="toc-oportunidades-de-pd">Oportunidades de
            P&amp;D</a></li>
            <li><a href="#recomendações-para-futuros-desenvolvedores"
            id="toc-recomendações-para-futuros-desenvolvedores">Recomendações
            para Futuros Desenvolvedores</a></li>
            </ul></li>
            <li><a href="#considerações-finais"
            id="toc-considerações-finais">Considerações Finais</a></li>
            <li><a href="#exercícios-4"
            id="toc-exercícios-4">Exercícios</a></li>
            </ul></li>
            <li><a href="#sobre-os-autores"
            id="toc-sobre-os-autores">Sobre os autores</a>
            <ul>
            <li><a href="#giseldo-da-silva-neo"
            id="toc-giseldo-da-silva-neo">Giseldo da Silva Neo</a></li>
            <li><a href="#alana-viana-borges-da-silva-neo"
            id="toc-alana-viana-borges-da-silva-neo">Alana Viana Borges
            da Silva Neo</a></li>
            <li><a href="#contato" id="toc-contato">Contato</a></li>
            <li><a href="#aviso-legal" id="toc-aviso-legal">Aviso
            Legal</a></li>
            <li><a href="#uso-da-ia-generativa"
            id="toc-uso-da-ia-generativa">Uso da IA Generativa</a></li>
            </ul></li>
            </ul>
          </nav>

    <!-- Conteúdo principal -->
    <main class="content">
      <header class="document-title">
    <h1>Construindo Chatbots: Do AIML e PLN até os Modernos LLMs</h1>
          <p class="author">Giseldo Neo<br />
Alana Neo</p>
              <p class="date">2025-12-16</p>
      </header>
      <div class="center">
      <p><strong><strong></strong></strong></p>
      <p><strong><strong></strong></strong></p>
      <p><strong><strong>Teoria e Prática com
      Python</strong></strong></p>
      </div>
      <div class="center">
      <table>
      <tbody>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;"></td>
      </tr>
      <tr>
      <td style="text-align: right;"><span><strong>Preparação e
      revisão</strong></span></td>
      <td style="text-align: left;">Giseldo Neo e Alana Neo</td>
      </tr>
      <tr>
      <td
      style="text-align: right;"><span><strong>Diagramação</strong></span></td>
      <td style="text-align: left;">Giseldo Neo</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;"></td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;"></td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">versão 0.8 (gerado em
      2025-12-16)</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;"></td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">E-mail: <a
      href="mailto:giseldo@gmail.com">giseldo@gmail.com</a></td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;"></td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">Todos os direitos reservados.</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">Nenhuma parte desta publicação</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">poderá ser armazenada ou
      reproduzida</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">por qualquer meio sem a
      autorização</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;">por escrito dos autores.</td>
      </tr>
      <tr>
      <td style="text-align: right;"></td>
      <td style="text-align: left;"></td>
      </tr>
      </tbody>
      </table>
      </div>
      <h1 class="unnumbered" id="agradecimentos">Agradecimentos</h1>
      <p>Expresso minha gratidão ao Professor Dr. Olival de Gusmão
      Freitas Júnior (UFAL) pela cuidadosa revisão deste livro, cujo
      olhar crítico contribuiu para elevar a qualidade do trabalho.
      Agradeço especialmente ao Pesquisador Dr. Joaquim José Cintra Maia
      Honório pelo auxílio no capítulo dos LLMs e <em>Fine-Tuning</em>;
      essas contribuições foram importantes e melhoraram muito o texto a
      partir do rascunho original. Agradeço ao Professor Dr. Evandro de
      Barros Costa (UFAL) pela apresentação do ELIZA; ao Professor Dr.
      José Antão Beltrão Moura (UFCG) pela inspiração e dedicação ao
      ensino; e agradeço ao Professor Msc. Otávio Monteiro Pereira
      (IFAL) pelo apoio relacionado à ética dos chatbots. Agradeço
      também ao Instituto Federal de Alagoas e a seus servidores.
      Estendo meus agradecimentos a todos que, de forma direta ou
      indireta, colaboraram com ideias, discussões e sugestões que
      enriqueceram o conteúdo apresentado.</p>
      <div class="flushright">
      <p>Giseldo Neo</p>
      </div>
      <p>Agradeço à minha família, que sempre esteve ao meu lado em cada
      etapa desta jornada. Aos meus pais, pelo amor, paciência e
      incentivo incondicional. Aos parentes, pelo apoio e pela
      compreensão das ausências nos momentos de dedicação intensa.
      Agradeço também aos amigos e colegas que contribuíram com ideias,
      sugestões e palavras de motivação nos dias desafiadores. Este
      livro é fruto não apenas do meu esforço, mas também da força
      coletiva daqueles que acreditaram e apoiaram para que esta obra se
      tornasse realidade.</p>
      <div class="flushright">
      <p>Alana Neo</p>
      </div>
      <h1 class="unnumbered" id="prefácio">Prefácio</h1>
      <p>Vivemos um momento histórico em que a inteligência artificial
      (IA) deixou de ser apenas tema de ficção científica para se tornar
      parte do nosso cotidiano. Os chatbots, os avanços em Processamento
      de Linguagem Natural (PLN) e os Modelos de Linguagem Grande (LLMs)
      são hoje protagonistas dessa transformação, moldando a forma como
      nos comunicamos, aprendemos, trabalhamos e interagimos com a
      tecnologia.</p>
      <p>O que antes era limitado a sistemas rígidos, que apenas
      repetiam respostas pré-programadas, evoluiu para assistentes
      virtuais capazes de compreender contexto, interpretar nuances da
      linguagem e até gerar conteúdo inédito. Essa trajetória revela não
      apenas conquistas tecnológicas, porém também desafios éticos,
      sociais e econômicos.</p>
      <p>Este livro é um convite acessível e profundo para quem deseja
      compreender como nascem, evoluem e funcionam os chatbots — desde
      os primórdios com ELIZA até os modernos modelos de linguagem como
      o ChatGPT. Ele nasce com o propósito de explicar, refletir e
      inspirar. Explicar como o PLN e os LLMs funcionam e por que se
      tornaram tão relevantes. Refletir sobre os impactos dessa
      revolução: o futuro do trabalho, a responsabilidade no uso da IA,
      a relação entre humano e máquina. Inspirar pesquisadores,
      profissionais, estudantes e curiosos a explorar esse campo em
      rápida expansão, contribuindo para um uso responsável e criativo
      dessas ferramentas tecnológicas. Ele também é um convite para
      explorar esse universo fascinante. Você encontrará explicações
      sobre como essas tecnologias funcionam, exemplos e reflexões sobre
      seus impactos na sociedade. Mais do que compreender as máquinas
      que conversam, trata-se de entender o futuro da linguagem e da
      interação entre humanos e sistemas inteligentes.</p>
      <p>Seja você estudante, pesquisador ou profissional do mercado,
      este livro pode inspirar novas ideias e mostrar como a
      inteligência artificial conversacional já está transformando o
      presente — e continuará a transformar o futuro.</p>
      <div class="flushright">
      <p>Boa leitura!</p>
      </div>
      <h1 id="informações">Informações</h1>
      <p>O código-fonte - com os mesmos códigos em python utilizados
      neste livro - está disponível no endereço:</p>
      <ul>
      <li><p><a
      href="https://github.com/giseldo/chatbotLivroCodigoFonte"
      class="uri">https://github.com/giseldo/chatbotLivroCodigoFonte</a>.</p></li>
      </ul>
      <p>Uma versão atualizada do livro pode ser encontrada no site</p>
      <ul>
      <li><p><a href="https://giseldo.github.io/"
      class="uri">https://giseldo.github.io/</a>.</p></li>
      </ul>
      <p>Citação Formato ABNT:</p>
      <ul>
      <li><p>Neo, Giseldo da Silva &amp; Neo Alana Viana Borges da
      Silva. Construindo Chatbots: Do AIML e PLN até os Modernos LLMs.
      2025. Edição do autor.</p></li>
      </ul>
      <p>Citação Bibtex:</p>
      <pre><code>    @book{neoneo2025,
        author = {Giseldo da Silva Neo &amp; Alana da Silva Neo},
        title = {Construindo Chatbots: Do AIML e PLN até 
            os Modernos LLMs},
        year = 2025,
        publisher = {Edição do autor}
    }</code></pre>
      <h1 class="unnumbered" id="resumo">Resumo</h1>
      <p>Se você já se perguntou como funcionam os chatbots que
      conversam como humanos, este livro é a sua porta de entrada para
      um dos campos mais fascinantes da inteligência artificial. Ele é
      mais do que um guia técnico — é uma jornada envolvente pela
      evolução, construção e futuro dos agentes conversacionais, unindo
      clareza didática com profundidade técnica. A obra começa
      explorando o conceito de chatbot, diferenciando suas principais
      categorias — conversacionais e orientados a tarefas — e
      contextualizando o leitor com um panorama histórico que vai do
      lendário ELIZA, criado nos anos 1960, até os impressionantes
      modelos atuais como ChatGPT. Essa viagem no tempo é pontuada por
      explicações acessíveis e exemplos práticos em Python que mostram,
      passo a passo, como esses sistemas funcionam por trás das
      cortinas. O livro se destaca por traduzir conceitos complexos em
      linguagem clara. Você vai entender o que é o pattern matching
      usado por chatbots clássicos, como funciona o AIML, e por que
      expressões regulares ainda são ferramentas utilizadas no
      desenvolvimento de bots. Em capítulos dedicados ao Processamento
      de Linguagem Natural (PLN), técnicas como tokenização, lematização
      e análise sintática ganham vida por meio de exemplos aplicáveis,
      preparando o leitor para dar seus próprios passos no mundo da IA
      conversacional. Mas ele não se limita ao passado. Ele mergulha
      fundo nas tecnologias que revolucionaram o campo, como
      <em>Transformers</em>, Word2Vec e os Modelos de Linguagem Grande
      (LLMs), incluindo BERT, GPT e LLaMA. O leitor descobre como essas
      arquiteturas funcionam, como treiná-las e como aplicá-las usando
      frameworks modernos como Hugging Face e LangChain. Além de guiar a
      construção técnica de um chatbot — do código ao deploy — o livro
      também traz reflexões sobre ética, privacidade e o impacto social
      desses agentes. Tópicos como personalização, explicabilidade e
      segurança ampliam o olhar do leitor para além da implementação.
      Combinando teoria, prática e visão de futuro, ele é um convite
      para programadores, educadores, empreendedores e curiosos que
      desejam dominar a arte de construir diálogos entre humanos e
      máquinas. É uma obra que ensina, inspira e instiga. Se você quer
      entender como os chatbots realmente pensam — ou pelo menos como
      fingem tão bem — este livro é para você.</p>
      <h1 id="introdução">Introdução</h1>
      <div class="epigraph">
      <p>“Podemos ver apenas uma curta distância à frente, mas pode-se
      ver muito que precisa ser feito.”</p>
      <p>Alan Turing</p>
      </div>
      <div class="myboxobj">
      <p><span>Objetivo</span> Apresentar os conceitos fundamentais de
      chatbots, sua classificação, histórico e principais abordagens,
      preparando o leitor para compreender a evolução dessas
      tecnologias.</p>
      </div>
      <h2 id="contexto">Contexto</h2>
      <p>Para uma parcela da população com maior afinidade com a
      inteligência artificial, o chatbot tornou-se uma ferramenta
      essencial para executar tarefas, redigir e-mails e traduzir
      textos. O que um dia causou espanto e admiração — como ocorreu com
      a descoberta do fogo, o computador pessoal, a Internet, as
      ferramentas de busca e as redes sociais — hoje se converteu em
      dependência.</p>
      <p>Neste contexto, um chatbot é um programa de computador que
      simula uma conversa humana, via texto ou áudio, oferecendo
      respostas diretas a perguntas e apoiando diversas atividades,
      desde conversas gerais até ações específicas, como abrir uma conta
      bancária ou agendar um voo para o show do seu cantor
      preferido.</p>
      <p>Para entender como chegamos a esse ponto, vale recordar a
      trajetória do próprio conceito. Embora o programa ELIZA, criado
      por <span class="citation" data-cites="Weizenbaum1996">Weizenbaum
      (1966)</span>, seja frequentemente citado como um dos primeiros
      exemplos de software conversacional, o termo “chatbot” ainda não
      era empregado à época. Sua origem remete a “chatterbot” — sinônimo
      de “chatbot” —, popularizado por Michael Mauldin em 1994 ao
      descrever seu programa JULIA <span class="citation"
      data-cites="Mauldin1994">(Mauldin 1994)</span>. Anos depois,
      outras publicações acadêmicas, como os anais da <em>Virtual Worlds
      and Simulation Conference</em> de 1998 <span class="citation"
      data-cites="Jacobstein1998">(Jacobstein et al. 1998)</span>,
      contribuíram para consolidar os termos entre os pesquisadores.</p>
      <p>O chatbot ELIZA representou um experimento marcante na
      interação entre humano e computador <span class="citation"
      data-cites="Weizenbaum1996">(Weizenbaum 1966)</span>. Seu roteiro
      (ou script) mais famoso, o DOCTOR, imitava rudimentarmente um
      psicoterapeuta, utilizando correspondência de padrões simples. Por
      exemplo, quando um usuário inseria a frase “<em>Estou triste</em>”
      no ELIZA, o programa respondia “<em>Por que você está triste
      hoje?</em>”, reformulando a entrada do usuário como uma pergunta.
      O funcionamento básico do sistema baseava-se em um conjunto
      restrito de regras e substituições, o que lhe permitia apenas uma
      compreensão superficial e limitada da linguagem humana.</p>
      <p>O roteiro DOCTOR do ELIZA adequou-se bem a um tipo de diálogo
      mais simples, pois suas respostas dependiam de pouco conhecimento
      sobre o ambiente externo. As regras no roteiro permitiam que o
      programa respondesse ao usuário com outras perguntas ou
      simplesmente refletisse a afirmação original.</p>
      <div class="mdframed">
      <p><span class="smallcaps">NOTA</span></p>
      <p>Uma descrição detalhada do funcionamento do ELIZA e o roteiro
      DOCTOR, com exemplos em Python, será apresentada no Capítulo <a
      href="#cap:ELIZA" data-reference-type="ref"
      data-reference="cap:ELIZA">3</a>.</p>
      </div>
      <p>Outro chatbot famoso é o ChatGPT da OpenAI. Ele é um programa
      capaz de gerar texto muito semelhante ao criado por humanos. Ele
      utiliza redes neurais, com aprendizagem profunda, para gerar
      sentenças e parágrafos com base nas entradas e informações
      fornecidas. Entre suas capacidades, ele pode traduzir e resumir
      textos, responder a perguntas e explicar conceitos. Contudo, o
      ChatGPT não possui consciência nem a capacidade de compreender
      contexto ou emoções.</p>
      <p>O chatGPT é um exemplo de Modelo de Linguagem Grande (em inglês
      Large Language Model - LLM), baseado na arquitetura
      <em>Transformers</em>, introduzida em 2017 <span class="citation"
      data-cites="vaswani2017attention">(Vaswani et al. 2017)</span>.
      Modelos deste tipo são treinados com terabytes de texto,
      utilizando mecanismos de autoatenção que avaliam a relevância de
      cada palavra em uma frase. Ao contrário das regras manuais do
      ELIZA, os LLMs extraem padrões linguísticos a partir da vasta
      quantidade de dados com que a rede neural foi treinada.</p>
      <p>Esses dois chatbots, ELIZA e ChatGPT, são bons representantes
      do tipo de chatbot conversacional. Apesar de terem surgido com
      décadas de diferença — ELIZA em 1966 e ChatGPT em 2022 — e de
      diferirem bastante na forma como geram suas respostas, ambos
      compartilham semelhanças em seu objetivo: conversar sobre
      determinado assunto ou responder perguntas, mantendo o usuário em
      um diálogo fluido quando necessário. Chatbots com essas
      características podem ser agrupados, de acordo com o objetivo,
      como chatbots conversacionais e são utilizados para interagir
      sobre assuntos gerais.</p>
      <p>Outro tipo de chatbot classificado em relação ao objetivo é o
      chatbot orientado a tarefas. Os chatbots orientados a tarefas
      executam ações específicas, como abrir uma conta bancária ou pedir
      uma pizza. Geralmente, as empresas disponibilizam chatbots
      orientados a tarefas para seus usuários, com regras de negócio
      embutidas na conversação e com fluxos bem definidos. Normalmente,
      não se espera pedir uma pizza e, no mesmo chatbot, discutir os
      estudos sobre Ética do filósofo Immanuel Kant (embora talvez haja
      quem queira).</p>
      <p>Essas duas classificações, “conversacional” e “orientado a
      tarefas”, ainda não são suficientes para uma completa
      classificação. Existem outras classificações que serão discutidas
      em seções posteriores. Além disso, uma abordagem híbrida, unindo
      funções de chatbots do tipo “conversacional” e “orientado a
      tarefas” vem sendo utilizada para atender às necessidades dos
      usuários.</p>
      <p>A popularidade dos chatbots tem crescido significativamente em
      diversos domínios de aplicação <span class="citation"
      data-cites="B2020 Klopfenstein2017 Sharma2020">(Marcondes,
      Almeida, and Novais 2020; Klopfenstein et al. 2017; Sharma, Verma,
      and Sahni 2020)</span>. Essa tendência é corroborada pelo aumento
      do interesse de busca pelo termo “chatbots”, conforme análise de
      dados do Google Trends no período entre 2020 e 2025 (Figura <a
      href="#fig:trends" data-reference-type="ref"
      data-reference="fig:trends">2.1</a>). Nesta figura, os valores
      representam o interesse relativo de busca ao longo do tempo, onde
      100 indica o pico de popularidade no período analisado e 0 (ou a
      ausência de dados) indica interesse mínimo ou dados
      insuficientes.</p>
      <figure id="fig:trends">
      <p><img src="./fig/trends.png" style="width:90.0%" alt="image" />
      <span id="fig:trends" data-label="fig:trends"></span></p>
      <figcaption>Figure 1: Evolução do interesse de busca pelo termo
      “chatbot” (Google Trends, 2020-2025).<br />
      Fonte: Google Trends acesso em 05/04/2025</figcaption>
      </figure>
      <h2 id="agentes">Agentes</h2>
      <p>Os termos “chatbots” e “agentes” são usadas
      indiscriminadamente, o que pode causar confusão. Vamos a uma
      definição mais precisa. Um chatbot é um programa computacional
      projetado para interagir com usuários por meio de linguagem
      natural. Por outro lado, o conceito de agente possui uma definição
      mais ampla. Um agente trata-se de uma entidade computacional que
      percebe seu ambiente por meio de sensores e atua sobre esse
      ambiente por meio de atuadores.</p>
      <p>Nesse contexto, um chatbot (Figura <a href="#fig:chatbot"
      data-reference-type="ref" data-reference="fig:chatbot">2.3</a>)
      pode ser considerado uma instanciação específica de um agente -
      veja na Figura <a href="#fig:agente" data-reference-type="ref"
      data-reference="fig:agente">2.2</a> a arquitetura conceitual de
      alto nível para um agente - cujo propósito primário é a interação
      conversacional em linguagem natural.</p>
      <figure id="fig:agente">
      <p><img src="./fig/original_agente.png" style="width:80.0%"
      alt="image" /> <span id="fig:agente"
      data-label="fig:agente"></span></p>
      <figcaption>Figure 2: Arquitetura conceitual de um agente.<br />
      Fonte: Adaptado de <span class="citation"
      data-cites="Russel2013">(Russel and Norving
      2013)</span></figcaption>
      </figure>
      <figure id="fig:chatbot">
      <p><img src="./fig/chatbot.png" style="width:80.0%" alt="image" />
      <span id="fig:chatbot" data-label="fig:chatbot"></span></p>
      <figcaption>Figure 3: Representação esquemática de um
      chatbot.</figcaption>
      </figure>
      <p>Com o advento de modelos de linguagem, como os baseados na
      arquitetura <em>Generative Pretrained Transformer</em> (GPT), a
      exemplo do ChatGPT, observou-se uma recontextualização do termo
      “agente” no domínio dos sistemas conversacionais. Nessa abordagem
      mais recente, um sistema focado predominantemente na geração de
      texto conversacional tende a ser denominado “chatbot”. Em
      contraste, o termo “agente” é frequentemente reservado para
      sistemas que, além da capacidade conversacional, integram e
      utilizam ferramentas externas (por exemplo, acesso à Internet,
      execução de código e interação com APIs) para realizar tarefas
      complexas e interagir proativamente com o ambiente digital. Um
      sistema capaz de realizar uma compra online, processar um
      pagamento e confirmar um endereço de entrega por meio do navegador
      do usuário seria, portanto, classificado como um agente,
      diferentemente de chatbots mais simples como ELIZA, ou mesmo
      versões mais simples do chatGPT (GPT-2), cujo foco era
      estritamente o diálogo.</p>
      <h2 id="fluxo-conversacional">Fluxo Conversacional</h2>
      <p>Um chatbot responde a uma entrada do usuário. Porém, essa
      interação textual mediada por chatbots não se constitui em uma
      mera justaposição aleatória de turnos de conversação ou pares
      isolados de estímulo-resposta. Pelo contrário, espera-se que a
      conversação exiba coerência e mantenha relações lógicas e
      semânticas entre os turnos consecutivos. O estudo da estrutura e
      organização da conversa humana é abordado por disciplinas como a
      Análise da Conversação.</p>
      <p>No contexto da análise da conversação em língua portuguesa, os
      trabalhos de Marcuschi <span class="citation"
      data-cites="Marchuschi1986">(Marcuschi 1986)</span> são relevantes
      ao investigar a organização dessa conversação. Marcuschi analisou
      a estrutura conversacional em termos de unidades coesas, como o
      “tópico conversacional”, que agrupa turnos relacionados a um mesmo
      assunto ou propósito interacional.</p>
      <p>Conceitos oriundos da Análise da Conversação, como a gestão de
      tópicos, têm sido aplicados no desenvolvimento de chatbots para
      aprimorar sua capacidade de manter diálogos coerentes e
      contextualmente relevantes com usuários humanos <span
      class="citation" data-cites="Neves2005">(Neves and Barros
      2005)</span>.</p>
      <p>Na prática de desenvolvimento de sistemas conversacionais, a
      estrutura lógica e sequencial da interação é frequentemente
      modelada e referida como “fluxo de conversação” ou “fluxo de
      diálogo”. Contudo, é importante ressaltar que a implementação
      explícita de modelos sofisticados de gerenciamento de diálogo,
      inspirados na Análise da Conversação, não é uma característica
      universal de todos os chatbots, variando conforme a complexidade e
      o propósito do sistema.</p>
      <p>Um exemplo esquemático de um fluxo conversacional é apresentado
      na Figura <a href="#fig:fluxo" data-reference-type="ref"
      data-reference="fig:fluxo">2.4</a>. Nesta figura, o fluxo de
      conversação inicia quando o usuário entra com o texto: I WANT
      PIZZA, o chatbot responde com uma pergunta: HI I AM THE PIZZA BOT.
      CAN I CONFIRM THIS DELIVERY TO YOUR HOUSE? O usuário então pode
      responder: YES, e o chatbot finaliza a conversa com: IT’S ON THE
      WAY. THANK YOU FOR CHOOSE OUR PRODUCT. Caso o usuário responda:
      NO, o chatbot responde com a pergunta original: HI I AM THE PIZZA
      BOT. CAN I CONFIRM THIS DELIVERY TO YOUR HOUSE? O fluxo de
      conversação continua até que o usuário responda com um “YES” para
      a pergunta inicial. Essa estrutura de perguntas e respostas é
      comum em chatbots orientados a tarefas, onde o objetivo é guiar o
      usuário por um processo específico, tal como fazer um pedido de
      pizza.</p>
      <figure id="fig:fluxo">
      <p><img src="./fig/fluxo.png" style="width:90.0%" alt="image" />
      <span id="fig:fluxo" data-label="fig:fluxo"></span></p>
      <figcaption>Figure 4: Exemplo esquemático de um fluxo
      conversacional em um chatbot.</figcaption>
      </figure>
      <p>Um outro tipo de fluxo para um chatbot que vende roupas online
      está representado na Figura <a
      href="#fig:representacaodeumaarvore" data-reference-type="ref"
      data-reference="fig:representacaodeumaarvore">2.5</a>.</p>
      <figure id="fig:representacaodeumaarvore">
      <p><img src="./fig/image14.png" style="width:70.0%" alt="image" />
      <span id="fig:representacaodeumaarvore"
      data-label="fig:representacaodeumaarvore"></span></p>
      <figcaption>Figure 5: Representação de uma árvore de decisão para
      vender roupas online.<br />
      Retirado de <span class="citation" data-cites="Raj2019">(Raj
      2019)</span>.</figcaption>
      </figure>
      <h2 id="histórico">Histórico</h2>
      <p>Um marco significativo na evolução dos chatbots depois do ELIZA
      foi o ALICE, que introduziu a Artificial Intelligence Markup
      Language (AIML), uma linguagem de marcação baseada em XML <span
      class="citation" data-cites="Wallace2000">(Richard S. Wallace
      2000)</span>. A AIML estabeleceu um paradigma para a construção de
      agentes conversacionais ao empregar algoritmos de correspondência
      de padrões. Essa abordagem utiliza modelos pré-definidos para
      mapear as entradas do usuário a respostas correspondentes,
      permitindo a definição modular de blocos de conhecimento <span
      class="citation" data-cites="Wallace2000">(Richard S. Wallace
      2000)</span>.</p>
      <p>No contexto brasileiro, um dos primeiros chatbots documentados
      capaz de interagir em português, inspirado no modelo ELIZA, foi o
      Cybele <span class="citation"
      data-cites="primo2001chatterbot">(PRIMO and COELHO 2001)</span>.
      Posteriormente, foi desenvolvido o Elecktra, também em língua
      portuguesa, com aplicação voltada para a educação a distância
      <span class="citation" data-cites="Leonhardt2003">(Leonhardt,
      Neisse, and Tarouco 2003)</span>. Em 2019, o processo de inscrição
      para o Exame Nacional do Ensino Médio (ENEM) foi disponibilizado
      por meio de uma interface conversacional baseada em chatbot
      (Figura <a href="#fig:enem" data-reference-type="ref"
      data-reference="fig:enem">2.6</a>).</p>
      <figure id="fig:enem">
      <p><img src="./fig/enem.png" style="width:70.0%" alt="image" />
      <span id="fig:enem" data-label="fig:enem"></span></p>
      <figcaption>Figure 6: Interface de chatbot para inscrição no ENEM
      2019.</figcaption>
      </figure>
      <p>O desenvolvimento de chatbots tem atraído investimentos de
      grandes corporações. Notavelmente, a IBM desenvolveu um sistema de
      resposta a perguntas em domínio aberto utilizando sua plataforma
      Watson <span class="citation" data-cites="Ferrucci2012">(Ferrucci
      2012)</span>. Esse tipo de tarefa representa um desafio
      computacional e de inteligência artificial (IA) considerável. Em
      2011, o sistema baseado em Watson demonstrou sua capacidade ao
      competir e vencer competidores humanos no programa de perguntas e
      respostas JEOPARDY! <span class="citation"
      data-cites="Ferrucci2012">(Ferrucci 2012)</span>.</p>
      <p>Diversos outros chatbots foram desenvolvidos para atender a
      demandas específicas em variados domínios. Exemplos incluem: BUTI,
      um companheiro virtual com computação afetiva para auxiliar na
      manutenção da saúde cardiovascular <span class="citation"
      data-cites="Junior2008">(Junior 2008)</span>; EduBot, um agente
      conversacional projetado para a criação e desenvolvimento de
      ontologias com lógica de descrição <span class="citation"
      data-cites="Lima2017">(Lima 2017)</span>; PMKLE, um ambiente
      inteligente de aprendizado focado na educação em gerenciamento de
      projetos <span class="citation" data-cites="Torreao2005">(Torreao
      2005)</span>; RENAN, um sistema de diálogo inteligente
      fundamentado em lógica de descrição <span class="citation"
      data-cites="AZEVEDO2015">(Azevedo 2015)</span>; e MOrFEu, voltado
      para a mediação de atividades cooperativas em ambientes
      inteligentes na Web <span class="citation"
      data-cites="Bada2012">(Bada 2012)</span>.</p>
      <p>Entre os chatbots baseados em LLMs de destaque atualmente estão
      o Qwen <a href="https://chat.qwen.ai/"
      class="uri">https://chat.qwen.ai/</a>, desenvolvido pela Alibaba,
      que se destaca por sua eficiência e suporte multilíngue; o
      DeepSeek <a href="https://chat.deepseek.com/"
      class="uri">https://chat.deepseek.com/</a>, de código aberto
      voltado para pesquisa e aplicações empresariais com foco em
      precisão e escalabilidade; o Maritaca <a
      href="https://chat.maritaca.ai/"
      class="uri">https://chat.maritaca.ai/</a>, modelo brasileiro
      otimizado para o português; o Gemini <a
      href="https://gemini.google.com/"
      class="uri">https://gemini.google.com/</a>, da Google, que integra
      capacidades multimodais e forte desempenho em tarefas diversas; o
      Mistral<a href="https://chat.mistral.ai/"
      class="uri">https://chat.mistral.ai/</a>, da Mistral AI, que
      utiliza arquitetura de mistura de especialistas para maior
      eficiência; o Llama <a href="https://www.meta.ai/"
      class="uri">https://www.meta.ai/</a>, da Meta, reconhecido por ser
      código aberto e ampla adoção na comunidade; o Claude <a
      href="https://claude.ai/" class="uri">https://claude.ai/</a>, da
      Anthropic, projetado com ênfase em segurança e alinhamento ético,
      que vem ganhando adeptos para tarefas e codificação; e o Nemotron
      <a href="https://build.nvidia.com/models"
      class="uri">https://build.nvidia.com/models</a>, da NVIDIA, que
      oferece modelos de linguagem otimizados para execução em GPUs e
      aplicações empresariais de alto desempenho. E o Groq <a
      href="https://grok.com/" class="uri">https://grok.com/</a> da
      xAI.</p>
      <h2 id="abordagens">Abordagens</h2>
      <p>Desde o pioneirismo do ELIZA, múltiplas abordagens e técnicas
      foram exploradas para o desenvolvimento de chatbots. Entre as mais
      relevantes, destacam-se: AIML com correspondência de padrões
      (pattern matching), análise sintática (Parsing), modelos de cadeia
      de Markov (Markov Chain Models), uso de ontologias, redes neurais
      recorrentes (RNNs), redes de memória de longo prazo (LSTMs),
      modelos neurais sequência-a-sequência (Sequence-to-Sequence),
      aprendizado adversarial para geração de diálogo, além de
      abordagens baseadas em recuperação (Retrieval-Based) e generativas
      (Generative-Based) <span class="citation"
      data-cites="borah2018survey Ramesh2019 Shaikh2016 Abdul-Kader2015 Li2018">(Borah
      et al. 2018; Ramesh et al. 2017; Shaikh et al. 2016; Abdul-Kader
      and Woods 2015; Li et al. 2018)</span>, entre outras.</p>
      <p>A seguir, uma lista resumida das tecnologias e marcos da
      criação dos chatbots:</p>
      <ul>
      <li><p>ELIZA: o primeiro chatbot, que utilizava correspondência de
      padrões simples para simular um psicoterapeuta. Foi um marco na
      história dos chatbots e influenciou o desenvolvimento de sistemas
      conversacionais subsequentes <span class="citation"
      data-cites="Weizenbaum1996">(Weizenbaum 1966)</span>.</p></li>
      <li><p>AIML: Artificial Intelligence Markup Language, uma
      linguagem de marcação baseada em XML que deu origem ao ALICE <span
      class="citation" data-cites="Wallace2000">(Richard S. Wallace
      2000)</span>. Essa linguagem de marcação permite a definição de
      regras de correspondência de padrões (pattern matching) para
      mapear entradas do usuário a respostas predefinidas.</p></li>
      <li><p>Transformers: arquitetura de rede neural baseada em
      atenção, que revolucionou o processamento de linguagem natural
      (NLP) <span class="citation"
      data-cites="vaswani2017attention">(Vaswani et al. 2017)</span>.
      Modelos como BERT e GPT são exemplos de arquiteturas baseadas em
      <em>Transformer</em> que têm sido amplamente utilizadas em
      chatbots modernos.</p></li>
      <li><p>GPT: Àdaptação do <em>Transformer</em> modelos de linguagem
      generativa, como o GPT-2, que utilizam redes neurais profundas
      para gerar texto coerente e relevante em resposta a entradas do
      usuário. Esses modelos são treinados em grandes quantidades de
      dados e podem ser adaptados para tarefas específicas, como
      atendimento ao cliente ou suporte técnico.</p></li>
      </ul>
      <p>Além disso, diversos frameworks têm sido desenvolvidos para
      facilitar a criação desses agentes complexos, como CrewAI <span
      class="citation" data-cites="crewai2025">(CrewAI 2025)</span> e
      Mangaba.AI <span class="citation"
      data-cites="mangabaAI2025">(Mangaba AI 2025)</span> e plataformas
      diversas como Hugging Face (e.g., <em>Transformers Agents</em>),
      que fornecem abstrações e ferramentas em Python para orquestrar
      múltiplos componentes e o uso de ferramentas externas.</p>
      <h2 id="chatbot-pensa">Chatbot pensa?</h2>
      <p>Apesar do progresso recente de chatbots, o mecanismo
      fundamental da inteligência em nível humano, frequentemente
      refletido na comunicação, ainda não está totalmente esclarecido
      <span class="citation" data-cites="Shum2018">(Shum, He, and Li
      2018)</span>. Para avançar na solução desses desafios, serão
      necessários progressos em diversas áreas da IA cognitiva, tais
      como: modelagem empática de conversas, modelagem de conhecimento e
      memória, inteligência de máquina interpretável e controlável, e
      calibração de recompensas emocionais <span class="citation"
      data-cites="Shum2018">(Shum, He, and Li 2018)</span>.</p>
      <p>Uma das dificuldades na construção de chatbots do tipo
      orientado a tarefas - a exemplo do Artificial Intelligence Markup
      Language (AIML) usado no ALICE - reside em gerenciar a
      complexidade das estruturas condicionais (“se-então”) que definem
      o fluxo do diálogo <span class="citation"
      data-cites="Raj2019">(Raj 2019)</span>. Quanto maior o número de
      decisões a serem tomadas, mais complexas tendem a ser essas
      estruturas condicionais. Contudo, elas são essenciais para
      codificar fluxos de conversação complexos. Se a tarefa que o
      chatbot visa simular é inerentemente complexa e envolve múltiplas
      condições, o código precisará refletir essa complexidade. Para
      facilitar a visualização desses fluxos, uma solução eficaz é a
      utilização de fluxogramas. Embora simples de criar e entender, os
      fluxogramas constituem uma ferramenta visual de representação para
      este problema.</p>
      <div class="mdframed">
      <p><span class="smallcaps">NOTA</span></p>
      <p>Uma explicação detalhada do AIML será apresentada no
      Capítulo <a href="#cap:ELIZA" data-reference-type="ref"
      data-reference="cap:ELIZA">3</a>.</p>
      </div>
      <p>Os chatbots baseados em AIML apresentam desvantagens
      específicas. Por exemplo, o conhecimento é representado como
      instâncias de arquivos AIML. Se esse conhecimento for criado com
      base em dados coletados da Internet, ele não será atualizado
      automaticamente, exigindo atualizações periódicas manuais <span
      class="citation" data-cites="Madhumitha2015">(Madhumitha,
      Keerthana, and Hemalatha 2015)</span>. No entanto, já existem
      abordagens para mitigar essa limitação, permitindo carregar
      conteúdo AIML a partir de fontes como arquivos XML <span
      class="citation" data-cites="Macedo2014">(Macedo and Fusco
      2014)</span>, um corpus textual <span class="citation"
      data-cites="DeGasperis2013">(De Gasperis, Chiari, and Florio
      2013)</span> ou dados do Twitter <span class="citation"
      data-cites="Yamaguchi2018">(Yamaguchi, Mozgovoy, and
      Danielewicz-Betz 2018)</span>. Além de abordagens no-code que
      geram o AIML a partir de fluxogramas <span class="citation"
      data-cites="neo2023BPMN">(G. S. Neo et al. 2023)</span>.</p>
      <p>Outra desvantagem do AIML, a exemplo do Eliza, reside na
      relativa complexidade de seus padrões de correspondência
      (patterns). Além disso, a manutenção do sistema pode ser árdua,
      pois, embora a inserção de conteúdo (categorias) seja
      conceitualmente simples, grandes volumes de informação
      frequentemente precisam ser adicionados manualmente <span
      class="citation" data-cites="Madhumitha2015">(Madhumitha,
      Keerthana, and Hemalatha 2015)</span>.</p>
      <p>Especificamente no caso do AIML, a construção e a visualização
      de fluxos de diálogo complexos enfrentam dificuldades adicionais.
      Devido ao seu formato baseado em texto, muitas vezes é difícil
      perceber claramente como as diferentes categorias (unidades de
      conhecimento e resposta) se interligam para formar a estrutura da
      conversação.</p>
      <p>O interesse pelos chatbots continua crescendo. No entanto, eles
      podem ser complicados para se construir e os usuários nem sempre
      têm experiência suficiente para configurá-los. Alguns usuários não
      têm necessariamente habilidades de programação ou de TI avançadas.
      Para que eles possam criar e personalizar os chatbots, é
      importante que a autoria seja fácil de usar e intuitiva. Ela não
      deve exigir conhecimento de linguagens de computador que sejam
      difíceis de entender para o público em geral. Para resolver esses
      problemas, várias ferramentas de autoria já foram propostas e
      podem ser utilizadas por usuários sem o uso de código, ferramentas
      no-code <span class="citation"
      data-cites="Neo2023Chatcreator">(Silva Neo et al.
      2023)</span>.</p>
      <p>Mesmo um chatbot baseado em LLM também não “pensa” no sentido
      cognitivo humano, mas opera a partir de um mecanismo estatístico
      de predição de tokens condicionado a um contexto. Sua forma de
      funcionamento é sustentada por arquiteturas neurais profundas,
      treinadas sobre grandes volumes de dados textuais, que capturam
      regularidades linguísticas, semânticas e pragmáticas. Assim, a
      resposta gerada resulta da estimativa da sequência de palavras
      mais provável dado um histórico de entrada, e não de um processo
      deliberativo consciente.</p>
      <p>A aparente coerência e racionalidade observadas nas respostas
      decorrem da capacidade do modelo em representar padrões complexos
      da linguagem natural em espaços vetoriais de alta dimensão. Esses
      modelos internalizam relações sintáticas, semânticas e
      discursivas, permitindo generalização para contextos não vistos
      durante o treinamento. Dessa forma, o chatbot consegue simular
      comportamentos como argumentação, explicação e síntese, ainda que
      tais comportamentos sejam emergentes do ajuste estatístico e não
      de compreensão genuína.</p>
      <p>A justificativa para esse modo de “pensar” reside na eficácia
      prática do paradigma. Ao invés de regras explícitas ou sistemas
      simbólicos rígidos, os LLMs oferecem flexibilidade, escalabilidade
      e adaptação a múltiplos domínios, o que os torna adequados para
      tarefas complexas de interação homem-máquina. A combinação de
      aprendizado profundo, atenção contextual e grandes corpora
      textuais permite ao chatbot responder de forma contextualizada,
      mantendo consistência local e global no discurso.</p>
      <p>Por fim, é fundamental reconhecer as limitações inerentes a
      esse modelo de funcionamento. A ausência de intencionalidade,
      consciência e compreensão semântica profunda implica que o chatbot
      pode produzir respostas plausíveis, porém factualmente incorretas
      ou logicamente inconsistentes. Assim, a forma de “pensar” de um
      chatbot baseado em LLM deve ser entendida como um mecanismo
      probabilístico avançado de geração de linguagem, útil e poderoso,
      mas distinto do raciocínio humano.</p>
      <h2 id="exercícios">Exercícios</h2>
      <ol>
      <li><p><strong>Qual é o objetivo principal de um
      chatbot?</strong></p>
      <ol>
      <li><p>Substituir completamente os seres humanos no atendimento ao
      cliente.</p></li>
      <li><p>Simular uma conversa humana para resolver problemas ou
      fornecer informações.</p></li>
      <li><p>Gerar textos literários complexos.</p></li>
      <li><p>Armazenar grandes quantidades de dados em tempo
      real.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes opções descreve corretamente um
      benefício dos chatbots?</strong></p>
      <ol>
      <li><p>Eles nunca precisam ser atualizados.</p></li>
      <li><p>Eles podem operar 24 horas por dia, 7 dias por semana, sem
      intervenção humana.</p></li>
      <li><p>Eles sempre tomam decisões melhores do que
      humanos.</p></li>
      <li><p>Eles substituem completamente a necessidade de suporte
      técnico.</p></li>
      </ol></li>
      <li><p><strong>Qual das opções a seguir é uma técnica comum usada
      por chatbots para entender o que o usuário está
      perguntando?</strong></p>
      <ol>
      <li><p>Mineração de Dados</p></li>
      <li><p>Tokenização</p></li>
      <li><p>Compressão de Dados</p></li>
      <li><p>Balanceamento de Carga</p></li>
      </ol></li>
      <li><p><strong>Qual é o papel dos embeddings de palavras em
      chatbots?</strong></p>
      <ol>
      <li><p>Converter palavras em vetores numéricos que capturam o
      significado semântico.</p></li>
      <li><p>Armazenar grandes quantidades de dados de
      conversação.</p></li>
      <li><p>Executar algoritmos de compressão de texto.</p></li>
      <li><p>Facilitar a tradução de texto entre diferentes
      idiomas.</p></li>
      </ol></li>
      <li><p><strong>Qual é a principal limitação dos chatbots baseados
      em regras?</strong></p>
      <ol>
      <li><p>Eles não conseguem operar em tempo real.</p></li>
      <li><p>Eles exigem grandes quantidades de dados para
      funcionar.</p></li>
      <li><p>Eles só podem responder a consultas específicas para as
      quais foram</p></li>
      <li><p>programados.</p></li>
      <li><p>Eles são incapazes de realizar tarefas
      repetitivas.</p></li>
      </ol></li>
      </ol>
      <h1 id="cap:ELIZA">ELIZA e AIML</h1>
      <div class="epigraph">
      <p>“Somos autómatos em três quartas partes das nossas ações.”</p>
      <p>Wilhelm Leibniz</p>
      </div>
      <div class="myboxobj">
      <p><span>Objetivo</span> Explorar o funcionamento do ELIZA, suas
      regras de transformação, implementação e legado histórico,
      evidenciando as bases dos sistemas conversacionais. Além de
      explicar a linguagem AIML, mostrando como ela possibilitou a
      construção de chatbots mais robustos e modulares.</p>
      </div>
      <h2 id="introdução-1">Introdução</h2>
      <p>O chatbot ELIZA foi um marco histórico e um dos primeiros
      programas de processamento de linguagem natural e foi apresentado
      em 1966 por Joseph Weizenbaum no MIT <span class="citation"
      data-cites="Weizenbaum1996">(Weizenbaum 1966)</span>.</p>
      <p>O conjunto de padrões e respostas predefinidas constitui o que
      Weizenbaum chamou de “roteiro” (ou script) de conversa. O
      mecanismo do ELIZA separa o motor de processamento dos dados,
      sendo o algoritmo de busca de palavras-chave e aplicação de
      regras, do script em si. Isso significa que ELIZA podia, em
      teoria, simular diferentes personalidades ou tópicos apenas
      carregando um script diferente, sem alterar o código do programa
      (veja na Figura <a href="#fig:elizaeditor"
      data-reference-type="ref"
      data-reference="fig:elizaeditor">3.1</a>). Também foi codificado
      um editor de texto para as alterações do próprio script.</p>
      <figure id="fig:elizaeditor">
      <p><img src="./fig/eliza.png" style="width:90.0%" alt="image" />
      <span id="fig:elizaeditor"
      data-label="fig:elizaeditor"></span></p>
      <figcaption>Figure 7: ELIZA: separação entre o código fonte, o
      script e o editor de texto.</figcaption>
      </figure>
      <p>Um destes roteiros, o script DOCTOR, deixou o ELIZA famoso. Com
      este script carregado, o ELIZA simulava precariamente um
      psicoterapeuta (do estilo Rogeriano), refletindo as palavras do
      usuário de volta em forma de pergunta.</p>
      <p>Na lista abaixo é apresentado um exemplo de diálogo (retirado
      do artigo original) entre um usuário humano e ELIZA, mostrando as
      respostas em letras maiúsculas. Nesse exemplo, o usuário faz
      afirmações pessoais (“<em>Men are all alike.</em>”, “<em>Well, my
      boyfriend made me come here.</em>”) e ELIZA responde com perguntas
      ou reformulações (<em>“IN WHAT WAY”</em>, “<em>YOUR BOYFRIEND MADE
      YOU COME HERE?</em>”) reutilizando termos do próprio usuário. Esse
      estilo reflexivo encoraja o usuário a continuar falando sem
      fornecer respostas diretas.</p>
      <pre data-caption="Exemplo de diálogo do ELIZA."><code>Men are all alike. 
IN WHAT WAY 
They&#39;re always bugging us about something or other. 
CAN YOU THINK OF A SPECIFIC EXAMPLE 
Well, my boyfriend made me come here. 
YOUR BOYFRIEND MADE YOU COME HERE 
He says I&#39;m depressed much of the time. 
I AM SORRY TO HEAR YOU ARE DEPRESSED 
It&#39;s true. I am unhappy. 
DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY 
I need some help, that much seems certain. 
WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP 
Perhaps I could learn to get along with my mother. 
TELL ME MORE ABOUT YOUR FAMILY 
My mother takes care of me. 
WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU 
My father. 
YOUR FATHER 
You are like my father in some ways. 
WHAT RESEMBLANCE DO YOU SEE 
You are not very aggressive but I think you don&#39;t want me to 
notice that. 
WHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE 
You don&#39;t argue with me. 
WHY DO YOU THINK I DON&#39;T ARGUE WITH YOU 
You are afraid of me. 
DOES IT PLEASE YOU TO BELIEVE I AM AFRAID OF YOU 
My father is afraid of everybody. 
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER 
Bullies. 
DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YOU COME HERE </code></pre>
      <h2 id="processamento">Processamento</h2>
      <p>O funcionamento do ELIZA baseia-se em correspondência de
      padrões por palavras-chave <span class="citation"
      data-cites="mittmann2021ElizaOZ">(Mittmann 2021)</span>. Técnica
      também chamada “casamento de padrão”, em inglês <em>pattner
      matching</em>, seu paradigma também se encaixa na técnica baseada
      em regras (ou <em>rule-based</em>). A cada turno do diálogo, o
      texto de entrada do usuário é analisado em busca de palavras-chave
      predefinidas. O ELIZA faz uma varredura da sentença da esquerda
      para a direita, identificando se alguma palavra correspondia a uma
      palavra-chave do script. Caso encontre uma palavra-chave no
      script, ELIZA empilha essa palavra até o final da frase. Depois,
      ele seleciona a palavra-chave mais “importante” até encontrar uma
      pontuação. Para isso, ele mantém uma lista de pesos associados a
      cada palavra-chave.</p>
      <p>Por exemplo, o script DOCTOR definia palavras-chave como
      “<em>ALIKE</em>” ou “<em>SAME</em>” com alta prioridade; assim, na
      frase “<em>Men are all alike</em>.” o programa detectava a palavra
      “<em>ALIKE</em>” e disparava uma resposta associada a ela (no
      caso: “<em>In what way?</em>”). Se múltiplas palavras-chave
      aparecessem, ELIZA escolhia aquela de maior peso para formular a
      resposta.</p>
      <p>Primeiro o texto de entrada digitado pelo usuário era separado
      em palavras, em uma técnica que hoje chamamos de tokenização de
      palavras, mas que ainda não existia na época. A palavra-chave era
      identificada, comparando-a sequencialmente até o fim das palavras
      existentes, ou até ser encontrada uma pontuação. Caso fosse
      encontrada uma pontuação (ponto final ou vírgula), o texto após a
      pontuação era ignorado se já tivesse sido identificada uma
      palavra-chave. Assim, cada processamento da resposta utiliza uma
      única frase do usuário. Se várias palavras-chave fossem
      encontradas antes da pontuação, a de maior peso era
      selecionada.</p>
      <p>Por exemplo, o usuário entra com o texto: “<em>I am sick. but,
      today is raining</em>”. Se houvesse uma palavra-chave no script
      ranqueando a palavra “<em>SICK</em>” com alta prioridade, a
      entrada processada seria somente “<em>I am sick</em>”, o restante
      depois da pontuação (neste caso, o ponto) seria ignorado pelo
      programa.</p>
      <p>Se nenhuma palavra-chave fosse encontrada na entrada, ELIZA
      recorria a frases genéricas programadas, chamadas de respostas
      vazias ou sem conteúdo. Nesses casos, o chatbot emitia mensagens
      do tipo “<em>I see</em>.” ou “<em>Please, go on</em>.”. Esse
      mecanismo evitava silêncio quando o usuário dizia algo fora do
      escopo do script.</p>
      <p>Além disso, a implementação original incluía uma estrutura de
      memória: algumas declarações recentes do usuário eram armazenadas
      e, se uma entrada subsequente não contivesse novas
      <em>keywords</em>, ELIZA poderia recuperar um tópico anterior e
      introduzi-lo na conversa. Por exemplo, se o usuário mencionasse
      família (em inglês <em>family</em>) em um momento e depois fizesse
      uma afirmação vaga, o programa poderia responder retomando o
      assunto da família (“<em>DOES THAT HAVE ANYTHING TO DO WITH YOUR
      FAMILY?</em>”). Essa estratégia dava uma pseudo-continuidade ao
      diálogo, simulando que o sistema “lembrava” de informações
      fornecidas anteriormente.</p>
      <h2 id="regras-de-transformação">Regras de Transformação</h2>
      <p>Encontrada a palavra-chave, ELIZA aplicava uma regra de
      transformação associada a ela para gerar a resposta. As regras são
      definidas em pares: um padrão de análise (<em>decomposition
      rule</em>) e um modelo de reconstrução de frase (<em>reassembly
      rule</em>).</p>
      <p>Primeiro, a frase do usuário é decomposta conforme um padrão
      que identifica a contexto mínimo em torno da palavra-chave. Essa
      decomposição frequentemente envolve separar a frase em partes e
      reconhecer pronomes ou estruturas gramaticais relevantes. Por
      exemplo, considere a entrada “<em>You are very helpful</em>.”. Uma
      regra de decomposição pode identificar a estrutura “<em>You are
      X</em>” — onde “<em>X</em>” representa o restante da frase — e
      extrair o complemento “<em>very helpful</em>” como um componente
      separado.</p>
      <p>Em seguida, a regra de reassembly correspondente é aplicada,
      remontando uma sentença de resposta em que “X” é inserido em um
      template pré-definido. No exemplo dado, o template de resposta
      poderia ser “<em>What makes you think I am X?</em>”; ao inserir X
      = “<em>very helpful</em>”, gera-se “<em>What makes you think I am
      very helpful?</em>”. Observe que há uma inversão de pessoa: o
      pronome “<em>you</em>” do usuário foi trocado por “<em>I</em>” na
      resposta do bot.</p>
      <p>De fato, uma parte importante das transformações do ELIZA
      envolve substituir pronomes (eu/você, meu/seu) para que a resposta
      faça sentido como uma frase do ponto de vista do computador
      falando com o usuário. Esse algoritmo de substituição é
      relativamente simples (por exemplo, “meu” → “seu”, “eu” → “você”,
      etc.), mas é essencial para dar a impressão de entendimento
      gramatical.</p>
      <p>Veja no código-fonte abaixo uma parte do conteúdo do arquivo
      DOCTOR adaptado. A primeira linha REMEMBER 5 estabelece que a
      prioridade da palavra REMEMBER é 5, caso o usuário entre com uma
      frase com a palavra REMEMBER, o ELIZA irá responder com uma das
      perguntas definidas nos parênteses, dentro do nível da palavra
      REMEMBER. A segunda lista dentro do REMEMBER são as regras de
      transformação de frases (chamadas de <em>decomposition rule</em> e
      <em>reassembly rule</em>) associadas à palavra-chave REMEMBER. O
      mesmo se repete para a palavra IF que tem um peso diferente de
      REMEMBER.</p>
      <pre
      data-caption="Trecho do roteiro DOCTOR do chatbot ELIZA. Adaptado de \citet{Weizenbaum1996}."><code>(REMEMBER 5 
(
        (0 YOU REMEMBER 0) 
        (DO YOU OFTEN THINK OF 4) 
        (DOES THINKING OF ~ BRING ANYTHING ELSE TO MINO) 
        (WHAT ELSE OO YOU REMEMBER)
        (WHY DO YOU REMEMBER 4 JUST NOW) 
        (WHAT IN THE PRESENT SITUATION REMINDS YOU OF 4) 
        (WHAT IS THE CONNECTION BETWEEN ME AND 4)
    ) 
    (
        (0 DO I REMEMBER 0) 
        (DID YOU THINK I WOULD FORGET 5) 
        (WHY DO YOU THINK I SHOULD RECALL 5 NOW) 
        (WHAT ABOUT 5) 
        (=WHAT) 
        (YOU MENTIONED S)
    ) 
) 
(IF 3 
    (
        (0 IF 0) 
        (DO YOU THINK ITS LIKELY THAT 3) 
        (DO YOU WISH THAT 3) 
        (WHAT DO YOU THINK ABOUT 3) 
        (REALLY, 2 3)
    )
)</code></pre>
      <h2 id="implementação-e-variações">Implementação e Variações</h2>
      <p>A implementação original de ELIZA foi feita em uma linguagem
      chamada MAD-SLIP (um dialeto de Lisp) rodando em um mainframe IBM
      7094 no sistema CTSS do MIT. O código-fonte do programa principal
      continha o mecanismo de correspondência, enquanto as regras de
      conversação (script DOCTOR) eram fornecidas separadamente em
      formato de listas associativas, similar a uma lista em Lisp.
      Infelizmente, Weizenbaum não publicou o código completo no artigo
      de 1966 (o que era comum na época), mas décadas depois o código em
      MAD-SLIP foi recuperado nos arquivos do MIT, comprovando os
      detalhes de implementação <span class="citation"
      data-cites="Lane2025">(Lane et al. 2025)</span>. De qualquer
      forma, a arquitetura descrita no artigo influenciou inúmeras
      reimplementações acadêmicas e didáticas nos anos seguintes.</p>
      <p>Diversos entusiastas e pesquisadores reescreveram ELIZA em
      outras linguagens de programação, dada a simplicidade relativa de
      seu algoritmo. Ao longo dos anos, surgiram versões em Lisp, PL/I,
      BASIC, Pascal, Prolog, Java, Python, OZ, JavaScript, entre muitas
      outras. Cada versão normalmente incluía o mesmo conjunto de regras
      do script terapeuta ou pequenas variações.</p>
      <p>As ideias de ELIZA também inspiraram chatbots mais avançados.
      Poucos anos depois, em 1972, surgiu PARRY, escrito pelo psiquiatra
      Kenneth Colby, que simulava um paciente paranoico. PARRY tinha um
      modelo interno de estado emocional e atitudes, mas na camada de
      linguagem ainda usava muitas respostas baseadas em regras,
      chegando a “conversar” com o próprio ELIZA em experimentos da
      época.</p>
      <p>Em 1995, Richard Wallace desenvolveu o chatbot ALICE
      (Artificial Linguistic Internet Computer Entity), que levava o
      paradigma de ELIZA a uma escala muito maior. ALICE utilizava um
      formato XML chamado AIML (Artificial Intelligence Markup Language)
      para definir milhares de categorias de padrões e respostas. Com
      mais de 16.000 templates mapeando entradas para saídas <span
      class="citation" data-cites="Wallace2000">(Richard S. Wallace
      2000)</span>, ALICE conseguia manter diálogos bem mais naturais e
      abrangentes que o ELIZA original, embora o princípio básico de
      correspondência de padrões permanecesse. Esse avanço rendeu a
      ALICE três vitórias no Prêmio Loebner (competição de chatbots) no
      início dos anos 2000 <span class="citation"
      data-cites="Wallace2000">(Richard S. Wallace 2000)</span>.</p>
      <p>Outras variações e sucessores notáveis incluem Jabberwacky
      (1988) – que já aprendia novas frases – e uma profusão de
      assistentes virtuais e bots de domínio específico nas décadas
      seguintes <span class="citation" data-cites="Wallace2000">(Richard
      S. Wallace 2000)</span>. Em suma, o legado de ELIZA perdurou por
      meio de inúmeros chatbots baseados em regras, até a transição para
      abordagens estatísticas e de aprendizado de máquina no final do
      século XX.</p>
      <h2 id="mecanismo-de-pesos">Mecanismo de Pesos</h2>
      <p>A técnica de ELIZA, baseada em palavras-chave com respostas
      predefinidas, contrasta fortemente com os métodos de Modelos de
      Linguagem Grande (LLMs) atuais, como o <em>Generative Pre-trained
      Transformer</em> (GPT), que utilizam redes neurais de milhões (ou
      trilhões) de parâmetros e mecanismos de atenção. Mais detalhes
      sobre LLM no Capítulo <a href="#cap:LLM" data-reference-type="ref"
      data-reference="cap:LLM">5</a>.</p>
      <p>No ELIZA, a “importância” de uma palavra era determinada
      manualmente pelo programador através de pesos ou rankings
      atribuídos a certas palavras-chave no script. Ou seja, o programa
      não aprendia quais termos focar – ele seguia uma lista fixa de
      gatilhos. Por exemplo, termos como “sempre” ou “igual” tinham
      prioridade alta no script DOCTOR para garantir respostas
      apropriadas.</p>
      <p>Em contraste, modelos modernos como o GPT não possuem uma lista
      fixa de palavras importantes; em vez disso, eles utilizam o
      mecanismo de <em>self-attention</em> para calcular dinamicamente
      pesos entre todas as palavras da entrada conforme o contexto <span
      class="citation" data-cites="vaswani2017attention">(Vaswani et al.
      2017)</span>.</p>
      <p>Na arquitetura <em>Transformer</em>, cada palavra (na prática
      não é uma palavra e sim um <em>token</em>) de entrada gera
      consultas e chaves que interagem com todas as outras, permitindo
      ao modelo atribuir pesos maiores às palavras mais relevantes
      daquela frase ou parágrafo <span class="citation"
      data-cites="vaswani2017attention">(Vaswani et al. 2017)</span>. Em
      outras palavras, o modelo aprende sozinho quais termos ou
      sequências devem receber mais atenção para produzir a próxima
      palavra na resposta. Esse mecanismo de atenção captura
      dependências de longo alcance e nuances contextuais que um sistema
      de palavras-chave fixas como o ELIZA não consegue representar.</p>
      <p>Além disso, o “vocabulário” efetivo de um LLM é imenso – um
      modelo GPT pode ser treinado com trilhões de palavras e ter
      ajustado seus parâmetros para modelar estatisticamente a linguagem
      humana <span class="citation"
      data-cites="vaswani2017attention">(Vaswani et al. 2017)</span>.
      Como resultado, pode-se dizer metaforicamente que os LLMs têm uma
      lista de “palavras-chave” milhões de vezes maior (na prática,
      distribuída em vetores contínuos) e um método bem mais sofisticado
      de calcular respostas do que o ELIZA.</p>
      <p>Enquanto ELIZA dependia de coincidências exatas de termos para
      disparar regras, modelos como GPT avaliam similaridades semânticas
      e contexto histórico graças às representações densas (embeddings)
      aprendidas durante o treinamento de rede neural.</p>
      <h2 id="geração-de-texto">Geração de Texto</h2>
      <p>Devido à sua abordagem baseada em regras locais, o ELIZA tinha
      capacidade de contextualização muito limitada. Cada input do
      usuário era tratado quase isoladamente: o programa não construía
      uma representação acumulada da conversa, além de artifícios
      simples como repetir algo mencionado (a estrutura de memória) ou
      usar pronomes para manter a ilusão de continuidade. Se o usuário
      mudasse de tópico abruptamente, o ELIZA não “perceberia” – ele
      apenas buscaria a próxima palavra-chave disponível ou recorreria a
      frases genéricas.</p>
      <p>Em contraste, modelos de linguagem modernos levam em conta um
      longo histórico de diálogo. Chatbots que usam GPT podem manter um
      contexto de centenas ou milhares de tokens (palavras ou
      fragmentos) em sua janela de atenção, o que significa que eles
      conseguem referenciar informações mencionadas vários parágrafos
      atrás e integrá-las na resposta corrente. O mecanismo de
      self-attention, em particular, permite que o modelo incorpore
      relações contextuais complexas: cada palavra gerada pode
      considerar influências de palavras distantes no texto de entrada
      <span class="citation" data-cites="vaswani2017attention">(Vaswani
      et al. 2017)</span>.</p>
      <p>Por exemplo, ao conversar com um LLM, se você mencionar no
      início da conversa que tem um irmão chamado Alex e depois
      perguntar “ele pode me ajudar com o problema?”, o modelo entenderá
      que “ele” se refere ao Alex mencionado anteriormente (desde que
      dentro da janela de contexto). Já o ELIZA original não teria como
      fazer essa ligação, a menos que houvesse uma regra explícita para
      “ele” e algum armazenamento específico do nome – algo impraticável
      de antecipar via regras fixas para todos os casos.</p>
      <p>Outra diferença está na geração de linguagem. O ELIZA não gera
      texto original no sentido pleno: suas respostas são em grande
      parte frases prontas (ou templates fixos) embaralhadas com partes
      da fala do usuário. Assim, seu vocabulário e estilo são limitados
      pelo script escrito manualmente. Modelos GPT, por sua vez, geram
      respostas novas combinando probabilisticamente o conhecimento
      adquirido de um extenso corpus. Eles não se restringem a repetir
      trechos da entrada, podendo elaborar explicações, fazer analogias,
      criar perguntas coerentes com os exemplos linguísticos em sua base
      de treinamento. Enquanto ELIZA tendia a responder com perguntas
      genéricas ou devolvendo as palavras do usuário, os LLMs podem
      produzir respostas informativas e detalhadas sobre o assunto (pois
      “aprenderam” uma ampla gama de tópicos durante o treinamento). Por
      exemplo, se perguntarmos algo factual ou complexo, o ELIZA
      falharia por não ter nenhuma regra a respeito, provavelmente dando
      uma resposta vazia. Já um modelo como GPT tentará formular uma
      resposta baseada em padrões linguísticos aprendidos e em
      conhecimento implícito dos dados, muitas vezes fornecendo detalhes
      relevantes.</p>
      <p>Em termos de fluência e variedade, os modelos modernos superam
      o ELIZA amplamente. O ELIZA frequentemente se repetia ou caía em
      loops verbais quando confrontado com inputs fora do roteiro – um
      limite claro de sistemas por regras estáticas. Os LLMs produzem
      linguagem muito mais natural e adaptável, a ponto de muitas vezes
      enganarem os usuários sobre estarem conversando com uma máquina
      (um efeito buscado desde o Teste de Turing). Ironicamente, ELIZA
      nos anos 60 já provocou um precursor desse fenômeno – o chamado
      <em>Efeito ELIZA</em>, em que pessoas atribuem compreensão ou
      sentimentos a respostas de computador que, na verdade, são
      superficiais. Hoje, em chatbots GPT, esse efeito se intensifica
      pela qualidade das respostas, mas a distinção fundamental
      permanece: ELIZA seguia scripts sem compreender, enquanto LLMs
      inferem padrões e significados de forma estatística, sem
      entendimento consciente, mas atingindo resultados que simulam
      compreensão de maneira muito mais convincente.</p>
      <p>Em resumo, os avanços de arquitetura (especialmente o mecanismo
      de atenção) ampliaram drasticamente a capacidade de
      contextualização e geração dos chatbots modernos, marcando uma
      evolução significativa desde o mecanismo simples, porém pioneiro,
      de ELIZA.</p>
      <h2 id="eliza-em-python">ELIZA em Python</h2>
      <p>A seguir o código-fonte de um programa que retorna o que o
      usuário digitou na linguagem de programação Python, inspirado no
      paradigma ELIZA.</p>
      <div class="sourceCode" id="cb4" data-language="Python"
      data-caption="Código-fonte de um programa  inspirado no ELIZA."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># método que processa a entrada do usuário</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> response(user_input):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&quot;Você disse: &quot;</span> <span class="op">+</span> user_input</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de uso</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>user_input <span class="op">=</span> <span class="st">&quot;Eu estou feliz&quot;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Você: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(user_input))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Eliza: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(response(user_input)))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>user_input <span class="op">=</span> <span class="st">&quot;Eu estou alegre&quot;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Você: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(user_input))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Eliza: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(response(user_input)))</span></code></pre></div>
      <pre><code>Você: Eu estou feliz
Eliza: Você disse: Eu estou feliz
Você: Eu estou alegre
Eliza: Você disse: Eu estou alegre</code></pre>
      <p>Este código acima simplesmente repete o que o usuário digita.
      Ele define a função <em>response</em>, que retorna a string
      <em>“Você disse: ”</em> concatenada ao texto recebido.</p>
      <p>Logo abaixo uma implementação um pouco mais robusta de um
      chatbot inspirado no paradigma ELIZA. Esta implementação demonstra
      a utilização de expressões regulares - mais sobre expressões
      regulares no Capítulo <a href="#cap:PLN" data-reference-type="ref"
      data-reference="cap:PLN">4</a> - para a identificação de padrões
      textuais (palavras-chave) na entrada fornecida pelo usuário e a
      subsequente geração de respostas, fundamentada em regras de
      transformação predefinidas manualmente.</p>
      <div class="sourceCode" id="cb6" data-language="Python"
      data-caption="Chatbot Eliza em Python."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re  </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random  </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>regras <span class="op">=</span> [</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="kw">(</span><span class="vs">hello</span><span class="cf">|</span><span class="vs">hi</span><span class="cf">|</span><span class="vs">hey</span><span class="kw">)</span><span class="dv">\b</span><span class="vs">&#39;</span>, re.IGNORECASE),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Hello. How do you do. Please tell me your problem.&quot;</span>]), <span class="co"># regra 1</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="kw">(</span><span class="vs">I am</span><span class="cf">|</span><span class="vs">I</span><span class="ch">\&#39;</span><span class="op">?</span><span class="vs">m</span><span class="kw">)</span><span class="vs"> </span><span class="kw">(</span><span class="dv">.</span><span class="op">+</span><span class="kw">)</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 2</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;How long have you been </span><span class="sc">{1}</span><span class="st">?&quot;</span>,   </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Why do you think you are </span><span class="sc">{1}</span><span class="st">?&quot;</span>]),</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="vs">I need </span><span class="kw">(</span><span class="dv">.</span><span class="op">+</span><span class="kw">)</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 3</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Why do you need </span><span class="sc">{1}</span><span class="st">?&quot;</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Would it really help you to get </span><span class="sc">{1}</span><span class="st">?&quot;</span>]),</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="vs">I can</span><span class="ch">\&#39;</span><span class="op">?</span><span class="vs">t </span><span class="kw">(</span><span class="dv">.</span><span class="op">+</span><span class="kw">)</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 4</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;What makes you think you can&#39;t </span><span class="sc">{1}</span><span class="st">?&quot;</span>,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Have you tried </span><span class="sc">{1}</span><span class="st">?&quot;</span>]),</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="vs">my </span><span class="kw">(</span><span class="vs">mother</span><span class="cf">|</span><span class="vs">father</span><span class="cf">|</span><span class="vs">mom</span><span class="cf">|</span><span class="vs">dad</span><span class="kw">)</span><span class="dv">\b</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 5</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Tell me more about your family.&quot;</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;How do you feel about your parents?&quot;</span>]),</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="kw">(</span><span class="vs">sorry</span><span class="kw">)</span><span class="dv">\b</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 6</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Please don&#39;t apologize.&quot;</span>]),</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="kw">(</span><span class="vs">maybe</span><span class="cf">|</span><span class="vs">perhaps</span><span class="kw">)</span><span class="dv">\b</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 7</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;You don&#39;t seem certain.&quot;</span>]),</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="vs">because</span><span class="dv">\b</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 8</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Is that the real reason?&quot;</span>]),</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="kw">(</span><span class="vs">are you</span><span class="cf">|</span><span class="vs">do you</span><span class="kw">)</span><span class="vs"> </span><span class="kw">(</span><span class="dv">.</span><span class="op">+</span><span class="kw">)</span><span class="ch">\?</span><span class="dv">$</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 9</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Why do you ask that?&quot;</span>]),</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    (re.<span class="bu">compile</span>(<span class="vs">r&#39;</span><span class="dv">\b</span><span class="vs">computer</span><span class="dv">\b</span><span class="vs">&#39;</span>, re.IGNORECASE), <span class="co"># regra 10</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>     [<span class="st">&quot;Do computers worry you?&quot;</span>]),</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>respostas_padrao <span class="op">=</span> [</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;I see.&quot;</span>,  </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Please tell me more.&quot;</span>,  </span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Can you elaborate on that?&quot;</span>  </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> response(entrada_usuario):</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> padrao, respostas <span class="kw">in</span> regras:</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        match <span class="op">=</span> padrao.search( entrada_usuario)  </span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> match:</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>            resposta <span class="op">=</span> random.choice(respostas)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> match.groups():</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>                resposta <span class="op">=</span> resposta.<span class="bu">format</span>( <span class="op">*</span>match.groups())</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> resposta</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random.choice(respostas_padrao)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;User: Hello.&quot;</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bot: &quot;</span> <span class="op">+</span> response(<span class="st">&quot;Hello.&quot;</span>))</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;User: I am feeling sad.&quot;</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bot: &quot;</span> <span class="op">+</span> response(<span class="st">&quot;I am feeling sad.&quot;</span>))</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Maybe I was not good enough.&quot;</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bot: &quot;</span> <span class="op">+</span> response(<span class="st">&quot;Maybe I was not good enough.&quot;</span>))</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;My mother tried to help.&quot;</span>)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bot: &quot;</span> <span class="op">+</span> response(<span class="st">&quot;My mother tried to help.&quot;</span>))</span></code></pre></div>
      <pre><code>User: Hello.
Bot: Hello. How do you do. Please tell me your problem.
User: I am feeling sad.
Bot: How long have you been feeling sad.?
Maybe I was not good enough.
Bot: You don&#39;t seem certain.
My mother tried to help.
Bot: How do you feel about your parents?</code></pre>
      <p>Na implementação anterior, são definidos múltiplos padrões de
      expressões regulares que correspondem a palavras-chave ou
      estruturas frasais de interesse (e.g., saudações, construções como
      “I am” ou “I need”, referências a termos familiares). A função
      <code>response</code>, ao receber uma string de entrada, itera
      sequencialmente sobre essas regras.</p>
      <p>Quando uma correspondência (<em>match</em>) é encontrada, uma
      resposta associada à regra é selecionada. É relevante notar que
      diversas respostas predefinidas contêm marcadores
      (<em>placeholders</em>), como <code>{1}</code>. Estes marcadores
      são dinamicamente substituídos pelo conteúdo dos grupos capturados
      pela expressão regular (mais detalhes sobre expressão regular no
      Capítulo <a href="#cap:PLN" data-reference-type="ref"
      data-reference="cap:PLN">4</a> na entrada original do usuário).
      Por exemplo, se a entrada for <em>“I am sad”</em> e esta
      corresponder a um padrão como <code>r“I am (.*)”</code>, o grupo
      capturado (<code>(.*)</code>) conterá a string <em>“sad”</em>. Se
      a resposta associada for <em>“Why do you think you are {1}?”</em>,
      o sistema a formatará como <em>“Why do you think you are
      sad?”</em>, substituindo <code>{1}</code> pelo conteúdo do
      primeiro grupo capturado. Na ausência de correspondência com
      qualquer regra específica, o sistema seleciona aleatoriamente uma
      resposta genérica a partir de um conjunto predefinido (denominado,
      por exemplo, <code>respostas_padrao</code>), tal como <em>“I
      see.”</em>.</p>
      <pre data-caption="Exemplo de Interação."><code>Usuário: Hello
Bot: Hello. How do you do. Please tell me your problem.
Usuário: I am feeling sad
Bot: Why do you think you are feeling sad?
Usuário: Because I lost my job
Bot: Is that the real reason?
Usuário: Maybe I was not good enough.
Bot: You don&#39;t seem certain.
Usuário: My mother tried to help.
Bot: Tell me more about your family.</code></pre>
      <p>Observa-se na interação que o chatbot inicialmente respondeu à
      saudação (<em>“Hello...”</em>) com base na regra 1. Em seguida, a
      entrada <em>“I am sad”</em> ativou a regra 2, resultando em uma
      resposta que incorpora o termo <em>“sad”</em>. A interação
      prossegue com o sistema respondendo a entradas iniciadas por
      <em>“Because...”</em> (regra 8) e <em>“Maybe...”</em> (regra 7),
      entre outras. Cada resposta gerada foi determinada pela regra
      correspondente ativada e pelo conteúdo específico da sentença do
      usuário, mimetizando o comportamento baseado em padrões do sistema
      ELIZA original <span class="citation"
      data-cites="Abdul-Kader2015">(Abdul-Kader and Woods
      2015)</span>.</p>
      <h2 id="eliza-com-python-e-gradio">ELIZA com Python e Gradio</h2>
      <p>Nesta seção, vamos explorar como criar um chatbot usando a
      biblioteca Gradio e Python.</p>
      <p>O Gradio é uma biblioteca de código aberto que permite criar
      interfaces de usuário para modelos de machine learning de forma
      rápida e fácil. Com Gradio, você pode criar aplicativos web
      interativos para visualizar e testar seus modelos, sem precisar de
      conhecimentos em frontend ou backend.</p>
      <p>Antes de começar, certifique-se de ter o Python e o pip
      instalados em sua máquina. Em seguida, instale a biblioteca Gradio
      usando o comando:</p>
      <pre data-caption="Instalação do Gradio."><code>pip install gradio</code></pre>
      <p>À seguir está um exemplo básico de como criar um chatbot com
      Gradio e Python:</p>
      <div class="sourceCode" id="cb10" data-language="Python"
      data-caption="Exemplo de código para criar um chatbot simples."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chatbot(message, history):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    resposta <span class="op">=</span> <span class="st">&quot;Olá! Eu sou um chatbot. Como posso ajudar você?&quot;</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> resposta</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.ChatInterface(</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>chatbot,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">&quot;Chatbot Simples&quot;</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code></pre></div>
      <p>Este código define uma função chatbot que processa o texto de
      entrada e retorna uma resposta. Em seguida, cria uma interface do
      chatbot usando a biblioteca Gradio, com um campo de texto para
      entrada e outro para saída.</p>
      <p>Execute o chatbot: Salve o arquivo e, no terminal, navegue até
      o diretório onde ele está salvo. Digite:</p>
      <pre data-caption="Instalação do Gradio."><code>python chatbot.py</code></pre>
      <p>Você verá um link local (algo como http://127.0.0.1:7860).
      Clique nele ou copie e cole no navegador. Uma interface simples
      aparecerá com um campo de texto. Experimente digitar “olá” ou
      “tchau” e veja as respostas. Veja na Figura <a
      href="#fig:chatsimplesgradio" data-reference-type="ref"
      data-reference="fig:chatsimplesgradio">3.2</a> um print da tela do
      chatbot já em execução.</p>
      <figure id="fig:chatsimplesgradio">
      <p><img src="fig/chatsimplesgradio.png" style="width:90.0%"
      alt="image" /> <span id="fig:chatsimplesgradio"
      data-label="fig:chatsimplesgradio"></span></p>
      <figcaption>Figure 8: Print do Chatbot.</figcaption>
      </figure>
      <p>Adicionando Lógica ao Chatbot: Agora que você tem uma interface
      básica, é hora de adicionar lógica ao chatbot. Você pode fazer
      isso adicionando condições e processamento de texto à função
      chatbot. Por exemplo:</p>
      <div class="sourceCode" id="cb12" data-language="Python"
      data-caption="Chatbot simples com Gradio."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install gradio</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chatbot(message, history):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">&quot;Olá&quot;</span> <span class="kw">in</span> message:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        resposta <span class="op">=</span> <span class="st">&quot;Olá! Eu sou um chatbot. Como posso ajudar você?&quot;</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="st">&quot;Quem é você?&quot;</span> <span class="kw">in</span> message:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        resposta <span class="op">=</span> <span class="st">&quot;Eu sou um chatbot criado para ajudar você com suas perguntas.&quot;</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        resposta <span class="op">=</span> <span class="st">&quot;Desculpe, não entendi sua pergunta. Pode tentar novamente?&quot;</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> resposta</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.ChatInterface(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>chatbot,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">&quot;Chatbot Simples&quot;</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code></pre></div>
      <p>Na Figura <a href="#fig:respaleatoria"
      data-reference-type="ref"
      data-reference="fig:respaleatoria">3.3</a> é possível visualizar o
      print da tela de um chatbot que responde de forma aleatória. Logo
      abaixo, o código-fonte que deu origem ao chatbot.</p>
      <div class="sourceCode" id="cb13" data-language="Python"
      data-caption="Chatbot simples com Gradio com respostas aleatórias."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install gradio</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chatbot(message, history):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    respostas <span class="op">=</span> [<span class="st">&quot;Interessante!&quot;</span>, <span class="st">&quot;Hmm, me conte mais!&quot;</span>, <span class="st">&quot;Não sei, mas vou fingir que sim!&quot;</span>]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">&quot;Olá&quot;</span> <span class="kw">in</span> message:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        resposta <span class="op">=</span> <span class="st">&quot;Olá! Eu sou um chatbot. Como posso ajudar você?&quot;</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="st">&quot;Quem é você?&quot;</span> <span class="kw">in</span> message:</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        resposta <span class="op">=</span> <span class="st">&quot;Eu sou um chatbot criado para ajudar você com suas perguntas.&quot;</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        resposta <span class="op">=</span> random.choice(respostas)        </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> resposta</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.ChatInterface(</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>chatbot,</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">&quot;Chatbot Simples&quot;</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code></pre></div>
      <figure id="fig:respaleatoria">
      <p><img src="fig/gradio2.png" style="width:90.0%" alt="image" />
      <span id="fig:respaleatoria"
      data-label="fig:respaleatoria"></span></p>
      <figcaption>Figure 9: Print do chatbot com respostas aleatórias e
      um pouco de lógica Conclusão.</figcaption>
      </figure>
      <p>Criar um chatbot com Gradio e Python é uma tarefa relativamente
      simples e rápida. Com essa biblioteca, você pode criar interfaces
      de usuário interativas para seus modelos de aprendizagem de
      máquina e criar chatbots para automação de tarefas ou suporte ao
      cliente. Este é apenas um exemplo básico, e você pode adicionar
      mais lógica e funcionalidades ao seu chatbot para torná-lo mais
      útil.</p>
      <h2 id="cap:AIML">Artificial Intelligence Markup Language
      (AIML)</h2>
      <p>Décadas depois do Eliza, as técnicas de programação avançaram,
      surgiu a web e as linguagens de marcação como o HTML ganharam
      tração. É neste cenário que é criado o Artificial Intelligence
      Markup Language (AIML), uma especificação baseada em XML, proposta
      por <span class="citation" data-cites="Wallace2009">Richard S.
      Wallace (2009)</span>, destinada à programação de chatbots. A
      concepção da linguagem prioriza o minimalismo, característica que
      simplifica o processo de criação de bases de conhecimento por
      indivíduos sem experiência prévia em programação <span
      class="citation" data-cites="Wallace2009">(Richard S. Wallace
      2009)</span>. A arquitetura fundamental de um interpretador AIML
      genérico é ilustrada na Figura <a href="#fig:interpretador"
      data-reference-type="ref"
      data-reference="fig:interpretador">3.4</a>.</p>
      <figure id="fig:interpretador">
      <p><img src="./fig/aimlbase.png" style="width:80.0%"
      alt="image" /> <span id="fig:interpretador"
      data-label="fig:interpretador"></span></p>
      <figcaption>Figure 10: Interpretador AIML arquitetura.<br />
      Adaptado de <span class="citation" data-cites="Silva2007">(A. C.
      da Silva and Costa 2007)</span></figcaption>
      </figure>
      <p>A técnica central empregada pelo AIML é a correspondência de
      padrões (<em>pattern matching</em>). Este método é amplamente
      utilizado no desenvolvimento de chatbots, particularmente em
      sistemas orientados a perguntas e respostas <span class="citation"
      data-cites="Abdul-Kader2015">(Abdul-Kader and Woods 2015)</span>.
      Uma das metas de projeto do AIML é possibilitar a fusão de bases
      de conhecimento de múltiplos chatbots especializados em domínios
      distintos. Teoricamente, um interpretador poderia agregar essas
      bases, eliminando automaticamente categorias redundantes para
      formar um <em>chatbot</em> mais abrangente <span class="citation"
      data-cites="Wallace2000">(Richard S. Wallace 2000)</span>.</p>
      <p>AIML é frequentemente associado aos chatbots de terceira
      geração <span class="citation" data-cites="Maria2010">(Maria et
      al. 2010)</span> e estima-se sua adoção em mais de 50.000
      implementações em diversos idiomas. Extensões da linguagem foram
      propostas, como o iAIML, que introduziu novas <em>tags</em> e
      incorporou o conceito de intenção com base nos princípios da
      Teoria da Análise da Conversação <span class="citation"
      data-cites="Neves2005">(Neves and Barros 2005)</span>.
      Adicionalmente, ferramentas baseadas na Web foram desenvolvidas
      para apoiar a construção de bases de conhecimento AIML <span
      class="citation" data-cites="Krassmann2017">(Krassmann et al.
      2017)</span>. Um exemplo proeminente é o <em>chatbot</em> ALICE,
      cuja implementação em AIML compreendia aproximadamente 16.000
      categorias, cada uma potencialmente contendo múltiplas
      <em>tags</em> XML aninhadas <span class="citation"
      data-cites="Wallace2000">(Richard S. Wallace 2000)</span>. Uma
      representação visual desta estrutura de conhecimento é apresentada
      na Figura <a href="#fig:representacaovisual"
      data-reference-type="ref"
      data-reference="fig:representacaovisual">3.5</a>.</p>
      <figure id="fig:representacaovisual">
      <p><img src="./fig/image9.png" style="width:90.0%" alt="image" />
      <span id="fig:representacaovisual"
      data-label="fig:representacaovisual"></span></p>
      <figcaption>Figure 11: Representação visual da base de
      conhecimento do chatbot ALICE.<br />
      Retirado de <span class="citation" data-cites="Wallace2003">(R.
      Wallace 2003)</span></figcaption>
      </figure>
      <p><span class="citation" data-cites="Wallace2000">Richard S.
      Wallace (2000)</span> estabeleceu analogias entre o funcionamento
      de interpretadores AIML e a teoria do Raciocínio Baseado em Casos
      (RBC). Nessa perspectiva, as categorias AIML funcionam como
      “casos”, onde o algoritmo identifica o padrão que melhor se alinha
      à entrada do usuário. Cada categoria estabelece um vínculo direto
      entre um padrão de estímulo e um modelo de resposta.
      Consequentemente, chatbots AIML inserem-se na tradição da robótica
      minimalista, reativa ou de estímulo-resposta <span
      class="citation" data-cites="Wallace2000">(Richard S. Wallace
      2000)</span>, conforme esquematizado na Figura <a
      href="#fig:teoriaestimulo" data-reference-type="ref"
      data-reference="fig:teoriaestimulo">3.6</a>. Vale notar que a
      própria técnica de RBC já foi integrada a interpretadores AIML
      como um mecanismo para consultar fontes de dados externas e
      expandir a base de conhecimento do agente <span class="citation"
      data-cites="Kraus2008">(Kraus and Fernandes 2008)</span>.</p>
      <figure id="fig:teoriaestimulo">
      <p><img src="./fig/image10.png" style="width:90.0%" alt="image" />
      <span id="fig:teoriaestimulo"
      data-label="fig:teoriaestimulo"></span></p>
      <figcaption>Figure 12: Teoria estímulo-resposta aplicada no
      AIML.<br />
      Retirado de <span class="citation" data-cites="Lima2017">(Lima
      2017)</span>.</figcaption>
      </figure>
      <p>Os chatbots que utilizam AIML são classificados como sistemas
      “baseados em recuperação” (retrieval-based). Tais modelos operam a
      partir de um repositório de respostas predefinidas, selecionando a
      mais apropriada com base na entrada do usuário e no contexto
      conversacional, guiando assim o fluxo da interação. Esta abordagem
      é frequentemente empregada na construção de chatbots destinados a
      operar em domínios de conhecimento restritos <span
      class="citation" data-cites="borah2018survey">(Borah et al.
      2018)</span>.</p>
      <p>O código-fonte à seguir, demonstra a estrutura elementar de um
      arquivo AIML. A <em>tag</em> <code>&lt;category&gt;</code>
      encapsula a unidade básica de conhecimento. Internamente, a
      <em>tag</em> <code>&lt;pattern&gt;</code> define o padrão de
      entrada a ser reconhecido (no exemplo, o caractere curinga
      <code>*</code>, que corresponde a qualquer entrada), enquanto a
      <em>tag</em> <code>&lt;template&gt;</code> contém a resposta
      associada. No exemplo ilustrado, o <em>chatbot</em> responderia
      “Hello!” a qualquer interação. Uma visão abstrata da árvore de
      conhecimento resultante pode ser observada logo abaixo. O AIML
      padrão suporta transições baseadas primariamente em
      correspondência de padrões, uma limitação inerente, embora
      extensões específicas de interpretadores possam permitir a
      integração de outras técnicas de processamento.</p>
      <pre
      data-caption="Exemplo de uma base de conhecimento em AIML. Adaptado de \citep{Wallace2000}."><code>&lt;aiml&gt;
&lt;category&gt;
    &lt;pattern&gt;*&lt;/pattern&gt;
    &lt;template&gt;Hello!&lt;/template&gt;
&lt;/category&gt;
&lt;/aiml&gt;</code></pre>
      <figure id="fig:representacaovisualabstrata">
      <p><img src="./fig/image12.png" style="width:70.0%" alt="image" />
      <span id="fig:representacaovisualabstrata"
      data-label="fig:representacaovisualabstrata"></span></p>
      <figcaption>Figure 13: Representação visual abstrata de uma base
      de conhecimento AIML.<br />
      Retirado de <a
      href="https://www.pandorabots.com/docs/aiml-fundamentals/"
      class="uri">https://www.pandorabots.com/docs/aiml-fundamentals/</a></figcaption>
      </figure>
      <p>O profissional responsável pela criação, manutenção e curadoria
      da base de conhecimento de um <em>chatbot</em> AIML é denominado
      <em>botmaster</em> <span class="citation"
      data-cites="Wallace2000">(Richard S. Wallace 2000)</span>. Suas
      atribuições englobam a edição da base (frequentemente via
      ferramentas auxiliares), a análise de logs de diálogo para
      identificar padrões de interação e a subsequente criação ou refino
      de respostas. Este papel pode ser exercido por indivíduos com
      diferentes perfis, incluindo <em>webmasters</em>, desenvolvedores,
      redatores, engenheiros ou outros interessados na construção de
      chatbots <span class="citation" data-cites="Wallace2000">(Richard
      S. Wallace 2000)</span>.</p>
      <p>Algumas implementações de interpretadores AIML podem incorporar
      capacidades rudimentares de compreensão semântica através do
      <em>Resource Description Framework</em> (RDF)<a href="#fn1"
      class="footnote-ref" id="fnref1"
      role="doc-noteref"><sup>1</sup></a>. O RDF é um padrão W3C para
      representação de informações na Web, usualmente por meio de
      triplas (sujeito-predicado-objeto) que descrevem relações entre
      entidades. No contexto AIML, RDF pode ser utilizado para armazenar
      e consultar fatos. Contudo, mesmo com tais adições, as capacidades
      linguísticas permanecem aquém da complexidade e do potencial
      gerativo da linguagem humana, conforme descrito por <span
      class="citation" data-cites="chomsky2002syntactic">Chomsky and
      Lightfoot (2002)</span>.</p>
      <p>Embora <span class="citation" data-cites="Hohn2019">Höhn
      (2019)</span> argumente que o AIML padrão carece de um conceito
      explícito de “intenção” (<em>intent</em>), similar ao encontrado
      em plataformas de <em>Natural Language Understanding</em> (NLU), é
      possível emular o reconhecimento de intenções. Isso é tipicamente
      alcançado definindo categorias que representam “formas canônicas”
      ou “padrões atômicos” para uma intenção específica<a href="#fn2"
      class="footnote-ref" id="fnref2"
      role="doc-noteref"><sup>2</sup></a>. Variações de entrada (e.g.,
      “oi”, “olá”) podem ser mapeadas para uma categoria canônica (e.g.,
      “saudação”) usando a <em>tag</em> <code>&lt;srai&gt;</code>
      (<em>Symbolic Reduction Artificial Intelligence</em>), que
      redireciona o fluxo de processamento (ver Figura <a
      href="#fig:sodatagsrai" data-reference-type="ref"
      data-reference="fig:sodatagsrai">3.8</a>). Dessa forma, um
      <em>chatbot</em> AIML pode gerenciar intenções distintas dentro de
      seu domínio, como realizar um pedido ou verificar o status de
      entrega.</p>
      <figure id="fig:sodatagsrai">
      <p><img src="./fig/image13.png" style="width:50.0%" alt="image" />
      <span id="fig:sodatagsrai"
      data-label="fig:sodatagsrai"></span></p>
      <figcaption>Figure 14: Uso da tag <code>&lt;srai&gt;</code>.<br />
      Retirado de <span class="citation" data-cites="DeGasperis2013">(De
      Gasperis, Chiari, and Florio 2013)</span></figcaption>
      </figure>
      <p>Os chatbots baseados em AIML têm obtido sucesso significativo
      em competições como o Prêmio Loebner. Notavelmente, o chatbot
      Mitsuku<a href="#fn3" class="footnote-ref" id="fnref3"
      role="doc-noteref"><sup>3</sup></a>, desenvolvido por Steve
      Worswick, conquistou múltiplos títulos <a href="#fn4"
      class="footnote-ref" id="fnref4"
      role="doc-noteref"><sup>4</sup></a>, seguindo vitórias anteriores
      do ALICE <span class="citation" data-cites="Wallace2000">(Richard
      S. Wallace 2000)</span>.</p>
      <p>Adicionalmente, Mitsuku foi classificado em primeiro lugar numa
      análise comparativa envolvendo oito chatbots <span
      class="citation" data-cites="Sharma2020">(Sharma, Verma, and Sahni
      2020)</span>. Nesse estudo, que avaliou atributos conversacionais
      com base em um conjunto padronizado de perguntas, o Google
      Assistant obteve a segunda posição, seguido pela Siri em terceiro.
      O <em>chatbot</em> ALICE alcançou a quarta posição, enquanto o
      ELIZA ficou na última colocação entre os sistemas comparados <span
      class="citation" data-cites="Sharma2020">(Sharma, Verma, and Sahni
      2020)</span>.</p>
      <h3 id="tags-do-aiml">Tags do AIML</h3>
      <p>Esta seção descreve as principais tags do AIML, versão 1.0.</p>
      <h4 class="unnumbered" id="aiml"><code>&lt;aiml&gt;</code></h4>
      <p>No contexto de AIML (Artificial Intelligence Markup Language),
      a tag &lt;aiml&gt; é usada para definir o início de um documento
      AIML que contém os padrões e respostas que um motor de chatbot
      deve usar. Ela envolve todo o documento, indicando que o conteúdo
      entre as tags &lt;aiml&gt; é escrito em AIML.</p>
      <pre
      data-caption="Tag raiz que engloba todo o conteúdo AIML."><code>&lt;aiml version=&quot;1.0&quot;&gt;
    &lt;!-- Categorias aqui --&gt;
&lt;/aiml&gt;</code></pre>
      <h4 class="unnumbered"
      id="category"><code>&lt;category&gt;</code></h4>
      <p><strong>Descrição:</strong> Unidade básica de conhecimento,
      contendo um padrão e uma resposta.</p>
      <pre
      data-caption="Unidade básica de conhecimento, contendo um padrão e uma resposta."><code>&lt;category&gt;
    &lt;pattern&gt;OLÁ&lt;/pattern&gt;
    &lt;template&gt;Oi! Como posso ajudar você hoje?&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="pattern"><code>&lt;pattern&gt;</code></h4>
      <p><strong>Descrição:</strong> Define o padrão de entrada do
      usuário, com curingas como <code>*</code> e <code>_</code>.</p>
      <pre
      data-caption="Define o padrão de entrada do usuário, com curingas."><code>&lt;category&gt;
    &lt;pattern&gt;EU GOSTO DE *&lt;/pattern&gt;
    &lt;template&gt;Que bom que você gosta de &lt;star/&gt;!&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="template"><code>&lt;template&gt;</code></h4>
      <p><strong>Descrição:</strong> Define a resposta do bot ao padrão
      correspondente.</p>
      <pre
      data-caption="Define a resposta do bot ao padrão correspondente."><code>&lt;category&gt;
    &lt;pattern&gt;QUAL É O SEU NOME&lt;/pattern&gt;
    &lt;template&gt;Meu nome é neo chatbot.&lt;/template&gt;
&lt;/category&gt; </code></pre>
      <h4 class="unnumbered" id="star"><code>&lt;star/&gt;</code></h4>
      <p><strong>Descrição:</strong> Captura o conteúdo do curinga
      <code>*</code> ou <code>_</code>.</p>
      <pre data-caption="Captura o conteúdo do curinga."><code>&lt;category&gt;
    &lt;pattern&gt;MEU NOME É *&lt;/pattern&gt;
    &lt;template&gt;Olá, &lt;star/&gt;!&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered" id="that"><code>&lt;that&gt;</code></h4>
      <p><strong>Descrição:</strong> Considera a última resposta do bot
      para decidir a próxima.</p>
      <pre
      data-caption="Considera a última resposta do bot para decidir a próxima."><code>&lt;category&gt;
    &lt;pattern&gt;SIM&lt;/pattern&gt;
    &lt;that&gt;Você gosta de programar?&lt;/that&gt;
    &lt;template&gt;Ótimo! Qual linguagem você prefere?&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered" id="topic"><code>&lt;topic&gt;</code></h4>
      <p><strong>Descrição:</strong> Define um contexto ou tópico para
      categorias.</p>
      <pre
      data-caption="Define um contexto ou tópico para categorias."><code>&lt;category&gt;
  &lt;pattern&gt;VAMOS FALAR SOBRE ESPORTE&lt;/pattern&gt;
  &lt;template&gt;Ok! &lt;topic name=&quot;esporte&quot;/&gt;&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="random-e-li"><code>&lt;random&gt;</code> e
      <code>&lt;li&gt;</code></h4>
      <p><strong>Descrição:</strong> Escolhe aleatoriamente uma resposta
      de uma lista.</p>
      <pre
      data-caption="Escolhe aleatoriamente uma resposta de uma lista."><code>&lt;category&gt;
  &lt;pattern&gt;COMO ESTÁ O TEMPO&lt;/pattern&gt;
  &lt;template&gt;
    &lt;random&gt;
      &lt;li&gt;Está ensolarado!&lt;/li&gt;
      &lt;li&gt;Está chovendo.&lt;/li&gt;
    &lt;/random&gt;
  &lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="condition"><code>&lt;condition&gt;</code></h4>
      <p><strong>Descrição:</strong> Adiciona lógica condicional baseada
      em variáveis.</p>
      <pre
      data-caption="Adiciona lógica condicional baseada em variáveis."><code>&lt;category&gt;
  &lt;pattern&gt;COMO EU ESTOU&lt;/pattern&gt;
  &lt;template&gt;
    &lt;condition name=&quot;humor&quot;&gt;
      &lt;li value=&quot;feliz&quot;&gt;Você está bem!&lt;/li&gt;
      &lt;li&gt;Não sei ainda!&lt;/li&gt;
    &lt;/condition&gt;
  &lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered" id="set-e-get"><code>&lt;set&gt;</code> e
      <code>&lt;get&gt;</code></h4>
      <p><strong>Descrição:</strong> Define e recupera variáveis.</p>
      <pre data-caption="Define e recupera variáveis."><code>&lt;category&gt;
    &lt;pattern&gt;MEU NOME É *&lt;/pattern&gt;
    &lt;template&gt;
        &lt;set name=&quot;nome&quot;&gt;&lt;star/&gt;&lt;/set&gt;Olá, &lt;get name=&quot;nome&quot;/&gt;!
    &lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered" id="srai"><code>&lt;srai&gt;</code></h4>
      <p><strong>Descrição:</strong> Redireciona a entrada para outro
      padrão.</p>
      <pre
      data-caption="Redireciona a entrada para outro padrão."><code>&lt;category&gt;
    &lt;pattern&gt;OI&lt;/pattern&gt;
    &lt;template&gt;&lt;srai&gt;OLÁ&lt;/srai&gt;&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered" id="think"><code>&lt;think&gt;</code></h4>
      <p><strong>Descrição:</strong> Executa ações sem exibir o
      conteúdo.</p>
      <pre data-caption="Executa ações sem exibir o conteúdo."><code>&lt;category&gt;
    &lt;pattern&gt;EU SOU TRISTE&lt;/pattern&gt;
    &lt;template&gt;
    &lt;think&gt;&lt;set name=&quot;humor&quot;&gt;triste&lt;/set&gt;&lt;/think&gt;Sinto muito!
    &lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="person-person2-gender"><code>&lt;person&gt;</code>,
      <code>&lt;person2&gt;</code>, <code>&lt;gender&gt;</code></h4>
      <p><strong>Descrição:</strong> Transforma pronomes ou ajusta
      gênero.</p>
      <pre data-caption="Transforma pronomes ou ajusta gênero."><code>&lt;category&gt;
    &lt;pattern&gt;EU TE AMO&lt;/pattern&gt;
    &lt;template&gt;&lt;person&gt;&lt;star/&gt;&lt;/person&gt; ama você também!&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="formal-uppercase-lowercase"><code>&lt;formal&gt;</code>,
      <code>&lt;uppercase&gt;</code>,
      <code>&lt;lowercase&gt;</code></h4>
      <p><strong>Descrição:</strong> Formata texto (capitaliza,
      maiúsculas, minúsculas).</p>
      <pre
      data-caption="Formata texto (capitaliza, maiúsculas, minúsculas)."><code>&lt;category&gt;
  &lt;pattern&gt;MEU NOME É joão&lt;/pattern&gt;
  &lt;template&gt;Olá, &lt;formal&gt;&lt;star/&gt;&lt;/formal&gt;!&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h4 class="unnumbered"
      id="sentence"><code>&lt;sentence&gt;</code></h4>
      <p><strong>Descrição:</strong> Formata como frase (primeira letra
      maiúscula, ponto final).</p>
      <pre
      data-caption="Formata como frase (primeira letra maiúscula, ponto final)."><code>&lt;category&gt;
  &lt;pattern&gt;oi&lt;/pattern&gt;
  &lt;template&gt;&lt;sentence&gt;&lt;star/&gt;&lt;/sentence&gt;&lt;/template&gt;
&lt;/category&gt;</code></pre>
      <h3 id="exemplo-em-python">Exemplo em Python</h3>
      <p>A seguir um exemplo do uso de um interpretador AIML em Python.
      O arquivo “cerebro.aiml” deve existir anteriormente. Use uma
      versão compatível com a biblioteca aiml que é somente compatível
      com versões antigas do Python, do Python 3.6 para trás, ou seja,
      não funciona no Python 3.12.</p>
      <div class="sourceCode" id="cb30" data-language="Python"
      data-caption="Um chatbot de terminal que responde a algumas palavras-chave pré-definidas."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install aiml</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> aiml</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Criar kernel (núcleo do bot)</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> aiml.Kernel()</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>cerebro_aiml_text <span class="op">=</span> <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;aiml version=&quot;1.0.1&quot; encoding=&quot;UTF-8&quot;&gt;</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;category&gt;</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="st">        &lt;pattern&gt;OI&lt;/pattern&gt;</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="st">        &lt;template&gt;Olá! Como posso ajudar você?&lt;/template&gt;</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;/category&gt;</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;category&gt;</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="st">        &lt;pattern&gt;OBRIGADO&lt;/pattern&gt;</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="st">        &lt;template&gt;De nada!&lt;/template&gt;</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;/category&gt;</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/aiml&gt;</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;&quot;&quot;</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Salvar o conteúdo AIML em um arquivo</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;cerebro.aiml&quot;</span>, <span class="st">&quot;w&quot;</span>, encoding<span class="op">=</span><span class="st">&quot;utf-8&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    f.write(cerebro_aiml_text)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o arquivo AIML</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>kernel.learn(<span class="st">&quot;cerebro.aiml&quot;</span>)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop de conversa</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    user_input <span class="op">=</span> <span class="bu">input</span>(<span class="st">&quot;Você: &quot;</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> user_input.lower() <span class="kw">in</span> [<span class="st">&quot;sair&quot;</span>, <span class="st">&quot;exit&quot;</span>, <span class="st">&quot;quit&quot;</span>]:</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> kernel.respond(user_input)</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Bot:&quot;</span>, response)</span></code></pre></div>
      <pre><code>Você: oi
Bot: Olá! Como posso ajudar você?</code></pre>
      <p>O arquivo std-startup.xml é um ponto de partida e geralmente
      carrega outros arquivos .aiml.</p>
      <div class="sourceCode" id="cb32" data-language="XML"
      data-caption="Estrutura básica de um arquivo cerebro.aiml."><pre
      class="sourceCode xml"><code class="sourceCode xml"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>&lt;<span class="kw">aiml</span><span class="ot"> version=</span><span class="st">&quot;1.0.1&quot;</span><span class="ot"> encoding=</span><span class="st">&quot;UTF-8&quot;</span>&gt;</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  &lt;<span class="kw">category</span>&gt;</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">pattern</span>&gt;OI&lt;/<span class="kw">pattern</span>&gt;</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">template</span>&gt;Olá! Como posso te ajudar?&lt;/<span class="kw">template</span>&gt;</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  &lt;/<span class="kw">category</span>&gt;</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>  &lt;<span class="kw">category</span>&gt;</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">pattern</span>&gt;QUAL SEU NOME&lt;/<span class="kw">pattern</span>&gt;</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">template</span>&gt;Eu sou um chatbot em AIML.&lt;/<span class="kw">template</span>&gt;</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>  &lt;/<span class="kw">category</span>&gt;</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>&lt;/<span class="kw">aiml</span>&gt;</span></code></pre></div>
      <h2 id="exercícios-1">Exercícios</h2>
      <ol>
      <li><p><strong>Assinale a alternativa que melhor descreve a
      principal diferença entre um chatbot conversacional e um chatbot
      orientado a tarefas:</strong></p>
      <ol>
      <li><p>O chatbot conversacional utiliza apenas regras, enquanto o
      orientado a tarefas utiliza apenas redes neurais.</p></li>
      <li><p>O chatbot conversacional busca manter diálogo aberto,
      enquanto o orientado a tarefas segue fluxos específicos para
      executar ações.</p></li>
      <li><p>Chatbots orientados a tarefas não podem utilizar linguagem
      natural.</p></li>
      <li><p>Chatbots conversacionais não podem responder perguntas
      sobre assuntos gerais.</p></li>
      </ol></li>
      <li><p><strong>Sobre o ELIZA, marque a alternativa
      correta:</strong></p>
      <ol>
      <li><p>Utilizava redes neurais profundas e mecanismos de
      autoatenção.</p></li>
      <li><p>Interpretava semanticamente as frases do usuário com
      compreensão profunda.</p></li>
      <li><p>Baseava-se em regras simples e padrões para reformular
      entradas do usuário.</p></li>
      <li><p>Possuía memória de longo prazo para manter contexto amplo
      de conversa.</p></li>
      </ol></li>
      <li><p><strong>A arquitetura <em>Transformers</em>, introduzida em
      2017, é caracterizada principalmente por:</strong></p>
      <ol>
      <li><p>Utilizar sequências fixas de regras
      pré-programadas.</p></li>
      <li><p>Empregar mecanismos de autoatenção para avaliar relações
      entre palavras.</p></li>
      <li><p>Substituir completamente qualquer forma de treinamento com
      dados.</p></li>
      <li><p>Impedir o uso de modelos generativos em tarefas de
      diálogo.</p></li>
      </ol></li>
      <li><p><strong>Qual a definição de Artificial Intelligence Markup
      Language (ou AIML):</strong></p>
      <ol>
      <li><p>Uma linguagem de marcação baseada em XML para construção de
      chatbots por correspondência de padrões.</p></li>
      <li><p>Um algoritmo estatístico baseado em cadeias de
      Markov.</p></li>
      <li><p>Uma técnica de aprendizado profundo utilizada em modelos
      generativos.</p></li>
      <li><p>Uma ferramenta exclusiva para chatbots
      multimodais.</p></li>
      </ol></li>
      <li><p><strong>De acordo com a discussão sobre agentes, qual das
      alternativas caracteriza corretamente um “agente” no contexto
      apresentado?</strong></p>
      <ol>
      <li><p>Um programa capaz apenas de gerar texto sem interagir com o
      ambiente.</p></li>
      <li><p>Uma entidade computacional que percebe seu ambiente e atua
      por meio de sensores e atuadores.</p></li>
      <li><p>Um sistema que realiza exclusivamente tarefas bancárias
      automatizadas.</p></li>
      <li><p>Qualquer modelo de linguagem de grande porte,
      independentemente de suas capacidades.</p></li>
      </ol></li>
      <li><p><strong>Explique em que sentido um chatbot pode ser
      entendido como uma instanciação específica de um agente. Em sua
      resposta, (a) apresente uma definição de chatbot, (b) apresente
      uma definição de agente e (c) discuta as diferenças e
      sobreposições entre esses conceitos, incluindo a distinção entre
      sistemas que apenas conversam e sistemas que também utilizam
      ferramentas externas (como navegação na Web, execução de código e
      interação com APIs).</strong></p></li>
      <li><p><strong>Compare os chatbots ELIZA e ChatGPT considerando
      pelo menos quatro aspectos: (a) período histórico e contexto de
      surgimento, (b) técnica ou abordagem principal utilizada para
      gerar respostas, (c) tipo de tarefa ou objetivo conversacional
      para o qual foram concebidos e (d) limitações em termos de
      compreensão de linguagem, contexto e emoções. Finalize discutindo
      por que ambos podem ser classificados como chatbots
      conversacionais.</strong></p></li>
      <li><p><strong>Diferencie chatbots conversacionais e chatbots
      orientados a tarefas, ilustrando sua resposta com exemplos. Em
      seguida, analise o papel do <em>fluxo conversacional</em> nesses
      sistemas: descreva como fluxos de diálogo (por exemplo, árvores de
      decisão e fluxos de perguntas e respostas) são empregados em
      chatbots orientados a tarefas, e comente sobre como esses fluxos
      se relacionam com conceitos da Análise da Conversação, como
      tópicos conversacionais e coerência entre
      turnos.</strong></p></li>
      <li><p><strong>Apresente e discuta a evolução das abordagens
      tecnológicas para a construção de chatbots, partindo de sistemas
      baseados em correspondência de padrões (como ELIZA e AIML/ALICE)
      até modelos neurais modernos baseados em <em>Transformers</em> e
      LLMs (como BERT, GPT e outros modelos recentes listados no
      capítulo). Em sua resposta, destaque: (a) a ideia central de cada
      abordagem, (b) vantagens e desvantagens principais e (c) impactos
      dessa evolução na qualidade das interações com os
      usuários.</strong></p></li>
      <li><p><strong>Desenvolva um texto em que você (a) explique por
      que um chatbot, mesmo avançado, não pode ser considerado pensante
      nos mesmos termos que um humano, (b) descreva problemas práticos
      de construção e manutenção de chatbots (por exemplo, complexidade
      de fluxos condicionais, atualização de conhecimento, dificuldade
      de visualização de diálogos complexos) e (c) discuta o papel de
      ferramentas de autoria <em>no-code</em> na tentativa de tornar o
      desenvolvimento de chatbots mais acessível a usuários sem formação
      em programação.</strong></p></li>
      </ol>
      <h1 id="cap:PLN">Processamento de Linguagem Natural (PLN)</h1>
      <div class="epigraph">
      <p>“Os ignorantes afirmam, os sábios duvidam, os sensatos
      refletem.”</p>
      <p>Aristóteles</p>
      </div>
      <div class="myboxobj">
      <p><span>Objetivo</span> Introduzir técnicas essenciais de PLN,
      como tokenização, lematização, POS tagging e NER, para capacitar o
      leitor a processar e analisar linguagem natural em projetos de
      chatbot; demonstrar o uso de expressões regulares; apresentar
      métodos de representação de texto, como Bag-of-Words, TF-IDF e
      embeddings; além disso, descrever como configurar o Python para
      executar os códigos.</p>
      </div>
      <h2 id="inteligência-artificial">Inteligência Artificial</h2>
      <p>A inteligência artificial (IA) é a força tecnológica mais
      transformadora do século XXI <span class="citation"
      data-cites="ribeiro2025eraIA">(Ribeiro 2025)</span>. Mas o que é
      IA? As definições de IA dependem do contexto e isso pode trazer
      confusão no entendimento e delimitação do tema. Menos abrangente,
      porém mais confuso ainda, é o termo “inteligência artificial”.
      Portanto, dado as diversas definições de inteligência artificial
      (IA), ou <em>artificial inteligence</em> em inglês, delimitaremos
      um pouco o escopo da inteligência em questão.</p>
      <p>A IA aparece em nossa cultura de diversas formas, tais como, o
      HAL 9000 do filme “2001: uma Odisseia no Espaço”, clássico de
      Stanley Kubrick, ou como a IA do filme “Ela”, com o ator Joaquin
      Phoenix, onde um humano se apaixona por um sistema
      operacional.</p>
      <p>Espero que você leitor seja um membro da espécie Homo-Sapiens.
      O termo “Homo-Sapiens” vem do latim e significa homem sábio <span
      class="citation" data-cites="wikipediahumano">(Wikipedia
      2024a)</span>. A importância da sapiência (sinônimo de
      inteligência) é tamanha que define a nossa espécie. Porém, neste
      contexto, consideramos que um gato ou cachorro também é dotado de
      inteligência. Uma abelha é praticamente uma cientista <span
      class="citation" data-cites="wikipediaabelhas">(Wikipedia
      2024b)</span>. Portanto, seremos mais contidos e reservados quanto
      ao significado do termo inteligência.</p>
      <p>O que confunde é que inteligência e artificial são palavras que
      têm significado implícito para pessoas que não são da área de
      computação. Naturalmente, médicos, advogados, engenheiros (só para
      citar alguns) querem verificar como a “inteligência artificial”
      pode ser inserida na sua rotina diária. Meu dentista já quis saber
      como a IA iria afetar seus procedimentos odontológicos. Porém, ele
      nunca me perguntou em como a “Transformada de Fourier” poderia
      melhorar o seu dia-a-dia, mesmo sabendo que a transformada já é
      utilizada em vários domínios do conhecimento e com entusiasmo
      <span class="citation" data-cites="wikipediafourier">(Wikipedia
      2024c)</span>.</p>
      <p>A inteligência artificial da computação está mais relacionada
      com a capacidade de realizar coisas que seres inteligentes (tais
      como um gato, um bebê, uma abelha ou um humano) realizam, como,
      por exemplo, puxar a mão (ou pata) instantaneamente ao tocar em
      uma superfície quente, realizar uma prova objetiva de anatomia ou
      elaborar um recurso para a anulação de uma questão de concurso. Se
      um programa realiza uma ação geralmente realizada por uma entidade
      dotada de inteligência, ele pode ser encarado como um programa que
      simula uma inteligência artificial. Convenhamos que praticamente
      qualquer coisa cabe neste conceito.</p>
      <p>Sobre este tema, o livro de Russell e Norvig (um dos livros
      mais lidos em todas as universidades do mundo sobre o tema) tem
      uma boa definição sobre o tema: “O campo da inteligência
      artificial [...] tenta não apenas compreender, mas também
      construir entidades inteligentes” (tradução nossa) <span
      class="citation" data-cites="Russel2013">(Russel and Norving
      2013)</span>. Em outras palavras, a inteligência artificial da
      ciência da computação tem o audacioso objetivo de construir
      agentes dotados de inteligência.</p>
      <p>A origem do termo “inteligência artificial”, na ciência da
      computação, é geralmente atribuída a John McCarthy, professor de
      Matemática da Universidade Dartmouth College <span
      class="citation" data-cites="blipblog">(blipblog 2024)</span>
      (Figura <a href="#fig:jhonalan" data-reference-type="ref"
      data-reference="fig:jhonalan">4.3</a>). Ele organizou uma
      conferência com duração de oito semanas com outros colegas em
      1956, alguns anos após a Segunda Guerra, e desde então o termo vem
      sendo utilizado para designar parte de conteúdos estudados em
      ciência da computação.</p>
      <p>Um pouco antes, o artigo seminal de Alan Turing, com quem John
      McCarthy trabalhou em conjunto, já apresentava reflexões sobre a
      inteligência que uma máquina poderia possuir <span
      class="citation" data-cites="Turing1950">(Turing 1950)</span>. No
      entanto, a inteligência artificial aparece na literatura há
      milhares de anos; um exemplo é o Gigante Talos de Creta, um
      autômato proveniente da mitologia grega <span class="citation"
      data-cites="pickover2021artificial">(Pickover 2021)</span>.</p>
      <figure id="fig:jhonalan">
      <figure id="fig:jhon">
      <p><img src="fig/jhon_macharthy.png" style="width:50.0%"
      alt="image" /> <span id="fig:jhon"
      data-label="fig:jhon"></span></p>
      <figcaption>Jhon MacCarthy.</figcaption>
      </figure>
      <figure id="fig:alan">
      <p><img src="fig/alan_turing.png" style="width:50.0%"
      alt="image" /> <span id="fig:alan"
      data-label="fig:alan"></span></p>
      <figcaption>Alan Turing.</figcaption>
      </figure>
      <figcaption>Figure 15: Jhon Maccarthy e Alan Turing. Imagens da
      internet.</figcaption>
      </figure>
      <p>Foi na década de 1970 que o uso da inteligência artificial
      começou a ser mais difundido. Uma das primeiras abordagens com
      relativo sucesso foi os Sistemas Especialistas (SE). Eles
      dependiam dos especialistas do domínio para transformar o
      conhecimento tácito (baseado em sua experiência) em explícito
      (formalizado, documentado), que era então codificado na forma de
      regras em lógica formal. O processo de aquisição desse
      conhecimento acabou sendo um grande obstáculo na adoção em massa
      dessa abordagem. Veja um exemplo de software que implementa um
      motor de inferência baseado na teoria dos SE na Figura <a
      href="#fig:expert" data-reference-type="ref"
      data-reference="fig:expert">4.4</a>.</p>
      <figure id="fig:expert">
      <p><img src="fig/expert.jpg" style="width:60.0%" alt="image" />
      <span id="fig:expert" data-label="fig:expert"></span></p>
      <figcaption>Figure 16: Interface de um Sistema Especialista
      ExpertSinta.</figcaption>
      </figure>
      <p>A superação de algumas limitações (tais como o aumento da
      capacidade de processamento e armazenamento dos computadores, a
      geração de grandes volumes de dados, novidades científicas e
      tecnológicas, chips supercondutores e a eficiência energética)
      permitiu o avanço de outras técnicas. Uma das técnicas que têm
      ganho notoriedade, por causa desses avanços, é o Aprendizado de
      Máquina.</p>
      <h2 id="aprendizado-de-máquina">Aprendizado de Máquina</h2>
      <p>O Aprendizado de Máquina (AM) é uma subárea da IA motivada pelo
      desenvolvimento de softwares mais independentes da intervenção
      humana para extração do conhecimento, o que era uma dificuldade
      nos Sistemas Especialistas. Geralmente, aplicações de AM utilizam
      indução para buscar modelos capazes de representar o conhecimento
      existente nos dados.</p>
      <p>Na Figura <a href="#fig:exemplosam" data-reference-type="ref"
      data-reference="fig:exemplosam">4.5</a>, é possível identificar
      alguns usos de AM integrado em algumas atividades cotidianas. São
      elas: (a) um smartphone com um assistente de voz fornecendo
      atualizações meteorológicas; (b) um sistema de casa inteligente
      ajustando o termostato com base nas preferências do usuário; (c)
      um carro autônomo dirigindo em uma rua movimentada da cidade; (d)
      uma plataforma de compras online recomendando produtos a um
      usuário com base em suas compras anteriores. Essa figura foi
      criada inclusive com uma inteligência artificial chamada DALL·E 3,
      disponível no ChatGPT. ChatGPT é um chatbot que ganhou
      notoriedade, sendo um dos aplicativos que mais ganhou usuários
      rapidamente no mundo.</p>
      <figure id="fig:exemplosam">
      <p><img src="fig/exemplos_am.png" style="width:60.0%"
      alt="image" /> <span id="fig:exemplosam"
      data-label="fig:exemplosam"></span></p>
      <figcaption>Figure 17: Exemplos AM.</figcaption>
      </figure>
      <p>As tarefas de aprendizado de máquina podem ser divididas entre
      tarefas <strong>preditivas</strong> e
      <strong>descritivas</strong>. As tarefas de aprendizado preditivas
      visam inferir o atributo alvo de uma nova entrada a partir da
      exposição prévia aos dados durante o treinamento do modelo. As
      tarefas descritivas buscam extrair padrões e correlações; além
      disso, não existe esta distinção entre atributos alvo e
      preditivos.</p>
      <figure id="fig:indutivo">
      <p><img src="fig/calssificacao.png" style="width:70.0%"
      alt="image" /> <span id="fig:indutivo"
      data-label="fig:indutivo"></span></p>
      <figcaption>Figure 18: Classificação de AM.</figcaption>
      </figure>
      <p>Ambas as tarefas podem ser categorizadas sob o conceito de
      aprendizado indutivo, sendo a capacidade de generalizar a partir
      de exemplos específicos, isto é, do conjunto de dados de
      treinamento. Em se tratando de tarefas preditivas, os algoritmos
      poderão implementar tarefas de <strong>classificação</strong>, nas
      quais o atributo alvo é <strong>qualitativo discreto</strong> (ou
      categórico), ou de <strong>regressão</strong>, em que o atributo
      alvo é <strong>quantitativo contínuo</strong> (ou numérico). Já as
      tarefas descritivas podem ser: <em>agrupamento</em>, que busca por
      similaridades, <em>associação</em>, que busca por padrões
      frequentes, e <em>sumarização</em>, que resulta em um resumo do
      conjunto de dados. No entanto, outras técnicas de aprendizagem de
      máquina supervisionadas e não supervisionadas estão fora do escopo
      deste livro.</p>
      <h2 id="processamento-de-linguagem-natural">Processamento de
      Linguagem Natural</h2>
      <p>O Processamento de Linguagem Natural (PLN) é um campo ligado à
      inteligência artificial, dedicando-se a equipar computadores com a
      capacidade de analisar e compreender a linguagem humana. Ele
      emprega técnicas computacionais com o propósito de aprender,
      compreender e produzir conteúdo em linguagem humana <span
      class="citation" data-cites="Zhao2020">(Zhao et al. 2020)</span>.
      Os sistemas de PLN podem suportar diferentes níveis ou combinações
      de níveis de análise linguística <span class="citation"
      data-cites="Zhao2020">(Zhao et al. 2020)</span>. Os níveis de
      análise linguística referem-se à análise fonética, morfológica,
      léxica, sintática, semântica, de discurso e análise pragmática da
      linguagem; existe uma suposição de que os seres humanos
      normalmente utilizam todos esses níveis para produzir ou
      compreender a linguagem <span class="citation"
      data-cites="Zhao2020">(Zhao et al. 2020)</span>.</p>
      <p>As abordagens de PLN podem ser classificadas em dois grandes
      grupos: o PLN simbólico e o PLN estatístico <span class="citation"
      data-cites="Zhao2020">(Zhao et al. 2020)</span>. Embora ambos os
      tipos de PLN tenham sido investigados ao mesmo tempo, foi o PLN
      simbólico que dominou o campo por algum tempo. Porém, abordagens
      estatísticas ganharam força principalmente após a divulgação do
      ChatGPT <span class="citation" data-cites="OpenAI2023">(OpenAI
      2023)</span>.</p>
      <p>Com o advento das abordagens estatísticas, em vez de depender
      exclusivamente de regras feitas à mão, os pesquisadores passaram a
      explorar os métodos estatísticos e probabilísticos alimentados por
      grandes coleções de textos. Nessa fase, algoritmos de aprendizado
      de máquina começaram a extrair padrões e regularidades a partir de
      grandes textos (também chamados de corpora). Essa abordagem
      empírica demonstrou ser mais robusta para lidar com a
      variabilidade intrínseca da língua – afinal, o sistema “aprende”
      com exemplos reais, capturando nuances que seriam difíceis de
      enumerar manualmente <span class="citation"
      data-cites="rcelebrone2025">(Celebrone 2025)</span>. Técnicas
      estatísticas impulsionaram aplicações como corretores
      ortográficos, sistemas de tradução automática e assistentes de
      voz, marcando uma mudança fundamental de estratégia no campo o
      PLN.</p>
      <p>Mas ainda não podemos descartar a abordagem simbólica - também
      são chamadas de baseadas em regras explícitas. Elas permitem que
      linguistas e programadores definiam manualmente gramáticas,
      ontologias e conjuntos de regras linguísticas para que o
      computador analise e gera frases corretamente. Essas abordagens
      conseguiram relativos sucessos em domínios restritos – como no
      ELIZA - mas mostrou-se limitada. Regras rígidas falham diante das
      inúmeras ambiguidades da linguagem humana, pois expressões fora do
      previsto escapam ao alcance dessas soluções programadas na íntegra
      <span class="citation" data-cites="rcelebrone2025">(Celebrone
      2025)</span>. Sistemas puramente baseados em regras eram
      inflexíveis frente à variação linguística natural, exigindo
      extensa manutenção manual para cobrir casos novos.</p>
      <p>Várias dessas técnicas de PLN são utilizadas em diversos
      chatbots. Desde o momento em que o usuário envia uma mensagem, o
      bot aplica PLN para entender a intenção por trás das palavras.
      Isso envolve identificar o que o usuário deseja ou pergunta, mesmo
      que haja várias maneiras de expressar a mesma coisa. Por exemplo,
      um assistente virtual deve reconhecer que perguntas como "Poderia
      me dizer se vai chover hoje?" e "Qual a previsão do tempo para
      hoje?" têm a mesma intenção (consulta sobre clima) ainda que usem
      palavras diferentes. Modelos modernos de compreensão de linguagem
      conseguem agrupar essas variações e mapear para a intenção correta
      graças a embeddings e classificadores treinados em múltiplas
      formulações de consulta <span class="citation"
      data-cites="rcelebrone2025">(Celebrone 2025)</span>.</p>
      <p>Na construção de chatbots, outra função muito utilizada é
      processar a entrada bruta do usuário, realizando a limpeza e a
      preparação dos dados textuais para que o sistema possa interpretar
      a mensagem e tomar as ações subsequentes apropriadas. Geralmente,
      o processo envolve a decomposição da linguagem em unidades
      menores, a compreensão do seu significado intrínseco e a
      determinação da resposta ou ação mais adequada.</p>
      <p>Além disso, o PLN ajuda o chatbot a extrair entidades e
      detalhes relevantes na frase do usuário – como datas, nomes,
      locais – que permitem contextualizar a solicitação. Esse
      entendimento refinado reduz drasticamente falhas de comunicação:
      em vez de responder “Desculpe, não entendi” a cada frase fora do
      roteiro, o bot passa a lidar bem com sinônimos, gírias e
      estruturas frasais incomuns, oferecendo respostas pertinentes.
      Grandes modelos de linguagem já incorporam esse tipo de
      generalização, o que explica por que assistentes atuais parecem
      entender até perguntas muito abertas ou indiretas <span
      class="citation" data-cites="rcelebrone2025">(Celebrone
      2025)</span>.</p>
      <p>Usando as técnicas de PLN adequadas os chatbots podem gerar
      respostas cada vez mais humanas. Nas gerações anteriores, muitos
      bots recorriam a respostas prontas ou scripts pré-definidos,
      conforme visto no Eliza, resultando em interações engessadas e
      repetitivas. Hoje, com os modelos de linguagem Grande (que serão
      discutidos no próximo capítulo), os chatbots podem produzir
      respostas inéditas, elaboradas em tempo real. Com isso o chatbot
      pode responder com frases gramaticalmente corretas, coerentes com
      o contexto da conversa e até ajustadas ao tom apropriado para o
      usuário.</p>
      <p>Portanto, neste capítulo conheceremos algumas das principais
      técnicas individualmente que não são apenas teorias acadêmicas,
      com elas temos um canivete suíço de partes que serão usadas de
      acordo com a necessidade do botmaster (o engenheiro que cria o
      chatbot algumas vezes é chamado de botmaster, enquanto quem
      conversa com ele é definido como usuário). Porém antes vamos
      configurar o Python que será a linguagem de programação utilizada
      durante todo o capítulo.</p>
      <h2 id="instalação-do-python">Instalação do Python</h2>
      <p>Nesta seção, abordaremos como preparar o ambiente necessário
      para trabalhar com vetorização de texto em Python. Isso inclui a
      instalação do Python e das bibliotecas necessárias, além de uma
      breve introdução ao uso do Jupyter Notebook.</p>
      <div class="mdframed">
      <p><span class="smallcaps">NOTA SOBRE REPRODUÇÃO DOS
      CÓDIGOS</span></p>
      <p>Todos os exemplos de código deste livro foram testados em
      Python na versão 3.12 na IDE Visual Studio Code em um PC com GPU.
      Alguns poucos códigos quando especificados foram executados no
      google Colab. No quadro abaixo do código-fonte é apresentado a
      saída do console.</p>
      </div>
      <p>Para começar a trabalhar com vetorização de texto, é essencial
      ter o Python instalado. Python é uma linguagem de programação
      amplamente utilizada para análise de dados e aprendizado de
      máquina devido à sua simplicidade e à vasta gama de bibliotecas
      disponíveis.</p>
      <p>Para instalar o Python, siga as instruções abaixo:</p>
      <ul>
      <li><p>No Windows, baixe o instalador do site oficial do Python
      (<a href="https://www.python.org/"
      class="uri">https://www.python.org/</a>) e siga as instruções do
      instalador.</p></li>
      <li><p>No macOS, você pode usar o <code>Homebrew</code> para
      instalar o Python executando o comando
      <code>brew install python</code>.</p></li>
      <li><p>No Linux, o Python geralmente já está instalado, mas você
      pode atualizá-lo usando o gerenciador de pacotes da sua
      distribuição.</p></li>
      </ul>
      <p>Uma vez que o Python esteja instalado, precisamos instalar
      algumas bibliotecas que são fundamentais para a vetorização de
      texto. Entre as principais estão “NumPy”, “Pandas”,
      “Scikit-learn”, “NLTK” e “SpaCy” e “Gensim”.</p>
      <p>O “SpaCy” é uma biblioteca de PLN de código aberto em Python,
      conhecida por sua velocidade e eficiência. O spaCy oferece APIs
      intuitivas e modelos pré-treinados para diversas tarefas de PLN,
      incluindo tokenização, POS tagging, lematização, NER e análise de
      dependências. Sua arquitetura é focada em desempenho para
      aplicações em produção.</p>
      <p>O “NLTK” (Natural Language Toolkit) é uma biblioteca Python
      fundamental para PLN, oferecendo uma ampla gama de ferramentas e
      recursos para tarefas como tokenização, stemming, POS tagging,
      análise sintática e NER. O NLTK é frequentemente utilizado para
      fins educacionais e de pesquisa.</p>
      <p>O “Gensim” é uma biblioteca Python especializada em modelagem
      de tópicos, análise de similaridade semântica e vetores de
      palavras. Ele é particularmente útil para identificar estruturas
      semânticas em grandes coleções de texto.</p>
      <p>O “pip” é o gerenciador de pacotes padrão do Python. Você pode
      instalar as bibliotecas necessárias usando o seguinte comando:</p>
      <pre
      data-caption="Comando do terminal para instalar algumas das bibliotecas necessárias do Python via pip."><code>pip install numpy pandas scikit-learn nltk spacy gensim</code></pre>
      <p>Após instalar as bibliotecas, é importante verificar se elas
      foram instaladas corretamente:</p>
      <div class="sourceCode" id="cb34" data-language="Python"
      data-caption="Exibe as versões das bibliotecas instaladas."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;NumPy version:&quot;</span>, np.__version__)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Pandas version:&quot;</span>, pd.__version__)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Scikit-learn version:&quot;</span>, sklearn.__version__)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;NLTK version:&quot;</span>, nltk.__version__)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;SpaCy version:&quot;</span>, spacy.__version__)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Gensim:&quot;</span>, gensim.__version__)</span></code></pre></div>
      <pre><code>NumPy version: 1.26.4
Pandas version: 2.2.2
Scikit-learn version: 1.5.1
NLTK version: 3.9.1
SpaCy version: 3.7.6</code></pre>
      <p>Este código importará as bibliotecas e exibirá suas versões,
      garantindo que todas estejam corretamente instaladas.</p>
      <p>O Jupyter Notebook é uma ferramenta poderosa para o
      desenvolvimento de scripts em Python, permitindo a combinação de
      código, texto, visualizações e resultados em um único
      documento.</p>
      <p>Você pode instalar o Jupyter Notebook usando o pip:</p>
      <pre
      data-caption="Comando do instalador de dependências do python para instalar a biblioteca para usar o notebook."><code>pip install jupyterlab</code></pre>
      <p>Para iniciar o Jupyter Notebook, execute o seguinte comando no
      terminal:</p>
      <pre
      data-caption="Comando no terminal depois de tudo instlalado para iniciar o notebook."><code>jupyter notebook</code></pre>
      <p>Isso abrirá o Jupyter Notebook no seu navegador padrão,
      permitindo que você comece a escrever e executar código Python de
      maneira interativa.</p>
      <p>Um exemplo simples de uso do Jupyter Notebook seria a criação
      de uma célula de código para calcular a soma de dois números:</p>
      <div class="sourceCode" id="cb38" data-language="Python"
      data-caption="Código Python para somar dois números."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;A soma de a e b é:&quot;</span>, a <span class="op">+</span> b)</span></code></pre></div>
      <pre><code>A soma de a e b é: 30</code></pre>
      <p>Este exemplo demonstra a simplicidade e interatividade que o
      Jupyter Notebook oferece, permitindo que você execute código
      Python célula por célula e veja os resultados imediatamente. Em
      resumo, configuramos o ambiente necessário para trabalhar com
      Python e as principais bibliotecas instaladas, além da
      configuração do Jupyter Notebook.</p>
      <h2 id="principais-técnicas-de-pln">Principais Técnicas de
      PLN</h2>
      <h3 id="tokenização">Tokenização</h3>
      <p>Tokenização é o processo de dividir um texto em unidades
      menores chamadas “tokens” que podem ser palavras, frases ou até
      mesmo sentenças <span class="citation"
      data-cites="jurafskyspeech">(Jurafsky and Martin 2023)</span>. A
      tokenização é o primeiro passo em muitos algoritmos de PLN, pois
      permite que os dados textuais sejam manipulados de forma
      programática.</p>
      <p>Tokenizar não é só separar por espaços, mas também lidar com
      pontuações, contrações e outros aspectos que podem afetar a
      análise. Um exemplo simples seria a frase “Eu estou feliz.”, que
      seria tokenizada em [“Eu”, “estou”, “feliz”, “.”]. Não
      necessariamente uma palavra equivale a um token. Em alguns casos,
      como em palavras compostas ou expressões idiomáticas, um único
      token pode representar uma ideia ou conceito mais amplo. Por
      exemplo, “São Paulo” poderia ser considerado um único token em vez
      de dois (“São” e “Paulo”).</p>
      <p>Existem diferentes abordagens para tokenização, incluindo
      tokenização baseada em regras, onde padrões específicos são
      definidos para identificar tokens (geralmente utilizando
      expressões regulares), e tokenização baseada em aprendizado de
      máquina, onde algoritmos aprendem a segmentar o texto com base em
      exemplos anteriores.</p>
      <p>A tokenização pode ser feita de várias maneiras, dependendo do
      idioma e do objetivo da análise. Em inglês, por exemplo, a
      tokenização pode ser mais simples devido à estrutura gramatical,
      enquanto em idiomas como o chinês, onde não há espaços entre as
      palavras, a tokenização pode ser mais complexa.</p>
      <p>A seguir, um exemplo do uso da biblioteca nltk e a tokenização
      de determinado texto, também chamado de corpus.</p>
      <div class="sourceCode" id="cb40" data-language="Python"
      data-caption="Este código demonstra como separar um texto em suas palavras e sentenças usando NLTK."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install nltk</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize, sent_tokenize</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Chatbots estão se tornando cada vez mais populares. Eles podem realizar muitas tarefas automaticamente.&quot;</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização em palavras</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>tokens_palavras <span class="op">=</span> word_tokenize(texto)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Tokens de palavras:&quot;</span>, tokens_palavras)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização em sentenças</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>tokens_sentencas <span class="op">=</span> sent_tokenize(texto)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Tokens de sentenças:&quot;</span>, tokens_sentencas)</span></code></pre></div>
      <pre><code>Tokens de palavras: [&#39;Chatbots&#39;, &#39;estão&#39;, &#39;se&#39;, &#39;tornando&#39;, &#39;cada&#39;, &#39;vez&#39;, &#39;mais&#39;, &#39;populares&#39;, &#39;.&#39;, &#39;Eles&#39;, &#39;podem&#39;, &#39;realizar&#39;, &#39;muitas&#39;, &#39;tarefas&#39;, &#39;automaticamente&#39;, &#39;.&#39;]
Tokens de sentenças: [&#39;Chatbots estão se tornando cada vez mais populares.&#39;, &#39;Eles podem realizar muitas tarefas automaticamente.&#39;]</code></pre>
      <p>SpaCy é uma biblioteca para PLN que oferece uma interface fácil
      de usar e é otimizada para processamento rápido. É possível também
      utilizar a biblioteca “spacy”, conforme os códigos a seguir:</p>
      <div class="sourceCode" id="cb42" data-language="Python"
      data-caption="Utiliza a biblioteca SpaCy para processamento de linguagem natural em português."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install spacy</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># import os</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># os.system(&quot;python -m spacy download pt_core_news_sm&quot;)</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o modelo de linguagem em português</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">&quot;pt_core_news_sm&quot;</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Chatbots estão se tornando cada vez mais populares.&quot;</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Processando o texto</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> nlp(texto)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Tokens:&quot;</span>, tokens)</span></code></pre></div>
      <pre><code>Tokens: [&#39;Chatbots&#39;, &#39;estão&#39;, &#39;se&#39;, &#39;tornando&#39;, &#39;cada&#39;, &#39;vez&#39;, &#39;mais&#39;, &#39;populares&#39;, &#39;.&#39;]</code></pre>
      <h3 id="lematização">Lematização</h3>
      <p>Lematização é o processo de reduzir palavras flexionadas ao seu
      lema, ou forma base <span class="citation"
      data-cites="singh2019building">(Singh, Ramasubramanian, and Shivam
      2019)</span>. Diferente da stemização, a lematização leva em
      consideração o contexto e a gramática da palavra para obter a
      forma correta.</p>
      <div class="sourceCode" id="cb44" data-language="Python"
      data-caption="Utiliza a biblioteca NLTK para realizar lematização de palavras em português. Ele define uma lista de palavras e aplica o lematizador a cada uma delas."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install nltk</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># import nltk</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co"># nltk.download(&#39;wordnet&#39;)</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicializando o lematizador</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>lemmatizer <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de palavras</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>palavras <span class="op">=</span> [<span class="st">&quot;correndo&quot;</span>, <span class="st">&quot;correu&quot;</span>, <span class="st">&quot;corredores&quot;</span>]</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Lematização das palavras</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>lematizadas <span class="op">=</span> [lemmatizer.lemmatize(palavra, pos<span class="op">=</span><span class="st">&#39;v&#39;</span>) <span class="cf">for</span> palavra <span class="kw">in</span> palavras]</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Palavras lematizadas:&quot;</span>, lematizadas)</span></code></pre></div>
      <pre><code>Palavras lematizadas: [&#39;correndo&#39;, &#39;correu&#39;, &#39;corredores&#39;]</code></pre>
      <p>Com spacy</p>
      <div class="sourceCode" id="cb46" data-language="Python"
      data-caption="Mesmo exemplo do código anterior porém utilizando a biblioteca Spacy."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">#pip install spacy</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o modelo de linguagem em português</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">&quot;pt_core_news_sm&quot;</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Chatbots estão se tornando cada vez mais populares.&quot;</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Processando o texto</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> nlp(texto)</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Tokens:&quot;</span>, tokens)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>lematizadas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Palavras lematizadas:&quot;</span>, lematizadas)</span></code></pre></div>
      <pre><code>Tokens: [&#39;Chatbots&#39;, &#39;estão&#39;, &#39;se&#39;, &#39;tornando&#39;, &#39;cada&#39;, &#39;vez&#39;, &#39;mais&#39;, &#39;populares&#39;, &#39;.&#39;]
Palavras lematizadas: [&#39;chatbots&#39;, &#39;estar&#39;, &#39;se&#39;, &#39;tornar&#39;, &#39;cada&#39;, &#39;vez&#39;, &#39;mais&#39;, &#39;popular&#39;, &#39;.&#39;]</code></pre>
      <h3 id="stemização">Stemização</h3>
      <p>A stemização é o processo de reduzir as palavras às suas raízes
      ou “stems”. É uma técnica mais simples que a lematização e,
      geralmente, não considera o contexto, o que pode levar a
      resultados menos precisos.</p>
      <div class="sourceCode" id="cb48" data-language="Python"
      data-caption="Utiliza a biblioteca NLTK para realizar a stemizacao de palavras. Ele define uma lista de palavras e aplica o stemmer para obter os radicais dessas palavras"><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install nltk</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> PorterStemmer</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Inicializando o stemizador</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>stemmer <span class="op">=</span> PorterStemmer()</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de palavras</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>palavras <span class="op">=</span> [<span class="st">&quot;correndo&quot;</span>, <span class="st">&quot;correu&quot;</span>, <span class="st">&quot;corredores&quot;</span>]</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Stemização das palavras</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>stems <span class="op">=</span> [stemmer.stem(palavra) <span class="cf">for</span> palavra <span class="kw">in</span> palavras]</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Stems das palavras:&quot;</span>, stems)</span></code></pre></div>
      <pre><code>Stems das palavras: [&#39;correndo&#39;, &#39;correu&#39;, &#39;corredor&#39;]</code></pre>
      <h3 id="stopwords">Stopwords</h3>
      <p>Stopwords são palavras comuns em um idioma (como “o”, “a”, “e”,
      “de”, “que”, etc.) que geralmente são removidas durante o
      pré-processamento de texto, pois não contribuem significativamente
      para o significado do texto <span class="citation"
      data-cites="Raj2019">(Raj 2019)</span>. A remoção dessas palavras
      pode melhorar a eficácia de certos algoritmos de PLN, focando nas
      palavras mais informativas do texto.</p>
      <div class="sourceCode" id="cb50" data-language="Python"
      data-caption="Demonstra como extrair o radical de palavras usando NLTK."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install nltk</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="co"># import nltk</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co"># nltk.download(&#39;stopwords&#39;)</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> word_tokenize</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar stopwords para o idioma portugues</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">&#39;portuguese&#39;</span>))</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Chatbots estão se tornando cada vez mais populares.&quot;</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Removendo stopwords</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>tokens_sem_stopwords <span class="op">=</span> [palavra <span class="cf">for</span> palavra <span class="kw">in</span> word_tokenize(texto) <span class="cf">if</span> palavra.lower() <span class="kw">not</span> <span class="kw">in</span> stop_words]</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Texto sem stopwords:&quot;</span>, tokens_sem_stopwords)</span></code></pre></div>
      <pre><code>Texto sem stopwords: [&#39;Chatbots&#39;, &#39;tornando&#39;, &#39;cada&#39;, &#39;vez&#39;, &#39;populares&#39;, &#39;.&#39;]</code></pre>
      <p>Com a biblioteca Spacy:<br />
      </p>
      <div class="sourceCode" id="cb52" data-language="Python"
      data-caption="Demonstra como tokenizar um texto e filtrar stopwords usando SpaCy em português."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install spacy</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="co"># python -m spacy download pt_core_news_sm</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o modelo de linguagem em português</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">&quot;pt_core_news_sm&quot;</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Chatbots estão se tornando cada vez mais populares.&quot;</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Processando o texto</span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> nlp(texto)</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Tokens:&quot;</span>, tokens)</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Removendo stopwords</span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>tokens_sem_stopwords <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Texto sem stopwords:&quot;</span>, tokens_sem_stopwords)</span></code></pre></div>
      <pre><code>Tokens: [&#39;Chatbots&#39;, &#39;estão&#39;, &#39;se&#39;, &#39;tornando&#39;, &#39;cada&#39;, &#39;vez&#39;, &#39;mais&#39;, &#39;populares&#39;, &#39;.&#39;]
Texto sem stopwords: [&#39;Chatbots&#39;, &#39;tornando&#39;, &#39;populares&#39;, &#39;.&#39;]</code></pre>
      <h2 id="outras-técnicas-de-pln">Outras técnicas de PLN</h2>
      <p>Marcação Morfossintática (POS Tagging): Esta técnica consiste
      em atribuir a cada <em>token</em> em um texto uma categoria
      gramatical, como substantivo, verbo, adjetivo, advérbio, etc. A
      marcação POS é utilizada para identificar entidades e compreender
      a estrutura gramatical das frases. Por exemplo, na frase “Eu estou
      aprendendo como construir chatbots”, a marcação POS poderia
      identificar “Eu” como um pronome (PRON), “estou aprendendo” como
      um verbo (VERB) e “chatbots” como um substantivo (NOUN).</p>
      <p>Reconhecimento de Entidades Nomeadas (NER): O NER é a tarefa de
      identificar e classificar entidades nomeadas em um texto, como
      nomes de pessoas (PERSON), organizações (ORG), localizações
      geográficas (GPE, LOC), datas (DATE), valores monetários (MONEY),
      etc. Por exemplo, na frase “Google tem sua sede em Mountain View,
      Califórnia, com uma receita de 109,65 bilhões de dólares
      americanos”, o NER identificaria “Google” como uma organização
      (ORG), “Mountain View” e “Califórnia” como localizações
      geográficas (GPE) e “109,65 bilhões de dólares americanos” como um
      valor monetário (MONEY). Essa capacidade é vital para que chatbots
      compreendam os detalhes relevantes nas <em>utterances</em> dos
      usuários.</p>
      <p>Análise de Dependências (Dependency Parsing): Esta técnica
      examina as relações gramaticais entre as palavras em uma frase,
      revelando a estrutura sintática e as dependências entre os
      <em>tokens</em>. A análise de dependências pode ajudar a entender
      quem está fazendo o quê a quem. Por exemplo, na frase “Reserve um
      voo de Corumbá para Maceió”, a análise de dependências pode
      identificar “Corumbá” e “Maceió” como modificadores de “voo”
      através das preposições “de” e “para”, respectivamente, e
      “Reserve” como a raiz da ação. Essa análise é útil para extrair
      informações sobre as intenções do usuário, mesmo em frases mais
      complexas.</p>
      <p>Classificação de Texto: Uma técnica de aprendizado de máquina
      que atribui um texto a uma ou mais categorias predefinidas. No
      contexto de chatbots, a classificação de texto é fundamental para
      a detecção de intenção (mais sobre detecção de <em>intenção</em>
      será apresentada nas seções seguintes), onde as categorias
      representam as diferentes intenções do usuário. Algoritmos como o
      <em>Naïve Bayes</em> são modelos estatísticos populares para essa
      tarefa, baseados no teorema de Bayes e em fortes suposições de
      independência entre as características. O treinamento desses
      classificadores requer um <em>corpus</em> de dados rotulados, onde
      cada <em>utterance</em> (entrada do usuário) é associada a uma
      intenção específica.</p>
      <h2 id="expressões-regulares">Expressões Regulares</h2>
      <p>Expressões regulares, frequentemente abreviadas como
      <em>regex</em>, são sequências de caracteres que definem padrões
      de busca. Elas são utilizadas em chatbots para diversas tarefas
      relacionadas ao processamento e à análise de texto fornecido pelos
      usuários.</p>
      <p>Frameworks populares para desenvolvimento de chatbots
      frequentemente integram o uso de expressões regulares para
      aprimorar a extração de entidades. Por exemplo, as regex podem ser
      definidas nos dados de treinamento para ajudar o sistema a
      reconhecer padrões específicos como nomes de ruas ou códigos de
      produtos. Essa abordagem permite melhorar a precisão do
      reconhecimento de entidades, um componente importante para a
      compreensão da intenção do usuário.</p>
      <p>A sintaxe das expressões regulares consiste em uma combinação
      de caracteres literais (que correspondem a si mesmos) e
      metacaracteres, que possuem significados especiais e permitem
      definir padrões de busca mais complexos. As expressões regulares
      podem ser aplicadas em uma variedade de cenários no
      desenvolvimento de chatbots. Algumas das aplicações de regex nos
      chatbots incluem:</p>
      <ul>
      <li><p>Extração de entidades: Identificação e extração de
      informações específicas, como endereços de e-mail, números de
      telefone, datas e outros dados estruturados presentes na entrada
      do usuário.</p></li>
      <li><p>Validação de entradas do usuário: Verificação se a entrada
      do usuário corresponde a um formato esperado, como datas em um
      formato específico (DD/MM/AAAA), códigos postais ou outros padrões
      predefinidos.</p></li>
      <li><p>Detecção de Intenção: Detecção de comandos específicos
      inseridos pelo usuário, como <code>/ajuda</code>,
      <code>/iniciar</code> ou palavras-chave que indicam uma intenção
      específica.</p></li>
      <li><p>Limpeza de texto: Remoção de ruídos e elementos indesejados
      do texto, como tags HTML, espaços em branco excessivos ou
      caracteres especiais que podem interferir no processamento
      subsequente.</p></li>
      <li><p>Tokenização simples: Embora métodos mais avançados sejam
      comuns em PLN, regex pode ser usada para dividir o texto em
      unidades menores (tokens) com base em padrões simples.</p></li>
      </ul>
      <p>Essas tarefas são fundamentais para garantir que o chatbot
      possa interpretar e responder adequadamente às entradas dos
      usuários, especialmente em cenários onde a informação precisa ser
      estruturada ou verificada antes de ser processada por modelos de
      linguagem mais complexos. Cada uma destas tarefas será detalhada
      neste Capítulo.</p>
      <p>O módulo <code>re</code> em Python é a biblioteca padrão para
      trabalhar com expressões regulares. Ele fornece diversas funções
      que permitem realizar operações de busca, correspondência e
      substituição em strings com base em padrões definidos por regex.
      Algumas das funções mais utilizadas incluem:</p>
      <ul>
      <li><p><code>re.match(pattern, string)</code>: Tenta encontrar uma
      correspondência do padrão no <em>início</em> da string. Se uma
      correspondência for encontrada, retorna um objeto de
      correspondência; caso contrário, retorna
      <code>None</code>.</p></li>
      <li><p><code>re.search(pattern, string)</code>: Procura a primeira
      ocorrência do padrão em <em>qualquer posição</em> da string.
      Retorna um objeto de correspondência se encontrado, ou
      <code>None</code> caso contrário.</p></li>
      <li><p><code>re.findall(pattern, string)</code>: Encontra
      <em>todas</em> as ocorrências não sobrepostas do padrão na string
      e as retorna como uma lista de strings.</p></li>
      <li><p><code>re.sub(pattern, repl, string)</code>: Substitui todas
      as ocorrências do padrão na string pela string de substituição
      <code>repl</code>. Retorna a nova string resultante.</p></li>
      </ul>
      <h4 class="unnumbered" id="extração-de-e-mails">Extração de
      E-mails</h4>
      <p>Um caso de uso comum em chatbots é a extração de endereços de
      e-mail do texto fornecido pelo usuário. O seguinte exemplo em
      Python demonstra como usar <code>re.findall</code> para realizar
      essa tarefa. Ilustrando o uso regex para identificar e extrair
      informações específicas de um texto.</p>
      <div class="sourceCode" id="cb54" data-language="Python"
      data-caption="Extração de e-mails com regex."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Entre em contato em exemplo@email.com ou suporte@outroemail.com.&quot;</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>padrao <span class="op">=</span> <span class="vs">r&#39;</span><span class="dv">\b</span><span class="pp">[A-Za-z0-9._%+-]</span><span class="op">+</span><span class="vs">@</span><span class="pp">[A-Za-z0-9.-]</span><span class="vs"> </span><span class="op">+</span><span class="ch">\.</span><span class="pp">[A-Z|a-z]</span><span class="op">{2,}</span><span class="dv">\b</span><span class="vs">&#39;</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>emails <span class="op">=</span> re.findall(padrao, texto)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(emails)</span></code></pre></div>
      <pre><code>[&#39;exemplo@email.com&#39;, &#39;suporte@outroemail.com&#39;]</code></pre>
      <h4 class="unnumbered" id="validação-de-datas">Validação de
      Datas</h4>
      <p>Chatbots que lidam com agendamentos ou reservas frequentemente
      precisam validar se a data fornecida pelo usuário está em um
      formato correto. O seguinte exemplo demonstra como validar datas
      no formato DD/MM/AAAA:</p>
      <div class="sourceCode" id="cb56" data-language="Python"
      data-caption="Validação de datas com regex."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>padrao_data <span class="op">=</span> <span class="vs">r&#39;</span><span class="dv">\b\d</span><span class="op">{2}</span><span class="vs">/</span><span class="dv">\d</span><span class="op">{2}</span><span class="vs">/</span><span class="dv">\d</span><span class="op">{4}</span><span class="dv">\b</span><span class="vs">&#39;</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>datas_teste <span class="op">=</span> [<span class="st">&quot;31/12/2020&quot;</span>, <span class="st">&quot;1/1/2021&quot;</span>, <span class="st">&quot;2023-05-10&quot;</span>, <span class="st">&quot;25/06/2025 10:00&quot;</span>]</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> data <span class="kw">in</span> datas_teste:</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> re.match(padrao_data, data):</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;&#39;</span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">&#39; é uma data válida no formato DD/MM/AAAA.&quot;</span>)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;&#39;</span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">&#39; não é uma data válida no formato DD/MM/AAAA.&quot;</span>)</span></code></pre></div>
      <p>A saída deste código ilustra quais das strings de teste
      correspondem ao padrão de data especificado.</p>
      <pre><code>&#39;31/12/2020&#39; é uma data válida no formato DD/MM/AAAA.
&#39;1/1/2021&#39; não é uma data válida no formato DD/MM/AAAA.
&#39;2023-05-10&#39; não é uma data válida no formato DD/MM/AAAA.
&#39;25/06/2025 10:00&#39; é uma data válida no formato DD/MM/AAAA.</code></pre>
      <h4 class="unnumbered" id="análise-de-comandos">Análise de
      Comandos</h4>
      <p>Em interfaces de chatbot baseadas em texto, os usuários podem
      interagir através de comandos específicos, como
      <code>/ajuda</code> ou <code>/iniciar</code>. As regex podem ser
      usadas para detectar esses comandos de forma eficiente. Este
      exemplo abaixo mostra como identificar strings que começam com uma
      barra seguida por um ou mais caracteres alfanuméricos.</p>
      <div class="sourceCode" id="cb58" data-language="Python"
      data-caption="Análise de comandos com regex."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>padrao_comando <span class="op">=</span> <span class="vs">r&#39;</span><span class="dv">^</span><span class="vs">/</span><span class="dv">\w</span><span class="op">+</span><span class="vs">&#39;</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>comandos_teste <span class="op">=</span> [<span class="st">&quot;/ajuda&quot;</span>, <span class="st">&quot;/iniciar&quot;</span>, <span class="st">&quot;ajuda&quot;</span>, <span class="st">&quot;iniciar/&quot;</span>]</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> comando <span class="kw">in</span> comandos_teste:</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> re.match(padrao_comando, comando):</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;&#39;</span><span class="sc">{</span>comando<span class="sc">}</span><span class="ss">&#39; é um comando válido.&quot;</span>)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;&#39;</span><span class="sc">{</span>comando<span class="sc">}</span><span class="ss">&#39; não é um comando válido.&quot;</span>)</span></code></pre></div>
      <pre><code>&#39;/ajuda&#39; é um comando válido.
&#39;/iniciar&#39; é um comando válido.
&#39;ajuda&#39; não é um comando válido.
&#39;iniciar/&#39; não é um comando válido.</code></pre>
      <h4 class="unnumbered" id="tokenização-simples">Tokenização
      Simples</h4>
      <p>Embora para tarefas complexas de PLN sejam utilizadas técnicas
      de tokenização mais avançadas, as regex podem ser úteis para
      realizar uma tokenização básica, dividindo o texto em palavras ou
      unidades menores com base em padrões de separação. A saída será
      uma lista de strings, onde o padrão <code>\W+</code> corresponde a
      um ou mais caracteres não alfanuméricos, utilizados como
      delimitadores.</p>
      <div class="sourceCode" id="cb60" data-language="Python"
      data-caption="Tokenização simples com regex."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Olá, como vai você?&quot;</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> re.split(<span class="vs">r&#39;</span><span class="dv">\W</span><span class="op">+</span><span class="vs">&#39;</span>, texto)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokens)</span></code></pre></div>
      <pre><code>[&#39;Olá&#39;, &#39;como&#39;, &#39;vai&#39;, &#39;você&#39;, &#39;&#39;]</code></pre>
      <h4 class="unnumbered" id="limpeza-de-texto">Limpeza de Texto</h4>
      <p>Chatbots podem precisar processar texto que contém elementos
      indesejados, como tags HTML. As regex podem ser usadas para
      remover esses elementos:</p>
      <div class="sourceCode" id="cb62" data-language="Python"
      data-caption="Limpeza de texto removendo tags HTML."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>texto_html <span class="op">=</span> <span class="st">&quot;&lt;p&gt;Este é um parágrafo com &lt;b&gt;texto em negrito&lt;/b&gt;.&lt;/p&gt;&quot;</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>texto_limpo <span class="op">=</span> re.sub(<span class="vs">r&#39;&lt;</span><span class="pp">[^&gt;]</span><span class="op">+</span><span class="vs">&gt;&#39;</span>, <span class="st">&#39;&#39;</span>, texto_html)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(texto_limpo)</span></code></pre></div>
      <pre><code>Este é um parágrafo com texto em negrito.</code></pre>
      <p>Embora os fundamentos das regex sejam suficientes para muitas
      tarefas, existem construções mais avançadas que podem ser úteis em
      cenários complexos. Alguns exemplos incluem Lookaheads e
      Lookbehinds que permitem verificar se um padrão é seguido ou
      precedido por outro padrão, sem incluir esse outro padrão na
      correspondência; além disso, correspondência não-gulosa: que ao
      usar quantificadores como <code>*</code> ou <code>+</code>, a
      correspondência padrão é "gulosa", ou seja, tenta corresponder à
      maior string possível. Adicionar um <code>?</code> após o
      quantificador (<code>*?</code>, <code>+?</code>) torna a
      correspondência "não-gulosa", correspondendo à menor string
      possível.</p>
      <p>É importante reconhecer que, apesar de sua utilidade, as
      expressões regulares têm limitações significativas quando se trata
      de compreender a complexidade da linguagem natural. As regex são
      baseadas em padrões estáticos e não possuem a capacidade de
      entender o contexto, a semântica ou as nuances da linguagem
      humana.</p>
      <p>No contexto de um fluxo de trabalho de chatbot, as expressões
      regulares são frequentemente mais eficazes nas etapas de
      pré-processamento, como limpeza e validação de entradas, enquanto
      técnicas de PLN mais sofisticadas são empregadas para a
      compreensão da linguagem em um nível mais alto.</p>
      <p>Porém, para tarefas que exigem uma compreensão mais profunda do
      significado e da intenção por trás das palavras, técnicas
      avançadas de Processamento de Linguagem Natural (PLN), como
      modelagem de linguagem, análise de sentimentos e reconhecimento de
      entidades nomeadas (NER) baseadas em aprendizado de máquina, são
      indispensáveis e serão apresentadas no Capítulo <a href="#cap:LLM"
      data-reference-type="ref" data-reference="cap:LLM">5</a>.</p>
      <h2 id="entendimento-de-linguagem-natural">Entendimento de
      Linguagem Natural</h2>
      <p>O Entendimento de Linguagem Natural (ULN) é um subdomínio
      específico dentro do universo mais vasto do PLN. Enquanto o PLN
      abarca um conjunto diversificado de operações sobre a linguagem, o
      ULN se concentra de maneira particular na habilidade da máquina de
      aprender e interpretar a linguagem natural tal como ela é
      comunicada pelos seres humanos. Em outras palavras, o ULN é o ramo
      do PLN dedicado à extração de significado e à identificação da
      intenção por trás do texto inserido pelo usuário. As aplicações do
      ULN são extensas e incluem funcionalidades cruciais para chatbots,
      como a capacidade de responder a perguntas, realizar buscas em
      linguagem natural, identificar relações entre entidades, analisar
      o sentimento expresso no texto, sumarizar informações textuais e
      auxiliar em processos de descoberta legal.</p>
      <p>No cerne da funcionalidade de um chatbot reside a sua
      capacidade de compreender as mensagens dos usuários e responder de
      forma adequada. O PLN desempenha um papel central nesse processo,
      permitindo que o chatbot: (i) detecte a intenção do usuário; (ii)
      extraia entidades relevantes; e (iii) processe linguagem variada e
      informal; e (iv) mantenha o contexto da conversa.</p>
      <p>(i) Detecção da Intenção do Usuário: O objetivo por trás da
      mensagem do usuário é o primeiro passo. Isso é frequentemente
      abordado como um problema de classificação de texto, onde o
      chatbot tenta classificar a <em>utterance</em> do usuário em uma
      das intenções predefinidas. Para detecção da intenção, é possível
      utilizar técnicas de aprendizado de máquina, como o algoritmo
      <em>Naïve Bayes</em>, para construir esses classificadores.
      Plataformas como Rasa NLU (<a href="https://rasa.com/"
      class="uri">https://rasa.com/</a>), LUIS.ai (<a
      href="https://www.luis.ai/" class="uri">https://www.luis.ai/</a>)
      e Dialogflow (<a href="https://dialogflow.cloud.google.com/"
      class="uri">https://dialogflow.cloud.google.com/</a>) simplificam
      significativamente o processo de treinamento e implantação desses
      modelos de intenção.</p>
      <p>O Rasa NLU é um componente de código aberto do framework Rasa
      para construir chatbots, focado em entendimento de linguagem
      natural. Ele permite treinar modelos personalizados para
      classificação de intenção e extração de entidades, oferecendo
      flexibilidade e controle sobre os dados.</p>
      <p>(ii) Extraia Entidades Relevantes: Além da intenção geral, as
      mensagens dos usuários frequentemente contêm detalhes específicos,
      conhecidos como entidades, que são essenciais para atender à
      solicitação. Por exemplo, em “Reserve um voo de Corumbá para
      Maceió amanhã”, a intenção é reservar um voo, e as entidades são a
      cidade de origem (“Corumbá”), a cidade de destino (“Maceió”) e a
      data (“amanhã”). As técnicas de NER e os modelos de extração de
      entidades fornecidos por ferramentas como spaCy, NLTK, CoreNLP,
      LUIS.ai e Rasa NLU são fundamentais para identificar e extrair
      essas informações.</p>
      <p>O CoreNLP é um conjunto de ferramentas de PLN robusto e
      amplamente utilizado, desenvolvido em Java. O CoreNLP oferece
      capacidades abrangentes de análise linguística, incluindo POS
      tagging, análise de dependências, NER e análise de sentimentos.
      Possui APIs para integração com diversas linguagens de
      programação, incluindo Python.</p>
      <p>(iii) Processe Linguagem Variada e Informal: Os usuários podem
      se comunicar com chatbots usando uma ampla gama de vocabulário,
      gramática e estilo, incluindo erros de digitação, abreviações e
      linguagem informal. As técnicas de PLN, como stemming, lematização
      e busca por similaridade, ajudam o chatbot a lidar com essa
      variabilidade e a compreender a essência da mensagem, mesmo que
      não seja expressa de forma perfeitamente gramatical.</p>
      <p>A arquitetura típica de um chatbot envolve uma camada de
      processamento de linguagem natural (NLP/NLU engine) que recebe a
      entrada de texto do usuário. Essa camada é responsável por
      realizar as tarefas de PLN mencionadas anteriormente: tokenização,
      análise morfossintática, extração de entidades, detecção de
      intenção, etc. O resultado desse processamento é uma representação
      estruturada da mensagem do usuário, que pode ser entendida pela
      lógica de negócios do chatbot.</p>
      <p>Com base nessa representação estruturada, um motor de decisão
      (<em>decision engine</em>) no chatbot pode então corresponder à
      intenção do usuário a fluxos de trabalho preconfigurados ou a
      regras de negócio específicas. Em alguns casos, a geração de
      linguagem natural (NLG), outro subcampo do PLN, é utilizada para
      formular a resposta do chatbot ao usuário.</p>
      <h3 id="intents">Intents</h3>
      <p>Os Intents representam a intenção ou o propósito por trás da
      mensagem de um usuário ao interagir com o chatbot <span
      class="citation" data-cites="Khan2018">(Khan and Das 2018)</span>.
      Em termos mais simples, é o que o usuário deseja que o chatbot
      faça ou sobre o que ele quer saber.</p>
      <p>Um intent é usado para identificar programaticamente a intenção
      da pessoa que está usando o chatbot. O chatbot deve ser capaz de
      executar alguma ação com base no “intent” que detecta na mensagem
      do usuário. Cada tarefa que o chatbot deve realizar define um
      intent. A aplicação prática dos intents varia conforme o domínio
      do chatbot, veja o exemplo:</p>
      <p>Por exemplo, para um chatbot de uma loja de moda, exemplos de
      intents seriam “busca de um produto” (quando um usuário quer ver
      produtos) e “endereço loja” (quando um usuário pergunta sobre
      lojas); em um chatbot para pedir comida, “consultar preços” e
      “realizar pedido” podem ser intents distintos.</p>
      <p>Detectar o <em>intent</em> da mensagem do usuário é um problema
      conhecido de aprendizado de máquina, realizado por meio de uma
      técnica chamada classificação de texto. O objetivo é classificar
      frases em múltiplas classes (os intents). O modelo de aprendizado
      de máquina é treinado com um conjunto de dados que contém exemplos
      de mensagens e seus intents correspondentes. Após o treinamento, o
      modelo pode prever o intent de novas mensagens que não foram
      vistas antes.</p>
      <h3 id="sec:intents_utterances">Utterances</h3>
      <p>Cada intent pode ser expresso de várias maneiras pelo usuário.
      Essas diferentes formas são chamadas de <em>utterances</em> ou
      expressões do usuário.</p>
      <p>Por exemplo, para o intent <code>realizar pedido</code>, as
      utterances poderiam ser “Eu gostaria de fazer um pedido”, “Quero
      pedir comida”, “Como faço para pedir?”, etc. Cada uma dessas
      expressões representa a mesma intenção, mas com palavras
      diferentes. O modelo de aprendizado de máquina deve ser capaz de
      reconhecer todas essas variações como pertencentes ao mesmo
      intent.</p>
      <p>É sugerido fornecer um número ótimo de utterances variadas por
      intent para garantir um bom treinamento do modelo de
      reconhecimento.</p>
      <h3 id="sec:intents_entities">Entities</h3>
      <p>As entidades extraídas permitem ao chatbot refinar sua resposta
      ou ação. Os Intents frequentemente contêm metadados importantes
      chamados <em>Entities</em>. Estas são palavras-chave ou frases
      dentro da utterance do usuário que ajudam o chatbot a identificar
      detalhes específicos sobre o pedido, permitindo fornecer
      informações mais direcionadas. Por exemplo, na frase “Eu quero
      pedir uma pizza de calabreza com borda recheada”, as entidades
      podem incluir: Para o <em>intent</em> <em>realizar pedido</em> e
      para as <em>Entities</em>: <em>pizza</em>, <em>calabreza</em> e
      <em>borda recheada</em>.</p>
      <h3 id="desafios">Desafios</h3>
      <p>O processo de treinamento envolve a construção de um modelo de
      aprendizado de máquina. Este modelo aprende a partir do conjunto
      definido de intents, suas utterances associadas e as entidades
      anotadas. O objetivo do treinamento é capacitar o modelo a
      categorizar corretamente novas utterances (que não foram vistas
      durante o treinamento) no intent apropriado e a extrair as
      entidades relevantes.</p>
      <p>Quando o chatbot processa uma nova mensagem do usuário, o
      modelo de reconhecimento de intent não apenas classifica a
      mensagem em um dos intents definidos, mas também fornece uma
      <em>pontuação de confiança</em> (geralmente entre 0 e 1). Essa
      pontuação indica o quão seguro o modelo está e que a classificação
      está correta. É comum definir um <em>limite (threshold)</em> de
      confiança. Se a pontuação do intent detectado estiver abaixo desse
      limite, o chatbot pode pedir esclarecimentos ao usuário em vez de
      executar uma ação baseada em uma suposição incerta.</p>
      <p>Uma vez que um intent é detectado com confiança suficiente, o
      chatbot pode executar a ação correspondente. Isso pode envolver
      consultar um banco de dados, chamar uma API externa, fornecer uma
      resposta estática ou iniciar um fluxo de diálogo mais complexo.
      Além disso, a análise dos intents mais frequentemente capturados
      fornece insights valiosos sobre como os usuários estão interagindo
      com o chatbot e quais são suas principais necessidades. Essas
      análises são importantes tanto para a otimização do bot quanto
      para as decisões de negócio.</p>
      <p>Apesar destes procedimentos, alguns problemas ainda desafiam os
      pesquisadores, tais como a Geração de texto coerente; a Sintaxe e
      gramática; a semântica; o Contexto; e a Ambiguidade.</p>
      <p>A <em>geração de texto coerente</em> é um desafio porque
      envolve não apenas a escolha de palavras, mas também a construção
      de frases que façam sentido no contexto da conversa. A <em>sintaxe
      e gramática</em> são importantes para garantir que o texto gerado
      seja gramaticalmente correto e compreensível. A <em>semântica</em>
      se refere ao significado das palavras e frases, e é importante
      para garantir que o texto gerado transmita a mensagem correta. O
      <em>contexto</em> é importante para entender o que foi dito
      anteriormente na conversa e como isso afeta a resposta atual. A
      <em>ambiguidade</em> pode surgir quando uma palavra ou frase tem
      múltiplos significados, tornando difícil para o modelo determinar
      qual interpretação é a correta.</p>
      <h2 id="vetorização-e-representação-de-texto">Vetorização e
      Representação de Texto</h2>
      <p>A vetorização de texto é um passo importante no Processamento
      de Linguagem Natural (PLN), pois permite transformar dados
      textuais em uma forma que os modelos de aprendizado de máquina
      podem entender e processar. Nesta seção, exploraremos algumas
      técnicas de vetorização, incluindo One-Hot Encoding, Bag of Words
      e TF-IDF (Term Frequency-Inverse Document Frequency), juntamente
      com implementações práticas em Python.</p>
      <h3 id="one-hot-encoding">One-Hot Encoding</h3>
      <p>One-Hot Encoding é uma das formas mais simples de vetorização
      de texto, onde cada palavra em um vocabulário é representada como
      um vetor binário. Cada posição no vetor corresponde a uma palavra
      do vocabulário, e apenas a posição da palavra que está sendo
      representada tem valor 1, enquanto todas as outras têm valor
      0.</p>
      <div class="sourceCode" id="cb64" data-language="Python"
      data-caption="Transforma palavras de um pequeno corpus de frases em vetores one-hot usando o OneHotEncoder do scikit-learn."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install scikit-learn</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">&quot;Eu amo programação&quot;</span>, <span class="st">&quot;A programação é divertida&quot;</span>]</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização simples</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>tokenized_corpus <span class="op">=</span> [sentence.split() <span class="cf">for</span> sentence <span class="kw">in</span> corpus]</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten para obter todas as palavras</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>all_words <span class="op">=</span> [word <span class="cf">for</span> sentence <span class="kw">in</span> tokenized_corpus <span class="cf">for</span> word <span class="kw">in</span> sentence]</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Remover duplicatas</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(all_words))</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Criar matriz de índices das palavras para cada frase</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>word_to_idx <span class="op">=</span> {word: idx <span class="cf">for</span> idx, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>corpus_idx <span class="op">=</span> [[word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> sentence] <span class="cf">for</span> sentence <span class="kw">in</span> tokenized_corpus]</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar o encoder para o vocabulário</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> OneHotEncoder(sparse_output<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>one_hot_encoded <span class="op">=</span> encoder.fit_transform(</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        np.array(vocab).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vocabulário:&quot;</span>, vocab)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;One-Hot Encoding:</span><span class="ch">\n</span><span class="st">&quot;</span>, one_hot_encoded)</span></code></pre></div>
      <pre><code>Vocabulário: [&#39;amo&#39;, &#39;é&#39;, &#39;programação&#39;, &#39;Eu&#39;, &#39;A&#39;, &#39;divertida&#39;]
One-Hot Encoding:
[[0. 0. 1. 0. 0. 0.]
[0. 0. 0. 0. 0. 1.]
[0. 0. 0. 0. 1. 0.]
[0. 1. 0. 0. 0. 0.]
[1. 0. 0. 0. 0. 0.]
[0. 0. 0. 1. 0. 0.]]</code></pre>
      <p>Embora seja fácil de entender e implementar, o One-Hot Encoding
      não leva em consideração a semântica das palavras, resultando em
      vetores muito esparsos e de alta dimensionalidade, especialmente
      para vocabulários grandes.</p>
      <h3 id="bag-of-words-bow">Bag of Words (BoW)</h3>
      <p>A técnica de Bag of Words (BoW) é uma melhoria em relação ao
      One-Hot Encoding. Aqui, em vez de vetores binários, usamos
      contagens de palavras. Para cada documento, construímos um vetor
      com a frequência de cada palavra no vocabulário.</p>
      <div class="sourceCode" id="cb66" data-language="Python"
      data-caption="Utiliza a biblioteca scikit-learn para transformar um conjunto de frases (corpus) em uma matriz BoW."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install scikit-learn</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de corpus</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">&quot;Eu amo programação&quot;</span>, <span class="st">&quot;A programação é divertida&quot;</span>, <span class="st">&quot;Eu amo a vida&quot;</span>]</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Criação do vetor de contagem (Bag of Words)</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>bow_matrix <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vocabulário:&quot;</span>, vectorizer.get_feature_names_out())</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bag of Words Matrix:</span><span class="ch">\n</span><span class="st">&quot;</span>, bow_matrix.toarray())</span></code></pre></div>
      <pre><code>Vocabulário: [&#39;amo&#39; &#39;divertida&#39; &#39;eu&#39; &#39;programação&#39; &#39;vida&#39;]
Bag of Words Matrix:
[[1 0 1 1 0]
[0 1 0 1 0]
[1 0 1 0 1]]</code></pre>
      <p>A matriz resultante tem cada linha representando um documento e
      cada coluna representando a contagem de uma palavra específica no
      documento. No entanto, essa técnica também não leva em
      consideração o contexto ou a ordem das palavras.</p>
      <h3 id="tf-idf">TF-IDF</h3>
      <p>TF-IDF é uma técnica que combina a frequência de termos (TF)
      com a frequência inversa de documentos (IDF). Esta abordagem ajuda
      a dar mais peso a palavras que são raras no corpus, mas aparecem
      frequentemente em um documento específico, o que geralmente indica
      sua importância.</p>
      <div class="sourceCode" id="cb68" data-language="Python"
      data-caption="Técnica comum em processamento de linguagem natural para converter texto em dados numéricos."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install scikit-learn</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de corpus</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">&quot;Eu amo programação&quot;</span>, <span class="st">&quot;A programação é divertida&quot;</span>, <span class="st">&quot;Eu amo a vida&quot;</span>]</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Criação do vetor TF-IDF</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> tfidf_vectorizer.fit_transform(corpus)</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vocabulário:&quot;</span>, tfidf_vectorizer.get_feature_names_out())</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;TF-IDF Matrix:</span><span class="ch">\n</span><span class="st">&quot;</span>, tfidf_matrix.toarray())</span></code></pre></div>
      <pre><code>Vocabulário: [&#39;amo&#39; &#39;divertida&#39; &#39;eu&#39; &#39;programação&#39; &#39;vida&#39;]
TF-IDF Matrix:
[[0.57735027 0.         0.57735027 0.57735027 0.        ]
[0.         0.79596054 0.         0.60534851 0.        ]
[0.51785612 0.         0.51785612 0.         0.68091856]]</code></pre>
      <p>A matriz TF-IDF dá valores diferentes para as palavras, não
      apenas com base em sua frequência, mas também na relevância,
      ajudando a reduzir o peso de palavras comuns que não são muito
      informativas para o contexto.</p>
      <h3 id="comparação-de-técnicas-de-vetorização">Comparação de
      Técnicas de Vetorização</h3>
      <p>As três técnicas abordadas têm seus prós e contras:</p>
      <ul>
      <li><p>One-Hot Encoding: Simples e fácil de implementar, mas
      resulta em vetores esparsos e de alta dimensionalidade.</p></li>
      <li><p>Bag of Words: Considera a frequência das palavras, mas
      ainda assim é cego ao contexto e a semântica.</p></li>
      <li><p>TF-IDF: Reduz o impacto de palavras comuns e realça
      palavras que são mais informativas para cada documento.</p></li>
      </ul>
      <p>A escolha da técnica depende do caso de uso específico. Por
      exemplo, para tarefas simples de classificação de texto, BoW ou
      TF-IDF podem ser suficientes. Entretanto, para tarefas que exigem
      uma compreensão mais profunda do contexto, abordagens mais
      avançadas como Word2Vec ou embeddings de palavras podem ser mais
      adequadas.</p>
      <h3 id="implementação-em-projetos-reais">Implementação em Projetos
      Reais</h3>
      <p>A vetorização de texto é frequentemente usada em pipelines de
      PLN para tarefas como classificação de texto, análise de
      sentimentos e sistemas de recomendação. Vamos ver um exemplo
      simples de como essas técnicas podem ser integradas em um pipeline
      completo.</p>
      <div class="sourceCode" id="cb70" data-language="Python"
      data-caption="Treino e avaliação de um modelo simples de classificação de sentimentos em textos."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install scikit-learn</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de corpus e rótulos</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">&quot;Eu amo programação&quot;</span>, <span class="st">&quot;A programação é divertida&quot;</span>, <span class="st">&quot;Eu odeio bugs&quot;</span>, <span class="st">&quot;A vida é bela&quot;</span>, <span class="st">&quot;Eu odeio erros&quot;</span>]</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># 1: Sentimento Positivo, 0: Sentimento Negativo</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Divisão dos dados em treino e teste</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(corpus, labels, test_size<span class="op">=</span><span class="fl">0.4</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Criação do pipeline TF-IDF + Classificador Naive Bayes</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> make_pipeline(TfidfVectorizer(), MultinomialNB())</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinamento do modelo</span></span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predição no conjunto de teste</span></span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> model.predict(X_test)</span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Avaliação do modelo</span></span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Relatório de Classificação:</span><span class="ch">\n</span><span class="st">&quot;</span>, metrics.classification_report(y_test, predicted))</span></code></pre></div>
      <pre><code>Relatório de Classificação:
precision     recall  f1-score  support
0             0.00    0.00      0.00         1
1             0.50    1.00      0.67         1
accuracy                        0.50         2
macro avg     0.25    0.50      0.33         2
weighted avg  0.25    0.50      0.33         2</code></pre>
      <p>Este exemplo mostra como transformar texto em vetores
      utilizando TF-IDF e depois aplicar um modelo de classificação
      simples, como o Naive Bayes. Esse pipeline pode ser adaptado para
      tarefas mais complexas, incluindo a integração de vetorização com
      modelos mais avançados.</p>
      <h3 id="embeddings-de-palavras">Embeddings de Palavras </h3>
      <p>Embeddings de palavras são representações vetoriais densas de
      palavras que capturam as relações semânticas entre elas. Ao
      contrário de técnicas como One-Hot Encoding ou Bag of Words, que
      resultam em vetores esparsos e de alta dimensionalidade,
      embeddings de palavras mapeiam palavras em um espaço vetorial de
      dimensões mais baixas, onde palavras com significados semelhantes
      estão mais próximas. Elas têm várias aplicações em PLN, tais
      como:</p>
      <ul>
      <li><p><strong>Classificação de texto</strong>: Representar
      documentos usando a média dos embeddings das palavras contidas
      nele.</p></li>
      <li><p><strong>Análise de sentimentos</strong>: Capturar nuances
      semânticas que necessárias para entender o sentimento.</p></li>
      <li><p><strong>Sistemas de recomendação</strong>: Usar embeddings
      para medir similaridade semântica entre produtos ou serviços
      descritos em texto.</p></li>
      </ul>
      <h3 id="word2vec">Word2Vec</h3>
      <p>Word2Vec, introduzido por <span class="citation"
      data-cites="mikolov2013efficient">Mikolov et al. (2013)</span> em
      2013, é uma das técnicas mais influentes para aprender embeddings
      de palavras. Existem duas abordagens principais no Word2Vec:
      <strong>Skip-gram</strong> e <strong>CBOW (Continuous Bag of
      Words)</strong>.</p>
      <p>Skip-gram: O modelo Skip-gram prevê as palavras contextuais
      (palavras ao redor) para uma palavra-alvo. A ideia é maximizar a
      probabilidade de prever palavras no contexto de uma palavra-alvo
      específica.</p>
      <p>CBOW (Continuous Bag of Words): O modelo CBOW, ao contrário do
      Skip-gram, prevê a palavra-alvo com base no contexto. Este modelo
      é útil para capturar o sentido de uma palavra com base em seu
      ambiente.</p>
      <p><strong>Implementação do Word2Vec em Python</strong>: Vamos
      utilizar a biblioteca <code>gensim</code>, que fornece uma
      implementação eficiente do Word2Vec.</p>
      <div class="sourceCode" id="cb72" data-language="Python"
      data-caption="Treina um modelo de Word2Vec usando o corpus news do Brown (disponível no NLTK) para gerar representações vetoriais de palavras."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install gensim nltk   </span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> brown</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o corpus de exemplo (Brown corpus)</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> brown.sents(categories<span class="op">=</span><span class="st">&#39;news&#39;</span>)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinamento do modelo Word2Vec</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">5</span>, sg<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenção de vetor para uma palavra</span></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>vector <span class="op">=</span> model.wv[<span class="st">&#39;economy&#39;</span>]</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vetor para &#39;economy&#39;:&quot;</span>, vector)</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Encontrando palavras semelhantes</span></span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.wv.most_similar(<span class="st">&#39;economy&#39;</span>)</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Palavras semelhantes a &#39;economy&#39;:&quot;</span>, similar_words)</span></code></pre></div>
      <pre><code>Vetor para &#39;economy&#39;: [-0.03323277  0.03501032 -0.00576794  0.0111884  -0.03380066 -0.11331949
0.01821606  0.10907217 -0.07971378 -0.06527615  0.03423027 -0.05620026
[.....]
0.08788168  0.04192578 -0.03881291 -0.02442176  0.15587321  0.05784223
0.05759267 -0.07984415 -0.02128893 -0.00980771]
Palavras semelhantes a &#39;economy&#39;: [(&#39;additional&#39;, 0.9950711727142334), (&#39;provide&#39;, 0.9949660301208496), 
(&#39;administration&#39;, 0.9946332573890686), (&#39;times&#39;, 0.9946231245994568), (&#39;can&#39;, 0.9946150183677673), 
(&#39;laws&#39;, 0.9945879578590393), (&#39;business&#39;, 0.9945263266563416), (&#39;could&#39;, 0.9944885969161987), 
(&#39;how&#39;, 0.9944777488708496), (&#39;made&#39;, 0.994467556476593)]</code></pre>
      <p><strong>Parâmetros Importantes</strong> -
      <strong>vector_size</strong>: Dimensionalidade dos vetores de
      palavras. - <strong>window</strong>: Tamanho da janela de
      contexto. - <strong>min_count</strong>: Mínimo de ocorrências para
      uma palavra ser considerada no treinamento. - <strong>sg</strong>:
      Se 0, usa CBOW; se 1, usa Skip-gram.</p>
      <p><strong>Explorando as Relações Semânticas</strong></p>
      <p>Embeddings de palavras como o Word2Vec capturam relações
      semânticas interessantes, como analogias.</p>
      <div class="sourceCode" id="cb74" data-language="Python"
      data-caption="Treina um modelo de Word2Vec e faz uma analogia de palavras: &quot;king&quot; - &quot;man&quot; + &quot;woman&quot;, buscando o termo mais próximo desse contexto."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install gensim nltk   </span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> brown</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o corpus de exemplo (Brown corpus)</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> brown.sents(categories<span class="op">=</span><span class="st">&#39;news&#39;</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinamento do modelo Word2Vec</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Analogias: &quot;rei&quot; - &quot;homem&quot; + &quot;mulher&quot; = ?</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.wv.most_similar(positive<span class="op">=</span>[<span class="st">&#39;king&#39;</span>, <span class="st">&#39;woman&#39;</span>], negative<span class="op">=</span>[<span class="st">&#39;man&#39;</span>])</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Resultado da analogia:&quot;</span>, result)</span></code></pre></div>
      <pre><code>Resultado da analogia: [(&#39;kid&#39;, 0.8575360178947449), (&#39;missed&#39;, 0.8566135764122009), (&#39;load&#39;, 0.854703426361084), (&#39;normally&#39;, 0.8536292314529419), (&#39;advisers&#39;, 0.8517050743103027), (&#39;Place&#39;, 0.8516084551811218), (&#39;Vieth&#39;, 0.8513892292976379), (&#39;yield&#39;, 0.8511804938316345), (&#39;decline&#39;, 0.8500499725341797), (&#39;Coe&#39;, 0.8490973114967346)]</code></pre>
      <h3 id="glove">GloVe</h3>
      <p>Embora o Word2Vec seja amplamente utilizado, outros modelos
      também oferecem técnicas avançadas para aprender embeddings de
      palavras. O GloVe, desenvolvido por <span class="citation"
      data-cites="pennington2014glove">Pennington, Socher, and Manning
      (2014)</span>. em 2014, é uma abordagem baseada em matrizes de
      ocorrência que combina as vantagens do Word2Vec com informações
      globais sobre o corpus. Este código a seguir demora um pouco; ele
      fará o download de um arquivo para seu computador, se estiver
      usando o Python localmente.</p>
      <div class="sourceCode" id="cb76" data-language="Python"
      data-caption="Faz o download dos embeddings de palavras GloVe e extrai o arquivo necessário, carrega os vetores e obtém o vetor de características para a palavra economy."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install requests</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install gensim</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> zipfile</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">&quot;https://nlp.stanford.edu/data/glove.6B.zip&quot;</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>zip_filename <span class="op">=</span> <span class="st">&quot;glove.6B.zip&quot;</span></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>txt_filename <span class="op">=</span> <span class="st">&quot;glove.6B.50d.txt&quot;</span></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Baixar o arquivo zip se não existir</span></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(zip_filename):</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Baixando embeddings GloVe 6B...&quot;</span>)</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> r:</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>        r.raise_for_status()</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(zip_filename, <span class="st">&quot;wb&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> chunk <span class="kw">in</span> r.iter_content(chunk_size<span class="op">=</span><span class="dv">8192</span>):</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>                f.write(chunk)</span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Download concluído.&quot;</span>)</span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Extrair o arquivo txt se não existir</span></span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(txt_filename):</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Extraindo glove.6B.50d.txt...&quot;</span>)</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> zipfile.ZipFile(zip_filename, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> zip_ref:</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>        zip_ref.extract(txt_filename)</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Extração concluída.&quot;</span>)</span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar embeddings GloVe</span></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a>glove_model <span class="op">=</span> KeyedVectors.load_word2vec_format(txt_filename, binary<span class="op">=</span><span class="va">False</span>, no_header<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenção de vetor para uma palavra</span></span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>vector_glove <span class="op">=</span> glove_model[<span class="st">&#39;economy&#39;</span>]</span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vetor para &#39;economy&#39; com GloVe:&quot;</span>, vector_glove)</span></code></pre></div>
      <pre><code>Conversão concluída.
Vetor para &#39;economy&#39; com GloVe: [-1.2027e-01 -7.2505e-01  8.7014e-01 -6.3944e-01  1.7259e-01 -3.5168e-01 [...] 4.9483e-01 -1.0151e+00  8.9959e-02  4.6090e-01  1.7585e-03  6.2182e-01
1.1893e+00  8.4410e-02]</code></pre>
      <h3 id="fasttext">FastText</h3>
      <p>O FastText, desenvolvido pelo Facebook AI Research, estende o
      Word2Vec ao considerar subpalavras, o que permite gerar embeddings
      para palavras que não estão no vocabulário, lidando melhor com
      palavras raras e morfologicamente ricas.</p>
      <div class="sourceCode" id="cb78" data-language="Python"
      data-caption="Este código treina um modelo de linguagem FastText usando o corpus news do Brown."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install gensim nltk</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> brown</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o corpus de exemplo (Brown corpus)</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> brown.sents(categories<span class="op">=</span><span class="st">&#39;news&#39;</span>)</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinamento do modelo FastText</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>fasttext_model <span class="op">=</span> gensim.models.FastText(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenção de vetor para uma palavra</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>vector_fasttext <span class="op">=</span> fasttext_model.wv[<span class="st">&#39;economy&#39;</span>]</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vetor para &#39;economy&#39; com FastText:&quot;</span>, vector_fasttext)</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Encontrando palavras semelhantes</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>similar_words_ft <span class="op">=</span> fasttext_model.wv.most_similar(<span class="st">&#39;economy&#39;</span>)</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Palavras semelhantes a &#39;economy&#39; com FastText:&quot;</span>, similar_words_ft)</span></code></pre></div>
      <pre><code>Vetor para &#39;economy&#39; com FastText: [-7.31775016e-02  2.45773837e-01 -6.83162957e-02 -2.00441748e-01
5.88673912e-02  4.55363989e-02 -7.13645061e-03  1.42574251e-01
[...]
9.44181085e-02 -1.94346443e-01 -2.52598636e-02  5.90402260e-02]
Palavras semelhantes a &#39;economy&#39; com FastText: [(&#39;economic&#39;, 0.9999745488166809), (&#39;province&#39;, 0.9999632835388184), (&#39;recommended&#39;, 0.9999628067016602), (&#39;secretary&#39;, 0.9999626278877258), (&#39;generally&#39;, 0.9999624490737915), (&#39;defensive&#39;, 0.9999622106552124), (&#39;often&#39;, 0.9999621510505676), (&#39;concerned&#39;, 0.9999621510505676), (&#39;maintenance&#39;, 0.9999617338180542), (&#39;primary&#39;, 0.9999616742134094)]</code></pre>
      <h2 id="resumo-1">Resumo</h2>
      <p>Nesta seção, cobrimos os fundamentos de Processamento de
      Linguagem Natural (PLN), incluindo tokenização, lemmatização,
      stemização e remoção de stopwords. Também vimos como implementar
      essas técnicas em Python, utilizando as bibliotecas NLTK e
      SpaCy.</p>
      <p>Além disso, explicamos o que são Intents, um conceito central
      na arquitetura de chatbots modernos baseados em NLU (Natural
      Language Understanding). Eles representam o objetivo do usuário e
      permitem que o chatbot compreenda a intenção por trás das
      mensagens para agir de forma adequada. Os Intents estão
      intrinsecamente ligados a outros conceitos fundamentais: (i)
      Entities: Fornecem os detalhes específicos dentro de um intent.
      (ii) Utterances: São as diversas maneiras como um usuário pode
      expressar um mesmo intent. (iii) Actions/Responses: São as tarefas
      ou respostas que o chatbot executa após identificar um intent. A
      definição cuidadosa, o treinamento robusto e o gerenciamento
      contínuo dos intents são relevantes para a eficácia, a
      inteligência e a qualidade da experiência do usuário oferecida por
      um chatbot.</p>
      <p>Também exploramos várias técnicas de vetorização de texto,
      desde as mais simples, como One-Hot Encoding, até abordagens mais
      sofisticadas, como TF-IDF. Com exemplos práticos em Python,
      demonstramos como essas técnicas podem ser aplicadas em pipelines
      de PLN. Além disso, abordamos o conceito de embeddings de
      palavras, com ênfase no Word2Vec, e exploramos outras técnicas
      como GloVe e FastText. Demonstramos como esses embeddings capturam
      relações semânticas entre palavras e suas aplicações em tarefas de
      PLN.</p>
      <h2 id="exercícios-2">Exercícios</h2>
      <ol>
      <li><p><strong>O que é vetorização de texto no contexto de
      processamento de linguagem natural?</strong></p>
      <ol>
      <li><p>A compressão de textos longos para reduzir o tamanho dos
      arquivos.</p></li>
      <li><p>A transformação de palavras ou documentos em representações
      numéricas.</p></li>
      <li><p>A categorização de textos em diferentes classes.</p></li>
      <li><p>A separação de um texto em frases ou parágrafos
      distintos.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes técnicas de vetorização é
      baseada na frequência de termos em um documento?</strong></p>
      <ol>
      <li><p>Word2Vec</p></li>
      <li><p>TF-IDF</p></li>
      <li><p>Bag of Words (BoW)</p></li>
      <li><p>Embeddings de Palavras</p></li>
      </ol></li>
      <li><p><strong>O que é a técnica Bag of Words (BoW)?</strong></p>
      <ol>
      <li><p>Um método de combinar várias palavras em uma única
      representação vetorial.</p></li>
      <li><p>Um método de contar a frequência de palavras em um
      documento sem considerar a ordem das palavras.</p></li>
      <li><p>Um algoritmo de compressão de texto.</p></li>
      <li><p>Uma técnica para traduzir texto entre diferentes
      idiomas.</p></li>
      </ol></li>
      <li><p><strong>Qual das alternativas a seguir é uma desvantagem do
      modelo Bag of Words?</strong></p>
      <ol>
      <li><p>Ele não consegue capturar a ordem das palavras.</p></li>
      <li><p>Ele é muito difícil de implementar.</p></li>
      <li><p>Ele não suporta múltiplos idiomas.</p></li>
      <li><p>Ele exige uma grande quantidade de dados para
      funcionar.</p></li>
      </ol></li>
      <li><p><strong>O que é um embedding de palavras?</strong></p>
      <ol>
      <li><p>Uma matriz de números binários que representa a presença ou
      ausência de palavras em um texto.</p></li>
      <li><p>Um vetor de números que representa o significado semântico
      de uma palavra no contexto de um grande corpus de textos.</p></li>
      <li><p>Um conjunto de palavras agrupadas por temas
      semelhantes.</p></li>
      <li><p>Uma técnica para compactar arquivos de texto.</p></li>
      </ol></li>
      <li><p><strong>Qual é a principal diferença entre o modelo
      Word2Vec e a técnica TF-IDF?</strong></p>
      <ol>
      <li><p>Word2Vec captura relações semânticas entre palavras,
      enquanto TF-IDF mede a importância de uma palavra em um
      documento.</p></li>
      <li><p>TF-IDF é um modelo de aprendizado profundo, enquanto
      Word2Vec não é.</p></li>
      <li><p>Word2Vec utiliza a frequência de termos em um documento,
      enquanto TF-IDF utiliza redes neurais.</p></li>
      <li><p>Word2Vec é usado apenas para classificação de texto,
      enquanto TF-IDF é usado para tradução automática.</p></li>
      </ol></li>
      <li><p><strong>O que significa a sigla TF-IDF?</strong></p>
      <ol>
      <li><p>Term Frequency - Inverse Document Frequency</p></li>
      <li><p>Term Frequency - Inverse Data Frequency</p></li>
      <li><p>Total Frequency - Inverse Document Frequency</p></li>
      <li><p>Text Frequency - Inverse Data Frequency</p></li>
      </ol></li>
      <li><p><strong>No contexto do Word2Vec, o que significa
      "skip-gram"?</strong></p>
      <ol>
      <li><p>Um método de usar uma palavra central para prever palavras
      de contexto ao seu redor.</p></li>
      <li><p>Um método de ignorar palavras raras em um texto.</p></li>
      <li><p>Um algoritmo para classificar palavras por
      frequência.</p></li>
      <li><p>Uma técnica para comprimir textos longos.</p></li>
      </ol></li>
      <li><p><strong>Qual é o principal objetivo do modelo
      TF-IDF?</strong></p>
      <ol>
      <li><p>Classificar documentos por sua relevância.</p></li>
      <li><p>Converter palavras em vetores numéricos que capturam seu
      significado semântico.</p></li>
      <li><p>Identificar as palavras mais importantes em um documento,
      levando em conta a frequência dessas palavras em um conjunto de
      documentos.</p></li>
      <li><p>Traduzir automaticamente textos entre diferentes
      idiomas.</p></li>
      </ol></li>
      <li><p><strong>Como o modelo Word2Vec aprende a representar
      palavras em um espaço vetorial?</strong></p>
      <ol>
      <li><p>Através da análise de frequência de palavras em um único
      documento.</p></li>
      <li><p>Usando a coocorrência de palavras em um grande corpus de
      textos, para aprender vetores que representam o significado
      semântico das palavras.</p></li>
      <li><p>Convertendo palavras em números aleatórios e ajustando-os
      conforme a necessidade.</p></li>
      <li><p>Usando apenas o contexto imediato de uma palavra em uma
      frase.</p></li>
      </ol></li>
      <li><p><strong>O que é ajuste fino (fine-tuning) de um modelo
      pré-treinado?</strong></p>
      <ol>
      <li><p>A criação de um novo modelo a partir do zero.</p></li>
      <li><p>A adaptação de um modelo pré-treinado para uma tarefa
      específica usando um conjunto de dados menor e
      específico.</p></li>
      <li><p>A otimização de hiperparâmetros de um modelo sem modificar
      seus pesos.</p></li>
      <li><p>O treinamento de um modelo exclusivamente em dados de alta
      qualidade.</p></li>
      </ol></li>
      </ol>
      <h1 id="cap:LLM">Modelos de Linguagem Grande (LLM)</h1>
      <div class="epigraph">
      <p>“A inteligência desses sistemas é uma miragem — o que se vê não
      é compreensão, mas um ajuste estatístico de padrões.”</p>
      <p>Emily Bender</p>
      </div>
      <div class="myboxobj">
      <p><span>Objetivo</span> Discutir a arquitetura de
      <em>Transformers</em>, modelos como BERT, GPT e LLaMA,
      <em>Fine-Tuning</em> e RAG, mostrando como essas técnicas são
      aplicadas na construção de chatbots modernos.</p>
      </div>
      <h2 id="introdução-2">Introdução</h2>
      <p>Um <em>Large Language Model</em> (LLM) - em inglês, Large
      Language Models - pode ser definido como um sistema computacional
      fundamentado em técnicas de aprendizado de máquina, cuja
      finalidade consiste em gerar texto a partir de uma sequência
      textual fornecida como entrada. Nesse contexto, o texto de entrada
      é denominado <em>prompt</em>, também referido em inglês como
      <em>input</em>, enquanto o texto produzido pelo modelo é
      denominado resposta, ou <em>output</em>. A figura a seguir
      apresenta uma representação esquemática do processo de entrada e
      saída de texto em um LLM.</p>
      <figure id="fig:placeholder2">
      <p><img src="fig/llm1.png" style="width:90.0%" alt="image" />
      <span id="fig:placeholder2"
      data-label="fig:placeholder2"></span></p>
      <figcaption>Figure 19: Fluxo entrada e saída do LLM.</figcaption>
      </figure>
      <p>Um LLM nada mais é do que um tipo específico de rede neural
      artificial (a rede neural profunda) treinada com dados textuais.
      Em outras palavras, uma grande rede neural treinada com muitos
      dados. O conceito de “grande rede neural” está relacionado à
      quantidade de parâmetros que compõem a rede, na casa de milhões em
      diante; já em relação a “muitos dados” refere-se a dezenas de
      gigabytes em diante, abrangendo conjuntos de dados brutos que
      podem incluir livros, artigos, páginas da web, documentos técnicos
      e outros materiais escritos.</p>
      <p>O processo de treino envolve duas etapas (veja Figura 4): (i)
      um treinamento com dados textuais brutos e (ii) um treinamento com
      dados textuais anotados (ou seja, mais ajustados) às necessidades
      de um LLM <span class="citation"
      data-cites="raschka2024build">(Raschka 2024)</span>.</p>
      <figure id="fig:placeholder">
      <p><img src="fig/llm.png" style="width:90.0%" alt="image" /> <span
      id="fig:placeholder" data-label="fig:placeholder"></span></p>
      <figcaption>Figure 20: Alto nível do processo de treino de um
      modelo de LLM genérico.</figcaption>
      </figure>
      <p>Quando o LLM é treinado com dados brutos, ele é denominado
      modelo “base” e tem como objetivo prever a próxima sequência de
      palavras a partir do <em>prompt</em> de entrada. Por exemplo, para
      o <em>prompt</em>: O “livro está”, a resposta de um modelo
      <em>base</em> poderia ser: “O livro está em cima da mesa.”. A
      seguir, no Exemplo 1, apresenta-se um conjunto ilustrativo de
      dados textuais de treino.</p>
      <div class="sourceCode" id="cb80" data-language="Python"
      data-caption="Exemplo 1 - Dados textuais brutos para a primeira etapa do treino, omitindo alguns detalhes."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco.&quot;</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>Trecho de Assis, Machado. Todos os Romances: Machado de Assis (Portuguese Edition) (p. <span class="dv">226</span>). Edição do Kindle.</span></code></pre></div>
      <p>Na segunda etapa, o modelo <em>base</em> é treinado novamente,
      mas agora com dados de texto anotados (por humanos, muitas vezes),
      o que permite ao modelo aprender não apenas padrões linguísticos
      gerais, mas também instruções explícitas de interação. Veja o
      exemplo de dados textuais de treino no Exemplo 2.</p>
      <div class="sourceCode" id="cb81" data-language="Python"
      data-caption="Exemplo 2 - Dados textuais anotados, omitindo alguns detalhes."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>user: <span class="st">&quot;Quem descobriu o Brasil?&quot;</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>bot: <span class="st">&quot;Que boa pergunta. Quem descobriu o Brasil foi Pedro Álvares Cabral. Você quer saber mais sobre o Brasil?&quot;</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>user: <span class="st">&quot;traduza o trecho The book is on the table&quot;</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>bot: <span class="st">&quot;A tradução é: O livro está na mesa. Gostaria de saber mais alguma coisa?&quot;</span></span></code></pre></div>
      <p>Os Exemplos 1 e 2 foram construídos apenas para fins didáticos,
      com o objetivo de ilustrar, de maneira simplificada, como trechos
      textuais podem ser utilizados no treinamento de um modelo de
      linguagem. Em aplicações reais, contudo, o processo é muito mais
      complexo: envolve conjuntos massivos de dados textuais,
      frequentemente com bilhões de palavras provenientes de fontes
      diversas e organizados com formatações específicas. A execução
      desse treinamento em larga escala só se tornou possível a partir
      da arquitetura <em>Transformer</em>, que introduziu o mecanismo de
      atenção capaz de lidar, de forma eficiente e paralela, com
      dependências de longo alcance em sequências de texto.</p>
      <p>Modelos de linguagem de larga escala são treinados justamente
      para prever a próxima palavra em uma sequência, habilidade que
      serve de base para a construção de representações linguísticas
      sofisticadas. Esse processo permite que os modelos compreendam
      contextos extensos e realizem tarefas complexas de PLN, como
      tradução automática, sumarização de documentos e resposta a
      perguntas. A capacidade de capturar relações de longo alcance nos
      textos é um dos diferenciais desses modelos em relação a
      arquiteturas anteriores.</p>
      <p>A introdução dos <em>Transformers</em>, proposta por <span
      class="citation" data-cites="vaswani2017attention">Vaswani et al.
      (2017)</span> no artigo seminal <em>“Attention is All You
      Need”</em> <span class="citation"
      data-cites="vaswani2017attention">(Vaswani et al. 2017)</span>,
      representou uma mudança fundamental no campo. A principal inovação
      foi o mecanismo de atenção, que atribui pesos diferentes às
      palavras de entrada de acordo com seu contexto, permitindo ao
      modelo identificar quais termos são mais relevantes em cada
      situação. Essa característica torna possível o processamento
      paralelo de sequências longas, acelerando substancialmente o
      treinamento e aumentando a eficácia na modelagem de dependências
      distantes.</p>
      <p>Antes dessa arquitetura, os modelos de PLN eram dominados por
      RNNs (<em>Recurrent Neural Networks</em>) e LSTMs (<em>Long
      Short-Term Memory Networks</em>). Embora funcionais, esses métodos
      apresentavam limitações significativas: dificuldade em capturar
      dependências de longo alcance devido ao problema do <em>vanishing
      gradient</em> e restrições de desempenho, já que processavam o
      texto de maneira estritamente sequencial, reduzindo o potencial de
      paralelização. O <em>Transformer</em> superou essas limitações ao
      considerar todas as palavras de entrada simultaneamente,
      ponderando sua importância relativa por meio da atenção.</p>
      <p>Com isso, os LLMs não apenas aumentaram a precisão em tarefas
      tradicionais de PLN, como também ampliaram o escopo de aplicações,
      abrangendo geração criativa de texto, tradução contextualizada e
      correção gramatical automática. Por outro lado, o avanço desses
      modelos também trouxe questionamentos relacionados à ética, viés e
      uso responsável da inteligência artificial, aspectos que vêm sendo
      discutidos intensamente pela comunidade científica e que serão
      abordados no Capítulo <a href="#conclusao"
      data-reference-type="ref" data-reference="conclusao">6</a>.</p>
      <h2 id="arquitetura-geral-do-transformer">Arquitetura Geral do
      Transformer</h2>
      <p>Os LLMs modernos são em grande parte fundamentados na
      arquitetura <em>Transformer</em>, proposta originalmente por <span
      class="citation" data-cites="vaswani2017attention">Vaswani et al.
      (2017)</span>. Essa arquitetura se tornou a base de inúmeros
      avanços no processamento de linguagem natural porque oferece uma
      forma eficiente e escalável de lidar com sequências de texto.
      Diferente de abordagens anteriores, que processavam as palavras
      uma a uma em ordem, os <em>Transformers</em> permitem que todo o
      contexto seja considerado em paralelo, o que torna o treinamento e
      a inferência mais rápidos e flexíveis.</p>
      <p>A estrutura geral de um <em>Transformer</em> é composta por
      blocos que se repetem, cada um deles combinando mecanismos de
      atenção e redes neurais totalmente conectadas
      (<em>feedforward</em>). Em sua formulação original, a arquitetura
      possui dois grandes componentes: o <strong>encoder</strong>,
      responsável por transformar a sequência de entrada em uma
      representação interna, e o <strong>decoder</strong>, que utiliza
      essa representação para gerar uma saída, como no caso da tradução
      automática. Embora alguns modelos atuais utilizem apenas a parte
      do encoder (como o BERT) ou apenas o decoder (como o GPT), o
      princípio fundamental continua o mesmo: ambos dependem
      intensamente do mecanismo de atenção.</p>
      <p>Cada camada do encoder e do decoder contém três elementos
      centrais:</p>
      <ul>
      <li><p><strong>Mecanismo de atenção múltipla (Multi-Head
      Attention)</strong>: avalia, em paralelo, diferentes formas de
      relacionar as palavras entre si.</p></li>
      <li><p><strong>Redes <em>feedforward</em></strong>: transformam as
      representações intermediárias, permitindo maior capacidade de
      modelagem.</p></li>
      <li><p><strong>Normalização e conexões residuais</strong>:
      estabilizam o treinamento e ajudam a preservar informações ao
      longo das camadas.</p></li>
      </ul>
      <p>O conceito de <strong>self-attention</strong> é um dos pontos
      mais inovadores. Nesse mecanismo, cada palavra da sequência não é
      processada isoladamente, mas em comparação com todas as outras
      palavras da mesma sequência. Isso permite que o modelo entenda
      relações de dependência de longo alcance, como entre o sujeito no
      início de uma frase e o verbo que aparece muito depois. O
      <strong>multi-head attention</strong> amplia essa ideia ao aplicar
      várias atenções em paralelo, cada uma aprendendo a capturar um
      tipo de relação — algumas cabeças podem identificar proximidade
      sintática, enquanto outras captam conexões semânticas mais
      distantes.</p>
      <p>Um aspecto central da arquitetura é que todo esse processamento
      ocorre de forma paralela. Diferente das redes recorrentes
      (<em>RNNs</em>) ou das LSTMs, que analisavam o texto palavra por
      palavra em sequência, o <em>Transformer</em> avalia a frase
      inteira de uma só vez. Essa mudança de paradigma permitiu que
      modelos fossem treinados em coleções massivas de texto, chegando a
      bilhões de parâmetros e escalando para tamanhos sem
      precedentes.</p>
      <p>Além de sua eficiência estrutural, o <em>Transformer</em>
      também é flexível: pode ser adaptado a diferentes tarefas com
      pequenas modificações. Modelos <strong>autoregressivos</strong>,
      como a família GPT, são baseados apenas no decoder e são treinados
      para prever a próxima palavra em uma sequência de forma
      unidirecional. Já os modelos <strong>bidirecionais</strong>, como
      o BERT, exploram tanto o contexto à esquerda quanto o à direita, o
      que os torna particularmente eficazes em tarefas de compreensão
      textual.</p>
      <p>O impacto da arquitetura <em>Transformer</em> é amplo e vai
      além do processamento de linguagem natural em sua forma mais
      básica. A combinação entre paralelismo, capacidade de capturar
      dependências de longo alcance e flexibilidade estrutural abriu
      caminho para a criação de famílias inteiras de modelos com
      diferentes finalidades. Alguns deles utilizam apenas o encoder,
      outros apenas o decoder, e há ainda os que combinam os dois de
      formas distintas. Essa diversidade deu origem a arquiteturas
      conhecidas, como BERT, GPT, T5 e LLaMA, que se tornaram
      referências na área e serão discutidas a seguir em maior
      detalhe.</p>
      <h3
      id="bert-bidirectional-encoder-representations-from-transformers">BERT
      (<em>Bidirectional Encoder Representations from
      Transformers</em>)</h3>
      <p>O BERT (<em>Bidirectional Encoder Representations from
      Transformers</em>) introduziu uma nova forma de treinamento em
      larga escala, na qual o modelo considera tanto o contexto à
      esquerda quanto o contexto à direita de uma palavra-alvo. Essa
      abordagem permitiu uma compreensão mais rica da linguagem em
      comparação com modelos anteriores, que eram predominantemente
      unidirecionais.</p>
      <p>O BERT é construído sobre a pilha de <em>encoders</em> do
      <em>Transformer</em>. Isso significa que ele não é projetado para
      gerar texto de forma autoregressiva, mas sim para produzir
      representações contextuais profundas de palavras e frases, que
      podem depois ser utilizadas em diversas tarefas de PLN. Essa
      característica torna o BERT especialmente adequado para problemas
      de compreensão textual, como classificação, resposta a perguntas e
      extração de informações.</p>
      <p>O modelo é pré-treinado em uma grande quantidade de texto
      utilizando duas tarefas principais:</p>
      <ul>
      <li><p><strong>Masked Language Modeling (MLM)</strong>: Palavras
      aleatórias em uma sequência são substituídas por um token especial
      de máscara, e o modelo é treinado para prever as palavras
      originais com base no restante do contexto. Esse procedimento
      permite que o BERT aprenda relações bidirecionais, uma vez que a
      previsão de uma palavra depende de termos anteriores e posteriores
      na frase.</p></li>
      <li><p><strong>Next Sentence Prediction (NSP)</strong>: O modelo
      recebe pares de frases e deve identificar se a segunda frase segue
      logicamente a primeira. Essa tarefa ajuda o BERT a capturar
      relações discursivas entre sentenças, algo essencial em aplicações
      como resposta a perguntas e inferência textual.</p></li>
      </ul>
      <p>Após o pré-treinamento, o BERT pode ser ajustado finamente
      (<em>Fine-Tuning</em>) para tarefas específicas, muitas vezes com
      datasets muito menores que aqueles usados no pré-treinamento. Esse
      processo de adaptação é o que permite que um único modelo,
      previamente treinado de maneira genérica, seja aplicado em uma
      ampla gama de problemas.</p>
      <div class="sourceCode" id="cb82" data-language="Python"
      data-caption="Uso do BERT."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> softmax</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BERT</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained( <span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained( <span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">&quot;Chatbots são muito úteis para automação.&quot;</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(input_text, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predição</span></span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>input_ids)</span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> softmax(outputs.logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probabilidades de classe:&quot;</span>, probs)</span></code></pre></div>
      <pre><code>Probabilidades de classe: tensor([[0.7097, 0.2903]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
      <p>O BERT rapidamente estabeleceu novos patamares em benchmarks de
      PLN, como GLUE (General Language Understanding Evaluation) e SQuAD
      (Stanford Question Answering Dataset). Sua eficácia impulsionou o
      desenvolvimento de uma série de variações e aprimoramentos, como
      RoBERTa, DistilBERT e ALBERT, que buscaram melhorar desempenho,
      eficiência ou reduzir custos de treinamento.</p>
      <p>O BERT é amplamente utilizado em:</p>
      <ul>
      <li><p><strong>Classificação de Texto</strong>: Análise de
      sentimento, detecção de spam, categorização de
      documentos.</p></li>
      <li><p><strong>Respostas a Perguntas</strong>: Modelos que
      identificam trechos relevantes em um texto para responder
      perguntas formuladas em linguagem natural.</p></li>
      <li><p><strong>Extração de Informações</strong>: Identificação de
      entidades nomeadas (pessoas, lugares, organizações) e relações
      entre elas.</p></li>
      </ul>
      <p>Apesar de seu impacto, o BERT apresenta limitações relevantes.
      O modelo é pesado em termos computacionais, exigindo grande
      capacidade de processamento para treinamento e mesmo para
      inferência em aplicações práticas. Além disso, sua janela de
      contexto é limitada, o que dificulta o processamento de documentos
      muito extensos sem estratégias adicionais de segmentação.</p>
      <div class="sourceCode" id="cb84" data-language="Python"
      data-caption="Exemplo das representações do BERT."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BERT pré-treinado</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained( <span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained( <span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>texto <span class="op">=</span> <span class="st">&quot;Machine learning is fascinating.&quot;</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer(texto, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)[<span class="st">&#39;input_ids&#39;</span>]</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtenção das representações do modelo BERT</span></span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(input_ids)</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>last_hidden_states <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Representações BERT:&quot;</span>, last_hidden_states)</span></code></pre></div>
      <pre><code>Representações BERT: tensor([[[ 0.0836,  0.0931, -0.2452,  ..., -0.3743,  0.0502,  0.5606],
[ 0.2351,  0.0642, -0.1859,  ..., -0.1147,  0.5381,  0.5704],
...,
[ 0.8129,  0.1013, -0.2131,  ...,  0.0133, -0.6965, -0.0085]]],
grad_fn=&lt;NativeLayerNormBackward0&gt;)</code></pre>
      <h3 id="gpt-generative-pre-trained-transformer">GPT
      (<em>Generative Pre-trained Transformer</em>)</h3>
      <p>O <em>Generative Pre-trained Transformer</em> (GPT) é um dos
      LLMs mais conhecidos. Ele é treinado de forma autoregressiva, o
      que significa que prediz a próxima palavra em uma sequência, dada
      a entrada anterior. Isso o torna excelente para tarefas de geração
      de texto. O GPT 2 foi uma versão inicial do GPT, contendo 1,5
      bilhões de parâmetros. Ele mostrou que, ao ser treinado em grandes
      quantidades de texto, poderia gerar conteúdo coerente e complexo.
      GPT é uma evolução ainda maior, com 175 bilhões de parâmetros.
      Esse modelo pode realizar uma ampla gama de tarefas de PLN sem a
      necessidade de ajustes finos específicos, simplesmente recebendo
      exemplos de como a tarefa deve ser executada (aprendizado por
      poucos exemplos, ou few-shot learning).</p>
      <div class="sourceCode" id="cb86" data-language="Python"
      data-caption="Geração de texto com GPT-2."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo GPT-2</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">&#39;gpt2&#39;</span>)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">&#39;gpt2&#39;</span>)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">&quot;Chatbots modernos podem&quot;</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Texto gerado:&quot;</span>, generated_text)</span></code></pre></div>
      <pre><code>Texto gerado: Chatbots modernos podemodel.com The following is a list of all the bots that have been added to the podemodel.com community. Bot Name Description bot_bot_name_id bot_</code></pre>
      <p>O GPT tem sido aplicado em diversas áreas, incluindo:</p>
      <ul>
      <li><p><strong>Geração de Texto</strong>: Criação de conteúdo,
      histórias, artigos, etc.</p></li>
      <li><p><strong>Assistentes Virtuais</strong>: Implementação de
      sistemas de diálogo baseados em IA.</p></li>
      <li><p><strong>Tradução Automática</strong>: Utilização de
      contexto amplo para melhorar a tradução entre idiomas.</p></li>
      </ul>
      <p>O GPT é um modelo autoregressivo que se concentra na geração de
      texto. É treinado para prever a próxima palavra em uma sequência,
      o que o torna excelente para tarefas de geração de texto, como
      chatbots.</p>
      <div class="sourceCode" id="cb88" data-language="Python"
      data-caption="Outro exemplo de geração de texto com GPT-2."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo GPT-2 pré-treinado</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">&#39;gpt2&#39;</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">&#39;gpt2&#39;</span>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de texto</span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">&quot;In the future, artificial intelligence will&quot;</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Texto gerado:&quot;</span>, generated_text)</span></code></pre></div>
      <pre><code>Texto gerado: In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people. We&#39;re going to see a lot of things that are going to be very interesting in</code></pre>
      <p>Além de BERT e GPT, há muitos outros modelos baseados em
      <em>Transformers</em> projetados para tarefas específicas. Alguns
      exemplos incluem o T5 (<em>Text-To-Text Transfer Transformer</em>)
      que converte qualquer tarefa de PLN em um problema de tradução; o
      XLNet que combina ideias de BERT e <em>Transformers</em>
      autoregressivos para melhorar a modelagem de dependências de longo
      alcance; o RoBERTa (A Robustly Optimized BERT Pretraining
      Approach) que é uma variação do BERT com treinamento aprimorado,
      além do DistilBERT.</p>
      <p>O T5 transforma qualquer tarefa de PLN em um problema de
      tradução, onde a entrada e a saída são tratadas como texto. Isso
      simplifica o Fine-Tuning para diferentes tarefas.</p>
      <div class="sourceCode" id="cb90" data-language="Python"
      data-caption="Realiza uma tradução automática de uma frase usando o T5."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install sentencepiece</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> T5Tokenizer, T5ForConditionalGeneration</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo T5</span></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained( <span class="st">&#39;t5-small&#39;</span>)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained( <span class="st">&#39;t5-small&#39;</span>)</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">&quot;translate English to German: The weather is nice today.&quot;</span></span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids)</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Tradução gerada:&quot;</span>, generated_text)</span></code></pre></div>
      <pre><code>Tradução gerada: Das Wetter ist heute schon.</code></pre>
      <p>Cabe ressaltar ainda o XLNet e o DistilBERT. O XLNet combina
      vantagens dos modelos autoregressivos e bidirecionais, como o GPT
      e BERT, para capturar dependências de longo alcance de forma mais
      eficiente. Já o DistilBERT é uma versão reduzida do BERT, com
      menos parâmetros, mas mantendo uma alta performance, o que o torna
      mais eficiente para uso em produção.</p>
      <div class="sourceCode" id="cb92" data-language="Python"
      data-caption="um exemplo de geração de texto com IA usando XLNet."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> XLNetTokenizer, XLNetLMHeadModel</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo XLNet</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> XLNetTokenizer.from_pretrained( <span class="st">&#39;xlnet-base-cased&#39;</span>)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> XLNetLMHeadModel.from_pretrained( <span class="st">&#39;xlnet-base-cased&#39;</span>)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada de exemplo</span></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">&quot;Natural Language Processing is&quot;</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenização</span></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração de texto</span></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Decodificação e exibição do texto gerado</span></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Texto gerado com XLNet:&quot;</span>, generated_text)</span></code></pre></div>
      <pre><code>Texto gerado com XLNet: Natural Language Processing is and in and in [...]</code></pre>
      <h3 id="distilbert-base-uncased">distilbert-base-uncased</h3>
      <p>O modelo distilbert-base-uncased <span class="citation"
      data-cites="akhila2023comparative">(Akhila et al. 2023)</span> foi
      lançado em 2019, sendo menor e mais rápido que o BERT e otimizado
      para tarefas que processam frases ou sentenças. Sua versão
      <em>uncased</em> é útil para descrições de código e problemas, já
      que não considera diferenças de maiúsculas/minúsculas. Ele é
      pré-treinado em um grande corpus, o que ajuda na generalização, e
      tem uma boa arquitetura para esse tipo de tarefa, além de exigir
      hardware acessível.</p>
      <p>O <em>distilbert</em> resulta de um processo de knowledge
      distillation que reduz em aproximadamente 40% o número de
      parâmetros e acelera a inferência em cerca de 60%, preservando
      95–97% da acurácia do BERT-base (original) em benchmarks de
      compreensão de linguagem natural <span class="citation"
      data-cites="Sanh2019">(Sanh et al. 2019)</span>. Essa compacidade
      de fornecer representações ricas com custo computacional inferior
      é particularmente vantajosa em ambientes de hardware moderado. Com
      apenas 67M de parâmetros <span class="citation"
      data-cites="Sanh2019">(Sanh et al. 2019)</span>, a variante
      uncased cabe em uma GPU modesta (por exemplo, 6 GB de RAM da placa
      de vídeo), possibilitando fine-tuning e inferência dentro de
      recursos computacionais restritos. Assim, torna-se viável
      re-treinar o modelo à medida que novos dados de projeto se
      acumulam, mantendo a acurácia sem investir em infraestruturas
      onerosas.</p>
      <p>Por fim, o amplo suporte no ecossistema <em>Hugging Face</em>
      para os modelos do tipo BERT e para outros modelos simplifica a
      reprodutibilidade e a integração em pipelines de tarefas de
      processamento de linguagem natural.</p>
      <h2 id="fine-tuning-de-modelos-pré-treinados">Fine-Tuning de
      Modelos Pré-Treinados</h2>
      <p>Modelos pré-treinados como BERT e GPT demonstraram capacidade
      de compreender nuances semânticas em texto e transferir esse
      conhecimento para diversas tarefas específicas através de
      Fine-Tuning. <span class="citation"
      data-cites="Minaee2025">(Minaee et al. 2025)</span>.</p>
      <p>O <em>Fine-Tuning</em> de modelos pré-treinados é uma técnica
      fundamental no Processamento de Linguagem Natural (PLN) moderno,
      especialmente ao trabalhar com Modelos de Linguagem Grande (LLMs).
      <em>Fine-Tuning</em> permite adaptar um modelo geral para tarefas
      específicas, como classificação de texto, análise de sentimentos
      ou geração de linguagem, utilizando um conjunto de dados menor e
      específico.</p>
      <p><em>Fine-Tuning</em> é o processo de tomar um modelo
      pré-treinado em uma grande quantidade de dados gerais e adaptá-lo
      para uma tarefa específica. Este processo envolve ajustar os pesos
      do modelo, mas com uma taxa de aprendizado menor para não
      “desaprender” o que foi aprendido durante o pré-treinamento.
      Modelos como BERT <span class="citation"
      data-cites="devlin2019bert">(Devlin et al. 2019)</span>, GPT <span
      class="citation" data-cites="radford2019language">(Radford et al.
      2019)</span>, e T5 <span class="citation"
      data-cites="raffel2020exploring">(Raffel et al. 2020)</span> são
      comumente fine-tuned para tarefas específicas.</p>
      <p>Uma alternativa eficiente ao <em>Fine-Tuning</em> completo é o
      uso de métodos de adaptação com baixo número de parâmetros, como o
      LoRA (<em>Low-Rank Adaptation</em>). Em vez de atualizar todos os
      pesos do modelo pré-treinado, o LoRA introduz pequenas matrizes
      adicionais de baixa dimensão que são ajustadas durante o
      treinamento, enquanto os pesos originais permanecem congelados.
      Essa técnica reduz drasticamente o número de parâmetros que
      precisam ser treinados, diminuindo o custo computacional e de
      armazenamento. Na prática, isso torna viável aplicar
      <em>Fine-Tuning</em> em LLMs muito grandes, mesmo em ambientes com
      recursos limitados, preservando boa parte do desempenho obtido
      pelo ajuste completo.</p>
      <p>A principal vantagem do <em>Fine-Tuning</em> é a eficiência,
      pois permite que os modelos aprendam rapidamente uma nova tarefa,
      utilizando relativamente poucos dados. Além disso, modelos
      pré-treinados já capturam padrões linguísticos gerais, o que torna
      o Fine-Tuning uma abordagem útil para resolver problemas
      específicos sem precisar treinar um modelo do zero. O processo de
      <em>Fine-Tuning</em> geralmente envolve os seguintes passos:</p>
      <ul>
      <li><p><strong>Escolha do Modelo</strong>: Selecionar um modelo
      pré-treinado adequado para a tarefa. Modelos como BERT e GPT são
      populares devido à sua versatilidade.</p></li>
      <li><p><strong>Preparação dos Dados</strong>: Os dados precisam
      estar formatados de maneira que sejam compatíveis com a tarefa
      específica, como classificação de texto ou resposta a
      perguntas.</p></li>
      <li><p><strong>Configuração do Treinamento</strong>: Ajuste de
      hiperparâmetros como a taxa de aprendizado, número de épocas e
      tamanho do lote.</p></li>
      <li><p><strong>Treinamento</strong>: Executar o treinamento do
      modelo no conjunto de dados específico.</p></li>
      <li><p><strong>Avaliação</strong>: Avaliar o desempenho do modelo
      ajustado em um conjunto de validação ou teste.</p></li>
      </ul>
      <p>Vamos realizar o <em>Fine-Tuning</em> de um modelo BERT para
      uma tarefa de classificação de sentimentos usando o conjunto de
      dados IMDb.</p>
      <p>Cabe ressaltar que a biblioteca <em>transformers</em> da
      Hugging Face, utilizada neste exemplo, tornou-se a ferramenta de
      referência para trabalhar com modelos baseados em
      <em>Transformers</em>. Ela oferece uma ampla gama de modelos
      pré-treinados que podem ser facilmente integrados em pipelines de
      PLN.</p>
      <p>O código a seguir pode levar de 30 minutos a 2 horas para
      finalizar em um computador com uma boa GPU. Em CPU, pode demorar
      várias horas ou até mais de um dia. Uma sugestão é utilizar o
      Google Colab.</p>
      <div class="sourceCode" id="cb94" data-language="Python"
      data-caption="O código treina e avalia um modelo BERT para classificar sentimentos em textos do IMDb."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install datasets</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">&quot;WANDB_DISABLED&quot;</span>] <span class="op">=</span> <span class="st">&quot;true&quot;</span></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o dataset IMDb</span></span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">&quot;imdb&quot;</span>)</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BERT</span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained( <span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertForSequenceClassification.from_pretrained( <span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenizar os dados</span></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> tokenizer(examples[<span class="st">&#39;text&#39;</span>], padding<span class="op">=</span><span class="st">&#39;max_length&#39;</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir argumentos de treinamento</span></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">&#39;./results&#39;</span>,</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">&quot;none&quot;</span>,</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Criar o Trainer</span></span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a>model<span class="op">=</span>model,</span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a>args<span class="op">=</span>training_args,</span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a>train_dataset<span class="op">=</span>tokenized_datasets[<span class="st">&#39;train&#39;</span>],</span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>eval_dataset<span class="op">=</span>tokenized_datasets[<span class="st">&#39;test&#39;</span>],</span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Treinar o modelo</span></span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Avaliar o modelo</span></span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a>eval_result <span class="op">=</span> trainer.evaluate()</span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Resultado da Avaliação: </span><span class="sc">{</span>eval_result<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
      <pre><code>Resultado da Avaliação: {&#39;eval_loss&#39;: 0.2594565749168396, &#39;eval_runtime&#39;: 172.544, &#39;eval_samples_per_second&#39;: 144.891, &#39;eval_steps_per_second&#39;: 9.059, &#39;epoch&#39;: 3.0}</code></pre>
      <p>Neste exemplo, utilizamos o modelo BERT e a biblioteca
      <code>datasets</code> da Hugging Face para carregar o conjunto de
      dados IMDb, que é usado para tarefas de classificação de
      sentimentos. O processo de tokenização é realizado com o
      <code>BertTokenizer</code>, seguido pelo treinamento do modelo
      usando o <code>Trainer</code>, que automatiza o processo de
      <em>Fine-Tuning</em>.</p>
      <p>Embora o <em>Fine-Tuning</em> seja uma técnica com potencial, é
      importante considerar alguns desafios:</p>
      <ul>
      <li><p><strong>Overfitting</strong>: Ajustar demais o modelo para
      os dados de treinamento específicos pode reduzir a generalização
      para novos dados.</p></li>
      <li><p><strong>Biases Inerentes</strong>: Se o modelo pré-treinado
      já contém vieses, o <em>Fine-Tuning</em> pode reforçá-los,
      especialmente se os dados de treinamento forem limitados ou
      enviesados.</p></li>
      <li><p><strong>Requisitos Computacionais</strong>:
      <em>Fine-Tuning</em> de LLMs pode ser computacionalmente
      intensivo, especialmente para modelos maiores como GPT-3.</p></li>
      </ul>
      <p>Além disso, algumas abordagens avançadas para melhorar o
      processo de <em>Fine-Tuning</em> incluem:</p>
      <ul>
      <li><p><strong>Learning Rate Warmup</strong>: Aumentar
      gradualmente a taxa de aprendizado no início do treinamento para
      evitar grandes atualizações de peso que poderiam desestabilizar o
      modelo.</p></li>
      <li><p><strong>Layer-Wise Learning Rate Decay</strong>: Aplicar
      diferentes taxas de aprendizado para diferentes camadas do modelo,
      com camadas inferiores aprendendo mais lentamente.</p></li>
      <li><p><strong>Data Augmentation</strong>: Aumentar a diversidade
      do conjunto de dados de treinamento para melhorar a robustez do
      modelo.</p></li>
      </ul>
      <p>O <em>Fine-Tuning</em> tem uma vasta gama de aplicações em PLN,
      incluindo:</p>
      <ul>
      <li><p><strong>Classificação de Texto</strong>: Análise de
      sentimentos, detecção de spam, categorização de notícias.</p></li>
      <li><p><strong>Respostas a Perguntas</strong>: Modelos que
      respondem a perguntas baseadas em um contexto textual
      específico.</p></li>
      <li><p><strong>Geração de Texto</strong>: <em>Fine-Tuning</em> de
      modelos como GPT para gerar textos específicos de um domínio, como
      redação de artigos científicos.</p></li>
      <li><p><strong>Tradução Automática</strong>: Adaptação de modelos
      de tradução para dialetos ou linguagens específicas.</p></li>
      </ul>
      <h2 id="few-shot-e-zero-shot-learning">Few Shot e Zero Shot
      Learning</h2>
      <p><strong>Zero Shot</strong>: A abordagem LLM zero-shot learning
      refere-se à capacidade dos LLMs de resolver tarefas sem a
      necessidade de exemplos explícitos fornecidos durante a
      inferência. Nesse contexto, a tarefa de processamento de linguagem
      natural <em>text classification</em> é especificada unicamente por
      meio de uma instrução textual (prompt), e o modelo deve inferir a
      ação esperada com base em seu conhecimento prévio adquirido
      durante o pré-treinamento <span class="citation"
      data-cites="radford2019language">(Radford et al. 2019)</span>.</p>
      <p><strong>Few Shot</strong>: Já o few-shot learning
      caracteriza-se pela inclusão de um pequeno conjunto de exemplos da
      tarefa no próprio prompt, com o objetivo de guiar a geração do
      modelo durante a inferência. Essa técnica permite ao modelo
      identificar padrões desejados com base nos exemplos fornecidos e
      aplicá-los a novos casos, mesmo sem reconfiguração ou ajuste de
      parâmetros. Trata-se de uma abordagem intermediária entre o
      zero-shot e o treinamento supervisionado tradicional, sendo
      especialmente eficaz em tarefas de classificação com variações
      contextuais <span class="citation"
      data-cites="raschka2024build">(Raschka 2024)</span>. Sua principal
      vantagem está na adaptação rápida a novas tarefas com custo
      computacional reduzido.</p>
      <h2 id="retrieval-augmented-generation-rag">Retrieval-Augmented
      Generation (RAG)</h2>
      <p>O <em>Retrieval-Augmented Generation</em> (RAG) é uma abordagem
      que combina duas técnicas na área de processamento de linguagem
      natural: recuperação de informações e geração de texto. A ideia
      central do RAG é aprimorar a capacidade de um modelo de linguagem
      ao integrá-lo com um sistema de recuperação que busca informações
      relevantes de uma base de dados ou de um conjunto de
      documentos.</p>
      <p>Na prática, o RAG opera em duas etapas. Primeiro, quando uma
      consulta ou pergunta é feita, um mecanismo de recuperação é
      acionado para identificar e extrair informações pertinentes de um
      repositório de dados. Isso pode incluir documentos, artigos ou
      qualquer outro tipo de conteúdo textual que possa fornecer
      contexto e detalhes adicionais sobre o tema em questão. Essa fase
      garante que o modelo de linguagem tenha acesso a informações
      atualizadas e específicas, em vez de depender apenas do
      conhecimento prévio que foi incorporado durante seu
      treinamento.</p>
      <p>Em seguida, na segunda etapa, o modelo de linguagem utiliza as
      informações recuperadas para gerar uma resposta mais rica e
      contextualizada. Essa geração não se limita a reproduzir o
      conteúdo recuperado, mas sim a integrar esses dados de forma
      coesa, criando uma resposta que não apenas responde à pergunta,
      mas também fornece uma narrativa mais completa e informativa. Isso
      resulta em respostas que são mais precisas e relevantes, pois são
      fundamentadas em dados concretos e atualizados.</p>
      <p>A combinação dessas duas etapas permite que o RAG supere
      algumas limitações dos modelos de linguagem tradicionais, que
      podem falhar em fornecer informações precisas ou atualizadas,
      especialmente em domínios que evoluem rapidamente. Além disso,
      essa abordagem é particularmente útil em aplicações como
      assistentes virtuais, chatbots e sistemas de perguntas e
      respostas, onde a precisão e a relevância da informação são
      fatores chaves na experiência do usuário.</p>
      <p>Portanto, ele é uma técnica que não apenas melhora a qualidade
      das respostas geradas por modelos de linguagem, mas também amplia
      o alcance e a aplicabilidade desses modelos em cenários do mundo
      real, onde a informação é dinâmica e em constante evolução.</p>
      <p>O RAG une dois componentes principais, a Recuperação de
      Informação (<em>Retrieval</em>): envolve buscar documentos,
      parágrafos ou passagens relevantes a partir de uma grande coleção
      de dados; e a Geração de Texto (<em>Generation</em>): uma vez que
      a informação relevante é recuperada, um modelo de linguagem, como
      GPT ou BART, é utilizado para gerar uma resposta coerente e
      informativa baseada nas informações recuperadas. Dessa forma, RAG
      é capaz de responder a perguntas e gerar conteúdo que não apenas
      utiliza o contexto imediato, mas também consulta uma base de
      conhecimento externa, aumentando a precisão e a relevância das
      respostas.</p>
      <p>Vamos implementar um exemplo simples de RAG usando as
      bibliotecas da Hugging Face, incluindo o modelo DPR para
      recuperação e o modelo BART para geração de texto.</p>
      <p>Recuperação de Passagens com DPR: Primeiro, precisamos carregar
      e configurar o modelo DPR para recuperar passagens relevantes a
      partir de uma base de dados. Sugerimos a execução do código abaixo
      no Google Colab.</p>
      <div class="sourceCode" id="cb96" data-language="Python"
      data-caption="O código identifica qual passagem é mais relevante para responder à consulta fornecida, usando embeddings densos e busca vetorial."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRContextEncoder, DPRContextEncoderTokenizer</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para as consultas (questions)</span></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>question_tokenizer <span class="op">=</span> DPRQuestionEncoderTokenizer.from_pretrained(</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>question_encoder <span class="op">=</span> DPRQuestionEncoder.from_pretrained(</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para os contextos (passages)</span></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>context_tokenizer <span class="op">=</span> DPRContextEncoderTokenizer.from_pretrained( <span class="st">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>context_encoder <span class="op">=</span> DPRContextEncoder.from_pretrained( <span class="st">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de consulta</span></span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">&quot;What is Retrieval-Augmented Generation?&quot;</span></span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar a consulta</span></span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>query_input <span class="op">=</span> question_tokenizer(query, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> question_encoder(<span class="op">**</span>query_input).pooler_output</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de passagens</span></span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> [</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation.&quot;</span>,</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;It allows for more accurate and contextually relevant answers by consulting external knowledge bases.&quot;</span>,</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model.&quot;</span></span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar as passagens</span></span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>passage_inputs <span class="op">=</span> context_tokenizer(passages, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a>passage_embeddings <span class="op">=</span> context_encoder(</span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>passage_inputs).pooler_output</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular similaridade e selecionar a passagem mais relevante</span></span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> torch.matmul(query_embedding, passage_embeddings.T)</span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>best_passage_index <span class="op">=</span> torch.argmax(similarity_scores, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a>best_passage <span class="op">=</span> passages[best_passage_index]</span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Passagem mais relevante:&quot;</span>, best_passage)</span></code></pre></div>
      <pre><code>Passagem mais relevante: Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation </code></pre>
      <p>Neste exemplo, utilizamos o modelo DPR para codificar uma
      consulta e várias passagens, e então calculamos a similaridade
      entre a consulta e as passagens para recuperar a mais
      relevante.</p>
      <p>Geração de Texto com BART: Uma vez que a passagem mais
      relevante foi recuperada, utilizamos o modelo BART para gerar uma
      resposta coerente.</p>
      <div class="sourceCode" id="cb98" data-language="Python"
      data-caption="O código busca identificar, entre vários textos, qual é o mais relevante para uma pergunta, usando embeddings e similaridade vetorial. Depois, prepara um modelo de geração para responder à consulta usando o contexto selecionado."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DPRContextEncoder, DPRContextEncoderTokenizer</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BartTokenizer, BartForConditionalGeneration</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para as consultas (questions)</span></span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>question_tokenizer <span class="op">=</span> DPRQuestionEncoderTokenizer.from_pretrained(</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>question_encoder <span class="op">=</span> DPRQuestionEncoder.from_pretrained(</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo para os contextos (passages)</span></span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>context_tokenizer <span class="op">=</span> DPRContextEncoderTokenizer.from_pretrained( <span class="st">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>context_encoder <span class="op">=</span> DPRContextEncoder.from_pretrained( <span class="st">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de consulta</span></span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">&quot;What is Retrieval-Augmented Generation?&quot;</span></span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar a consulta</span></span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a>query_input <span class="op">=</span> question_tokenizer(query, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb98-23"><a href="#cb98-23" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> question_encoder(<span class="op">**</span>query_input).pooler_output</span>
<span id="cb98-24"><a href="#cb98-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-25"><a href="#cb98-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de passagens</span></span>
<span id="cb98-26"><a href="#cb98-26" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> [</span>
<span id="cb98-27"><a href="#cb98-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Retrieval-Augmented Generation is a technique that combines retrieval of relevant information with text generation.&quot;</span>,</span>
<span id="cb98-28"><a href="#cb98-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;It allows for more accurate and contextually relevant answers by consulting external knowledge bases.&quot;</span>,</span>
<span id="cb98-29"><a href="#cb98-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;RAG is particularly useful in scenarios where the information required to answer a query is not present in the training data of the language model.&quot;</span></span>
<span id="cb98-30"><a href="#cb98-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb98-31"><a href="#cb98-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-32"><a href="#cb98-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar as passagens</span></span>
<span id="cb98-33"><a href="#cb98-33" aria-hidden="true" tabindex="-1"></a>passage_inputs <span class="op">=</span> context_tokenizer(passages, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb98-34"><a href="#cb98-34" aria-hidden="true" tabindex="-1"></a>passage_embeddings <span class="op">=</span> context_encoder</span>
<span id="cb98-35"><a href="#cb98-35" aria-hidden="true" tabindex="-1"></a>   <span class="op">**</span>passage_inputs).pooler_output</span>
<span id="cb98-36"><a href="#cb98-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-37"><a href="#cb98-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular similaridade e selecionar a passagem mais relevante</span></span>
<span id="cb98-38"><a href="#cb98-38" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> torch.matmul(query_embedding, passage_embeddings.T)</span>
<span id="cb98-39"><a href="#cb98-39" aria-hidden="true" tabindex="-1"></a>best_passage_index <span class="op">=</span> torch.argmax(similarity_scores, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb98-40"><a href="#cb98-40" aria-hidden="true" tabindex="-1"></a>best_passage <span class="op">=</span> passages[best_passage_index]</span>
<span id="cb98-41"><a href="#cb98-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-42"><a href="#cb98-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo BART</span></span>
<span id="cb98-43"><a href="#cb98-43" aria-hidden="true" tabindex="-1"></a>bart_tokenizer <span class="op">=</span> BartTokenizer.from_pretrained(</span>
<span id="cb98-44"><a href="#cb98-44" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;facebook/bart-large&quot;</span>)</span>
<span id="cb98-45"><a href="#cb98-45" aria-hidden="true" tabindex="-1"></a>bart_model <span class="op">=</span> BartForConditionalGeneration.from_pretrained(</span>
<span id="cb98-46"><a href="#cb98-46" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;facebook/bart-large&quot;</span>)</span>
<span id="cb98-47"><a href="#cb98-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-48"><a href="#cb98-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenar a consulta com a passagem relevante</span></span>
<span id="cb98-49"><a href="#cb98-49" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> query <span class="op">+</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> best_passage</span>
<span id="cb98-50"><a href="#cb98-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-51"><a href="#cb98-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Codificar e gerar resposta</span></span>
<span id="cb98-52"><a href="#cb98-52" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> bart_tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb98-53"><a href="#cb98-53" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> bart_model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">4</span>, early_stopping<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-54"><a href="#cb98-54" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> bart_tokenizer.decode(generated_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-55"><a href="#cb98-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-56"><a href="#cb98-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Resposta gerada:&quot;</span>, generated_text)</span></code></pre></div>
      <pre><code>Resposta gerada: What is Retrieval-Augmented Generation?Retrieval and Augmented Generation is a technique that combines retrieval of relevant information with text generation.What is retrieval?</code></pre>
      <p>Este código gera uma resposta baseada na passagem recuperada,
      criando uma resposta informativa que combina a informação
      relevante com a geração de texto fluida.</p>
      <p>A técnica RAG tem várias aplicações práticas, incluindo:</p>
      <ul>
      <li><p><strong>Sistemas de Resposta a Perguntas</strong>: Sistemas
      que precisam consultar bases de conhecimento extensivas para
      responder a perguntas de forma precisa.</p></li>
      <li><p><strong>Assistentes Virtuais</strong>: Assistentes que
      necessitam de acesso a informações específicas e detalhadas, além
      do treinamento inicial do modelo.</p></li>
      <li><p><strong>Geração de Conteúdo</strong>: Criação de conteúdo
      especializado que requer consulta de fontes externas para garantir
      precisão e relevância.</p></li>
      </ul>
      <p>A técnica RAG também apresenta alguns desafios:</p>
      <ul>
      <li><p><strong>Escalabilidade</strong>: A recuperação de
      informações em bases de dados muito grandes pode ser
      computacionalmente intensiva.</p></li>
      <li><p><strong>Relevância das Passagens</strong>: A qualidade das
      respostas geradas depende fortemente da relevância das passagens
      recuperadas.</p></li>
      <li><p><strong>Treinamento Conjunto</strong>: Treinar os
      componentes de recuperação e geração de maneira conjunta pode ser
      complexo e requer grandes volumes de dados.</p></li>
      </ul>
      <h2 id="llama">LLaMA</h2>
      <p>LLaMA é uma família de modelos de linguagem grandes (LLMs) que
      são menores em tamanho, mas ainda mantêm a capacidade de realizar
      tarefas complexas de PLN. A abordagem de LLaMA é baseada em uma
      arquitetura de <em>Transformer</em>, semelhante a outros LLMs, mas
      otimizada para eficiência em termos de parâmetros e recursos
      computacionais.</p>
      <h4 id="características-principais">Características
      Principais</h4>
      <ul>
      <li><p><strong>Tamanho Reduzido</strong>: LLaMA é projetado para
      ser mais leve que os modelos gigantescos, com diferentes variantes
      que variam de 7B a 65B parâmetros.</p></li>
      <li><p><strong>Eficiência Computacional</strong>: Devido ao seu
      design otimizado, o LLaMA pode ser treinado em menos tempo e com
      menos recursos, tornando-o acessível para organizações menores e
      pesquisadores.</p></li>
      <li><p><strong>Versatilidade</strong>: Apesar de seu tamanho
      reduzido, o LLaMA é capaz de realizar uma ampla gama de tarefas de
      PLN, incluindo geração de texto, tradução, e compreensão de
      linguagem.</p></li>
      </ul>
      <h3 id="arquitetura-do-llama">Arquitetura do LLaMA</h3>
      <p>A arquitetura do LLaMA é baseada no <em>Transformer</em>, mas
      com várias otimizações que permitem que ele mantenha uma alta
      qualidade de predição, enquanto usa menos parâmetros e recursos
      computacionais.</p>
      <h4 id="camadas-transformer-otimizadas">Camadas
      <em>Transformer</em> Otimizadas</h4>
      <p>O LLaMA utiliza camadas <em>Transformer</em> com melhorias
      específicas para otimizar o uso de memória e tempo de
      processamento. As principais diferenças incluem:</p>
      <ul>
      <li><p><strong>Atenção Multi-Head Otimizada</strong>: Reduz a
      redundância ao calcular a atenção em múltiplas cabeças.</p></li>
      <li><p><strong>Feedforward Otimizado</strong>: Utiliza técnicas de
      compressão para reduzir o número de operações
      necessárias.</p></li>
      <li><p><strong>Parâmetros Compactos</strong>: Redução do número de
      parâmetros, mantendo a capacidade de capturar relações complexas
      na linguagem.</p></li>
      </ul>
      <h4 id="treinamento-e-escalabilidade">Treinamento e
      Escalabilidade</h4>
      <p>O LLaMA foi treinado em grandes corpora de dados textuais,
      incluindo múltiplos idiomas e domínios. A arquitetura permite que
      o modelo seja escalado de maneira eficiente, com versões menores
      (7B parâmetros) adequadas para tarefas menos intensivas e versões
      maiores (65B parâmetros) competindo com modelos de ponta como
      GPT-3.</p>
      <h3 id="exemplo-de-uso-do-llama">Exemplo de uso do LLaMA</h3>
      <p>Embora o LLaMA seja relativamente novo, é possível utilizar as
      ferramentas da Hugging Face para trabalhar com variantes do modelo
      ou implementações semelhantes.</p>
      <p>Vamos utilizar uma versão de LLaMA para realizar uma tarefa
      simples de geração de texto. Assumiremos que a variante LLaMA foi
      integrada à <em>Hugging Face Transformers</em>.</p>
      <p>Para executar o código abaixo, recomendamos o Google Colab. É
      necessário informar a sua chave do Hugging Face e solicitar acesso
      ao modelo LLaMa no próprio Hugging Face.</p>
      <div class="sourceCode" id="cb100" data-language="Python"
      data-caption="cria uma frase continuando o texto inicial, simulando uma escrita criativa baseada em inteligência artificial."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> login</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> userdata</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> userdata</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>userdata.get(<span class="st">&#39;HF_TOKEN&#39;</span>)</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Log in to Hugging Face</span></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>login(token<span class="op">=</span>userdata.get(<span class="st">&#39;HF_TOKEN&#39;</span>))</span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo LLaMA</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;meta-llama/Meta-Llama-3-8B&quot;</span>)</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;meta-llama/Meta-Llama-3-8B&quot;</span>)</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Texto de entrada</span></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">&quot;Artificial intelligence is transforming the world of&quot;</span></span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenizar e gerar texto</span></span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">5</span>, early_stopping<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> tokenizer.decode(generated_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-25"><a href="#cb100-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Texto gerado:&quot;</span>, generated_text)</span></code></pre></div>
      <pre><code>Texto gerado: Artificial intelligence is transforming the world of business. It has the potential to revolutionize the way we work, communicate, and interact with each other. AI is already being used in a variety of industries, from healthcare to finance, and it is</code></pre>
      <p>Neste exemplo, carregamos o tokenizer e o modelo LLaMA e
      geramos um texto baseado em um prompt de entrada. O código pode
      ser ajustado para diferentes tamanhos de modelos e diferentes
      tarefas de geração de texto.</p>
      <h3 id="aplicações-de-llama">Aplicações de LLaMA</h3>
      <p>Devido a sua eficiência e versatilidade, LLaMA pode ser
      aplicado em uma variedade de cenários, incluindo:</p>
      <ul>
      <li><p><strong>Assistentes Virtuais</strong>: Implementação de
      assistentes que podem ser executados em dispositivos com recursos
      limitados.</p></li>
      <li><p><strong>Geração de Conteúdo</strong>: Produção de artigos,
      histórias e outros conteúdos textuais de alta qualidade.</p></li>
      <li><p><strong>Tradução Automática</strong>: Modelos LLaMA menores
      podem ser usados para traduções em tempo real em dispositivos
      móveis.</p></li>
      <li><p><strong>Análise de Sentimentos</strong>: Aplicações que
      exigem processamento eficiente de grandes volumes de dados
      textuais.</p></li>
      </ul>
      <h3 id="comparação-com-outros-modelos">Comparação com Outros
      Modelos</h3>
      <p>Quando comparado com modelos maiores como GPT-3, o LLaMA
      oferece um excelente equilíbrio entre desempenho e eficiência. Ele
      é particularmente útil em cenários onde os recursos computacionais
      são limitados ou onde a implantação em escala é uma consideração
      chave.</p>
      <p>Algumas Vantagens do LLaMA:</p>
      <ul>
      <li><p><strong>Redução de Custos</strong>: Menor demanda por
      recursos computacionais, resultando em custos reduzidos para
      treinamento e implantação.</p></li>
      <li><p><strong>Escalabilidade</strong>: Pode ser facilmente
      adaptado para diferentes tarefas e ambientes.</p></li>
      <li><p><strong>Rapidez</strong>: Menor latência em inferências
      devido ao menor tamanho do modelo.</p></li>
      </ul>
      <p>Algumas Limitações do LLaMA:</p>
      <ul>
      <li><p><strong>Capacidade Limitada</strong>: Embora eficiente,
      modelos menores podem não capturar todas as nuances de linguagem
      que modelos maiores conseguem.</p></li>
      <li><p><strong>Menor Variedade de Tarefas</strong>: Pode ser menos
      adequado para tarefas extremamente complexas que exigem modelos
      com bilhões de parâmetros.</p></li>
      </ul>
      <h2 id="llm-na-prática">LLM na prática</h2>
      <h3 id="hugging-face-pipeline">Hugging Face Pipeline</h3>
      <p>A biblioteca <em>Transformer</em> da <em>Hugging Face</em>
      torna muito mais fácil trabalhar com modelos pré-treinados como o
      GPT-2. Aqui está um exemplo de como gerar texto usando o GPT-2
      pré-treinado:</p>
      <div class="sourceCode" id="cb102" data-language="Python"
      data-caption="Exemplo de uso do GPT-2 com a biblioteca Transformers."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">&#39;text-generation&#39;</span>, model<span class="op">=</span><span class="st">&#39;gpt2&#39;</span>)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> <span class="st">&#39;Olá, como vai você?&#39;</span></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pipe(<span class="bu">input</span>)</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code></pre></div>
      <pre><code>[{&#39;generated_text&#39;: &#39;The book is on one of the most exciting,&#39;},
{&#39;generated_text&#39;: &#39;The book is on sale via Amazon.com for&#39;}, 
{&#39;generated_text&#39;: &#39;The book is on sale tomorrow for $2.&#39;}, 
{&#39;generated_text&#39;: &#39;The book is on sale now, read more at&#39;}, 
{&#39;generated_text&#39;: &#39;The book is on the bookshelf in the&#39;}]</code></pre>
      <p>Este código é simples porque ele usa um modelo que já foi
      treinado em um grande dataset. Também é possível ajustar
      (fine-tune) um modelo pré-treinado em seus próprios dados para
      obter resultados melhores.</p>
      <h3 id="llm-local-com-ollama">LLM Local com Ollama</h3>
      <p>Nesta seção, descreveremos detalhes da instalação do software
      Ollama, baixaremos o modelo LLM Qwen2 0.5B e exploraremos suas
      capacidades com algumas perguntas simples.</p>
      <p>O Ollama é uma plataforma baseada em linha de comando que
      facilita o uso de modelos de IA localmente; existem outras, porém
      a ideia geral é bem semelhante. Com ele, você pode baixar modelos
      pré-treinados, gerar texto e fazer inferências sem precisar de uma
      placa de vídeo potente. Sua simplicidade e rapidez o tornam
      perfeito para quem quer experimentar LLMs em computadores mais
      modestos.</p>
      <p>Baixando e Instalando o Ollama: Acesse o site oficial do Ollama
      e baixe o instalador para o seu sistema operacional. Os
      procedimentos descritos nesta seção são baseados no Windows, mas o
      Ollama também funciona no macOS e Linux.</p>
      <p>Entre no site do Ollama e clique no botão de download para
      Windows. Depois, salve o instalador (um arquivo pequeno, com
      poucos megabytes) na sua pasta de downloads. Em seguida, execute o
      instalador e siga as instruções para concluir a instalação. Depois
      de instalado, o Ollama já está pronto para ser usado pelo
      terminal. Veja na Figura <a href="#fig:downloadollama"
      data-reference-type="ref"
      data-reference="fig:downloadollama">5.3</a> a página do site do
      Ollama para download.</p>
      <figure id="fig:downloadollama">
      <p><img src="fig/ollama1.png" style="width:90.0%" alt="image" />
      <span id="fig:downloadollama"
      data-label="fig:downloadollama"></span></p>
      <figcaption>Figure 21: Site do Ollama par download.</figcaption>
      </figure>
      <p>Conhecendo os Comandos do Ollama: Com o Ollama instalado, abra
      o terminal (Prompt de Comando no Windows) e digite “ollama” para
      confirmar que a instalação deu certo. Você verá uma lista de
      comandos disponíveis. Veja na Figura <a
      href="#fig:comandolistarollama" data-reference-type="ref"
      data-reference="fig:comandolistarollama">5.4</a> o resultado da
      saída do comando list no prompt de comando.</p>
      <figure id="fig:comandolistarollama">
      <p><img src="fig/ollama2.png" style="width:90.0%" alt="image" />
      <span id="fig:comandolistarollama"
      data-label="fig:comandolistarollama"></span></p>
      <figcaption>Figure 22: Comando para listar os modelos LLM já
      instalado.</figcaption>
      </figure>
      <p>Vamos detalhar os comandos run e show do Ollama. O comando
      “ollama run &lt;nome-do-modelo&gt;” baixa e roda um modelo
      específico. Já o comando “ollama show &lt;nome-do-modelo&gt;”
      exibe detalhes sobre um modelo, como janela de contexto e
      parâmetros. Como ainda não baixamos nenhum modelo, o comando
      “ollama list” vai mostrar uma lista vazia. Vamos resolver isso
      baixando um modelo!</p>
      <p>Escolhendo e Baixando um Modelo: No site do Ollama, na seção
      “Models”, você encontra vários modelos disponíveis. Veja na
      Figura <a href="#fig:sitebuscaollamaprint"
      data-reference-type="ref"
      data-reference="fig:sitebuscaollamaprint">5.5</a> um print de tela
      do navegador do site do Ollama com o resultado da busca de um
      modelo LLM.</p>
      <figure id="fig:sitebuscaollamaprint">
      <p><img src="fig/ollama4.png" style="width:90.0%" alt="image" />
      <span id="fig:sitebuscaollamaprint"
      data-label="fig:sitebuscaollamaprint"></span></p>
      <figcaption>Figure 23: Site do ollama, busca pelo nome do
      modelo.</figcaption>
      </figure>
      <p>Vamos usar um modelo bem leve chamado “Qwen2 0.5B”, da Alibaba,
      que tem 0,5 bilhão de parâmetros e apenas 350 MB de tamanho. Ele é
      ideal para máquinas mais simples, como um computador com
      processador Pentium Gold sem GPU. Veja na Figura <a
      href="#fig:detalheswen" data-reference-type="ref"
      data-reference="fig:detalheswen">5.6</a> um print de tela do
      navegador do site do Ollama com o resultado do detalhamento do
      modelo Qwen2.5.</p>
      <figure id="fig:detalheswen">
      <p><img src="fig/ollama5.png" style="width:90.0%" alt="image" />
      <span id="fig:detalheswen"
      data-label="fig:detalheswen"></span></p>
      <figcaption>Figure 24: Tela exibindo detalhes do modelo LLM
      Qwen2.5.</figcaption>
      </figure>
      <p>Para baixar e rodar o modelo, acesse a página de modelos no
      site do Ollama. Localize o modelo Qwen2 e anote o comando para a
      versão de 0.5B: “ollama run qwen2:0.5b”. No terminal, digite:</p>
      <pre
      data-caption="Comando para executar (subir na memória) um modelo LLM no Ollama"><code>ollama run qwen2.5:0.5b</code></pre>
      <p>Esse comando vai baixar o modelo e abrir uma interface de texto
      interativa para você começar a conversar com ele. Veja na
      Figura <a href="#fig:promptrunollama" data-reference-type="ref"
      data-reference="fig:promptrunollama">5.7</a> um print de tela do
      navegador com o resultado do prompt do comando “run”.</p>
      <figure id="fig:promptrunollama">
      <p><img src="fig/ollama6.png" style="width:90.0%" alt="image" />
      <span id="fig:promptrunollama"
      data-label="fig:promptrunollama"></span></p>
      <figcaption>Figure 25: Prompt de comando demonstrando a execução
      do Ollama.</figcaption>
      </figure>
      <p>Interagindo com o modelo: Com o modelo baixado, o Ollama inicia
      uma interface no terminal para conversas baseadas em texto. Vamos
      testar com algumas perguntas.</p>
      <h4 class="unnumbered"
      id="exemplo-1-perguntando-sobre-aprendizado-de-máquina">Exemplo 1
      Perguntando Sobre Aprendizado de Máquina</h4>
      <p>.</p>
      <p>Digite a seguinte pergunta “O que é aprendizagem de máquina?”
      no console (ou prompt de comando) do Ollama instalado e com um
      modelo LLM carregado. Veja na Figura <a
      href="#fig:prompsaidaollamaaprendizagem" data-reference-type="ref"
      data-reference="fig:prompsaidaollamaaprendizagem">5.8</a> um print
      do prompt de comando com a resposta do LLM Qwen carregado em
      memória e respondendo ao prompt: o que é aprendizagem de
      máquina.</p>
      <pre data-caption="Prompt que será passado para o LLM"><code>O que é aprendizagem de máquina?</code></pre>
      <figure id="fig:prompsaidaollamaaprendizagem">
      <p><img src="fig/ollama7.png" style="width:90.0%" alt="image" />
      <span id="fig:prompsaidaollamaaprendizagem"
      data-label="fig:prompsaidaollamaaprendizagem"></span></p>
      <figcaption>Figure 26: Prompt de comando exibindo a saída do LLM
      em português.</figcaption>
      </figure>
      <p>Por ser um modelo pequeno, o Qwen2 0.5B pode “alucinar” um
      pouco ou dar respostas meio vagas, especialmente em português.
      Suas capacidades de diálogo emergente são limitadas comparadas a
      modelos maiores, mas ainda assim são impressionantes para o
      tamanho dele.</p>
      <h4 class="unnumbered"
      id="exemplo-2-perguntando-em-inglês">Exemplo 2: Perguntando em
      Inglês</h4>
      <p>.</p>
      <p>Agora, vamos tentar a mesma pergunta em inglês: “What is
      machine learning?”. Perguntando em inglês, a resposta costuma ser
      mais precisa. Figura <a href="#fig:promptrunollamaingles"
      data-reference-type="ref"
      data-reference="fig:promptrunollamaingles">5.9</a> um resultado da
      resposta (output) do LLM em inglês.</p>
      <pre
      data-caption="Prompt que será enviado para o LLM em inglês"><code>What is machine learning?</code></pre>
      <figure id="fig:promptrunollamaingles">
      <p><img src="fig/ollama8.png" style="width:90.0%" alt="image" />
      <span id="fig:promptrunollamaingles"
      data-label="fig:promptrunollamaingles"></span></p>
      <figcaption>Figure 27: Prompt de comando exibindo a saída do LLM
      em inglês.</figcaption>
      </figure>
      <p>O modelo tem uma resposta melhor em inglês, provavelmente
      porque foi treinado com mais dados nessa língua. Isso mostra como
      o desempenho de modelos de linguagem pode variar dependendo do
      idioma e dos dados de treinamento.</p>
      <h4 class="unnumbered"
      id="exemplo-3-prompt-simples-pergunta-e-resposta">Exemplo 3:
      prompt simples pergunta e resposta</h4>
      <p>Vamos facilitar, perguntando uma coisa simples, em que o modelo
      teria facilidade em perguntas: “O que é Ollama?”. Veja na
      Figura <a href="#fig:perguntasimples" data-reference-type="ref"
      data-reference="fig:perguntasimples">5.10</a> a saída desta
      pergunta.</p>
      <pre data-caption="Pergunta simples"><code>O que é Ollama?</code></pre>
      <figure id="fig:perguntasimples">
      <p><img src="fig/ollama9.png" style="width:90.0%" alt="image" />
      <span id="fig:perguntasimples"
      data-label="fig:perguntasimples"></span></p>
      <figcaption>Figure 28: Prompt de comando exibindo a saída do LLM
      em português.</figcaption>
      </figure>
      <p>O modelo confundiu o “Ollama” a que eu me referia com outra
      coisa, mostrando as limitações de modelos menores. Perguntando em
      inglês (“What’s Ollama in language models?”), ele também não
      acerta, sugerindo que é um tipo de música latina.</p>
      <p>A interface do Ollama tem alguns comandos úteis para gerenciar
      a sessão. Entre eles “/clear”: Limpa a sessão atual para começar
      do zero; “/help”: Mostra os comandos disponíveis; “/bye”: Sai do
      modo interativo do modelo. Por exemplo, depois de algumas
      perguntas, você pode digitar /clear para zerar o contexto ou /bye
      para encerrar o modelo.</p>
      <p>Verificando detalhes do Modelo LLM: Para saber mais sobre o
      modelo que você está usando, execute o comando “ollama show
      &lt;nome modelo&gt;”.</p>
      <pre
      data-caption="Comando para exibir detalhes do modelo LLM desejado"><code>ollama show qwen2.5:0.5b</code></pre>
      <p>Esse comando mostra informações, tais como a janela de
      contexto: 32k tokens (a quantidade de texto que o modelo consegue
      considerar de uma vez); a quantização: Q4 (um método para reduzir
      o tamanho do modelo e otimizar a performance). A janela de
      contexto é especialmente importante, pois ela acumula tanto o que
      você digita quanto as respostas do modelo, até o limite de 32.000
      tokens. Um limite considerável para um modelo pequeno.</p>
      <p>O Ollama funciona em computadores simples, tornando a IA
      acessível a todos; com ele é possível uma execução local pois não
      depende de serviços na nuvem, garantindo privacidade e controle.
      Sua configuração é rápida e sua interface de linha de comando é
      ideal para desenvolvedores que preferem um fluxo de trabalho
      minimalista e baseado em texto; por fim, embora existam
      plataformas mais visuais para rodar LLMs, a velocidade e o baixo
      consumo de recursos do Ollama o tornam uma ótima escolha para
      experimentos rápidos.</p>
      <h3 id="tokenizador-no-llm">Tokenizador no LLM</h3>
      <p>O tokenizador é responsável por dividir o texto em partes
      menores (tokens) que o modelo pode entender. Depois, precisamos de
      um modelo. O modelo é a parte que realmente faz o trabalho de
      entender e gerar texto. Vamos usar um tokenizador já existente no
      Hugging Face.</p>
      <div class="sourceCode" id="cb109" data-language="Python"
      data-caption="Este código utiliza a biblioteca Transformers para trabalhar com o modelo de linguagem GPT-2."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install transformers[torch]</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">&quot;gpt2&quot;</span>)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span><span class="op">=</span> <span class="st">&quot;Olá, como vai você?&quot;</span>    </span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>token_id <span class="op">=</span> tokenizer(<span class="bu">input</span>)</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(token_id)</span></code></pre></div>
      <pre><code>{&#39;input_ids&#39;: [30098, 6557, 11, 401, 78, 410, 1872, 12776, 25792, 30], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</code></pre>
      <p>A saída deste código será um dicionário com os ids dos tokens e
      a máscara de atenção. O id do token é o número que representa cada
      palavra ou parte da palavra no vocabulário do modelo. A máscara de
      atenção indica quais tokens devem ser considerados pelo modelo
      durante o processamento.</p>
      <p>Attention mask é uma lista de 1s e 0s que indica quais tokens
      devem ser considerados pelo modelo durante o processamento. Um
      valor de 1 significa que o token correspondente deve ser
      considerado, enquanto um valor de 0 significa que ele deve ser
      ignorado.</p>
      <h3 id="langchain">LangChain</h3>
      <p>LangChain é uma biblioteca de software de código aberto
      projetada para simplificar a interação com Modelos de Linguagem
      Grande (LLMs) e construir aplicativos de processamento de
      linguagem natural robustos. Ele fornece uma camada de abstração de
      alto nível sobre as complexidades de trabalhar diretamente com
      modelos de linguagem, tornando mais acessível a criação de
      aplicativos de compreensão e geração de linguagem.</p>
      <p>Trabalhar com LLMs pode ser complexo devido à sua natureza
      sofisticada e aos requisitos de recursos computacionais. LangChain
      lida com muitos detalhes complexos em segundo plano, permitindo
      que os desenvolvedores se concentrem na construção de aplicativos
      de linguagem eficazes. Aqui estão algumas vantagens do uso do
      LangChain:</p>
      <ul>
      <li><p>Simplicidade: LangChain oferece uma API simples e
      intuitiva, ocultando os detalhes complexos de interação com LLMs.
      Ele abstrai as nuances de carregar modelos, gerenciar recursos
      computacionais e executar previsões.</p></li>
      <li><p>Flexibilidade: A biblioteca suporta vários frameworks de
      deep learning, como TensorFlow e PyTorch, e pode ser integrada a
      diferentes LLMs. Isso oferece aos desenvolvedores a flexibilidade
      de escolher as ferramentas e modelos que melhor atendem às suas
      necessidades.</p></li>
      <li><p>Extensibilidade: LangChain é projetado para ser extensível,
      permitindo que os usuários criem seus próprios componentes
      personalizados. Você pode adicionar novos modelos, adaptar o
      processamento de texto ou desenvolver recursos específicos do
      domínio para atender aos requisitos exclusivos do seu
      aplicativo.</p></li>
      <li><p>Comunidade e suporte: LangChain tem uma comunidade ativa de
      desenvolvedores e pesquisadores que contribuem para o projeto. A
      documentação abrangente, tutoriais e suporte da comunidade tornam
      mais fácil começar e navegar por quaisquer desafios que surgirem
      durante o desenvolvimento.</p></li>
      </ul>
      <p>A arquitetura do LangChain pode ser entendida em três
      componentes principais:</p>
      <p>Camada de Abstração: Esta camada fornece uma interface simples
      e unificada para interagir com diferentes LLMs. Ela abstrai as
      complexidades de carregar, inicializar e executar previsões em
      modelos, oferecendo uma API consistente independentemente do
      modelo subjacente.</p>
      <p>Camada de Processamento de Texto: O LangChain inclui
      ferramentas robustas para processamento de texto, incluindo
      tokenização, análise sintática, reconhecimento de entidades
      nomeadas (NER) e muito mais. Esta camada prepara os dados de
      entrada e saída para que possam ser processados de forma eficaz
      pelos modelos de linguagem.</p>
      <p>Camada de Modelo: Aqui é onde os próprios LLMs residem. O
      LangChain suporta uma variedade de modelos de linguagem, desde
      modelos pré-treinados de uso geral até modelos personalizados
      específicos de domínio. Esta camada lida com a execução de
      previsões, gerenciamento de recursos computacionais e interação
      com as APIs dos modelos.</p>
      <p>Vamos ver um exemplo simples de como usar o LangChain para
      consultar um LLM e obter uma resposta. Neste exemplo, usaremos o
      gpt-4o-mini da OpenAI para responder a uma pergunta.</p>
      <p>Primeiro, importe as bibliotecas necessárias e configure o
      cliente LangChain. Em seguida, carregue o modelo de linguagem
      desejado. Agora, você pode usar o modelo para fazer uma consulta.
      Vamos perguntar quem é o presidente do Brasil.</p>
      <div class="sourceCode" id="cb111" data-language="Python"
      data-caption="Exemplo de uso do LangChain."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install langchain</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chat_models <span class="im">import</span> init_chat_model</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_core.messages <span class="im">import</span> HumanMessage</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>OPENAI_API_KEY <span class="op">=</span> os.environ.get(<span class="st">&quot;OPENAI_API_KEY&quot;</span>)</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> init_chat_model(<span class="st">&quot;gpt-4o-mini&quot;</span>, </span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>    model_provider<span class="op">=</span><span class="st">&quot;openai&quot;</span>,                        openai_api_key<span class="op">=</span>OPENAI_API_KEY)</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>user_message <span class="op">=</span> HumanMessage(content<span class="op">=</span><span class="st">&quot;Quem é o presidente do Brasil?&quot;</span>)</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> model.invoke([user_message])</span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre></div>
      <pre><code>Até a minha última atualização em outubro de 2023, o presidente do Brasil é Luiz Inácio Lula da Silva. Ele assumiu o cargo em janeiro de 2023. Para informações mais atualizadas, recomendo verificar fontes de notícias recentes.</code></pre>
      <p>Este exemplo básico demonstra a simplicidade de usar o
      LangChain para interagir com LLMs. No entanto, o LangChain oferece
      muito mais recursos e funcionalidades para construir aplicativos
      de chatbot mais robustos.</p>
      <h3 id="mangaba.ai">Mangaba.AI</h3>
      <p>O Mangaba.AI é um framework escrito em Python para a criação de
      agentes de IA autônomos que colaboram em equipe para resolver
      tarefas complexas. Ele permite montar equipes de agentes com
      funções especializadas — por exemplo, um agente pesquisador, outro
      analista, outro redator — que compartilham memória contextual
      (isto é, histórico de interações e resultados) para dar
      continuidade ao trabalho de forma inteligente. Além disso, o
      Mangaba integra modelos avançados (incluindo os modelos Gemini do
      Google), permite o uso de ferramentas externas via APIs e opera de
      forma assíncrona para executar múltiplas tarefas em paralelo, a
      que melhora a eficiência e a velocidade. Ele pode ser acessado em
      <a href="mangaba-ai.vercel.app"
      class="uri">mangaba-ai.vercel.app</a>.</p>
      <p>Entre seus usos práticos, o Mangaba.AI pode ser aplicado para
      automação de processos repetitivos, geração de relatórios com base
      em dados complexos, análise e extração de informações de grandes
      volumes de documentos, construção de assistentes virtuais, e apoio
      à pesquisa e desenvolvimento. Ele procura facilitar a vida de
      desenvolvedores ao fornecer uma API simples e intuitiva para
      definir agentes, atribuir papéis, configurar equipes, e delegar
      tarefas inteiras para essa equipe de IA colaborativa.</p>
      <div class="sourceCode" id="cb113" data-language="Python"
      data-caption="Exemplo de uso do Mangaba.AI."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># para instalação acesse https://github.com/Mangaba-ai/mangaba_ai</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mangaba <span class="im">import</span> Team, Agent</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Criar uma equipe de agentes</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>monitor <span class="op">=</span> Team(<span class="st">&quot;Autorregulação&quot;</span>)</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Adicionar agentes especializados</span></span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a>planejador <span class="op">=</span> Agent(<span class="st">&quot;Plano de Estudo&quot;</span>, role<span class="op">=</span><span class="st">&quot;Gera Plano de Estudo com cronograma baseado no tempo disponível do estudante&quot;</span>)</span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>apoio <span class="op">=</span> Agent(<span class="st">&quot;Suporte Emocional&quot;</span>, role<span class="op">=</span><span class="st">&quot;Motiva o estudante&quot;</span>)</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>pesquisador <span class="op">=</span> Agent(<span class="st">&quot;Guia de Estudo&quot;</span>, role<span class="op">=</span><span class="st">&quot;Busca materiais didáticos para o estudante&quot;</span>)</span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Adicionar agentes à equipe</span></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a>monitor.add_agents([pesquisador, apoio, planejador])</span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir uma tarefa complexa</span></span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> monitor.solve(</span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;Pesquise sobre Redes Neurais Artificiais&quot;</span></span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result.output)</span></code></pre></div>
      <h3 id="fluxos-em-llm-ou-engenharia-de-prompts">Fluxos em LLM (ou
      Engenharia de Prompts)</h3>
      <p>Modelos de Linguagem Grandes (LLMs), como a família GPT, são
      utilizados na compreensão e geração de texto. Uma maneira eficaz e
      relativamente rápida de criar um chatbot funcional é através da
      <strong>engenharia de prompts</strong>. Em vez de codificar regras
      complexas e árvores de decisão manualmente, você "programa" o LLM
      fornecendo-lhe um conjunto detalhado de instruções iniciais (o
      prompt).</p>
      <p>O prompt é o texto inicial que você fornece ao LLM. Ele
      define:</p>
      <ol>
      <li><p><strong>O Papel do Chatbot:</strong> Quem ele é (um
      atendente de pizzaria, um consultor de moda, etc.).</p></li>
      <li><p><strong>O Objetivo da Conversa:</strong> O que ele precisa
      alcançar (vender uma pizza, ajudar a escolher uma roupa, abrir uma
      conta, etc.).</p></li>
      <li><p><strong>As Regras da Conversa:</strong> A sequência exata
      de perguntas a fazer, as opções válidas para cada pergunta, e como
      lidar com diferentes respostas do usuário (lógica
      condicional).</p></li>
      <li><p><strong>O Tom e Estilo:</strong> Se o chatbot deve ser
      formal, informal, amigável, etc. (embora não especificado nos
      exemplos, pode ser adicionado).</p></li>
      <li><p><strong>O Formato da Saída Final:</strong> Como as
      informações coletadas devem ser apresentadas no final.</p></li>
      </ol>
      <h3 class="unnumbered" id="como-funciona">Como Funciona?</h3>
      <ol>
      <li><p><strong>Definição:</strong> Você escreve um prompt
      detalhado que descreve o fluxo da conversa passo a passo.</p></li>
      <li><p><strong>Instrução:</strong> Você alimenta este prompt no
      LLM.</p></li>
      <li><p><strong>Execução:</strong> O LLM usa o prompt como seu guia
      mestre. Ele inicia a conversa com o usuário seguindo o primeiro
      passo definido no prompt, faz as perguntas na ordem especificada,
      valida as respostas (se instruído), segue os caminhos condicionais
      e, finalmente, gera a saída desejada.</p></li>
      <li><p><strong>Iteração:</strong> Se o chatbot não se comportar
      exatamente como esperado, você ajusta e refina o prompt até que
      ele siga as regras perfeitamente.</p></li>
      </ol>
      <p>Algumas Vantagens do uso de Fluxos em Chatbots:</p>
      <ul>
      <li><p><strong>Rapidez:</strong> Muito mais rápido do que
      desenvolver um chatbot tradicional do zero.</p></li>
      <li><p><strong>Flexibilidade:</strong> Fácil de modificar o
      comportamento alterando o prompt.</p></li>
      <li><p><strong>Capacidade Conversacional:</strong> Aproveita a
      habilidade natural do LLM para conversas fluidas.</p></li>
      </ul>
      <p>Algumas Limitações do uso de Fluxos em Chatbots:</p>
      <ul>
      <li><p><strong>Controle Fino:</strong> Pode ser mais difícil
      garantir que sempre siga exatamente um caminho lógico muito
      complexo, embora prompts detalhados minimizem isso.</p></li>
      <li><p><strong>Estado:</strong> Gerenciar estados complexos ao
      longo de conversas muito longas pode exigir técnicas de prompt
      mais avançadas.</p></li>
      </ul>
      <h3 class="unnumbered" id="exemplos-de-requisitos">Exemplos de
      requisitos</h3>
      <p>Vamos simular uma necessidade real de alguns clientes em 5
      exercícios. Dados os requisitos de negócio a seguir, iremos
      implementar os chatbots utilizando LLM. Portanto, primeiro vem o
      requisito de negócio e, depois de apresentados todos os
      requisitos, a solução de sua implantação somente utilizando
      prompts.<br />
      </p>
      <h4 class="unnumbered" id="requisitos">Requisitos</h4>
      <pre data-caption="Requisito de negócio 1 Pizzaria."><code>Construa um chatbot para uma pizzaria. O chatbot será responsável por vender uma pizza.
Verifique com o usuário qual o o tipo de massa desejado da pizza (pan ou fina).
Verifique o recheio (queijo, calabresa ou bacon)
Se o usuário escolheu massa pan verifique qual o recheio da borda (gorgonzola ou cheddar)
Ao final deve ser exibido as opções escolhidas.</code></pre>
      <pre data-caption="Requisito de negócio 2 Loja de Roupas"><code>Construa um chatbot para uma loja de roupas, o chatbot será responsável por vender uma calça ou camisa.
Verifique se o usuário quer uma calça ou uma camisa.
Se o usuário quiser uma calça:
pergunte o tamanho da calça (34, 35 ou 36)
pergunte o tipo de fit da calça pode ser slim fit, regular fit, skinny fit.
Se ele quiser uma camisa:
verifique se a camisa é (P, M ou G)
verifique se ele deseja gola (v, redonda ou polo).
Ao final informe as opções escolhidas com uma mensagem informando que o pedido está sendo processado.</code></pre>
      <pre
      data-caption="Requisito de negócio 3 Empresa de Turismo."><code>Este chatbot deve ser utilizado por uma empresa de turismo para vender um pacote turístico
Verifique com o usuário quais das cidades disponíveis ele quer viajar (maceio, aracaju ou fortaleza)
Se ele for para maceio:
verifique se ele já conhece as belezas naturais da cidade.
sugira os dois pacotes (nove ilhas e orla de alagoas)
Se ele for a aracaju:
verifique com o usuário quais dos dois passeios disponíveis serão escolhidos. existem duisponíveis um na passarela do carangueijo e outro na orla de aracaju.
informe que somente existe passagem de ônibus e verifique se mesmo assim ele quer continuar
Caso ele deseje ir a fortaleza:
informe que o único pacote são as falasias cearenses.
verifique se ele irá de ônibus ou de avião para o ceará
Verifique a forma de pagamento cartão ou débito em todas as opções.
Ao final informe as opções escolhidas com uma mensagem informando que o pedido está sendo processado.</code></pre>
      <pre
      data-caption="Requisito de negócio 4 Banco Financeiro."><code>Crie uma aplicação para um banco que será responsável por abrir uma conta corrente para um usuário.
Verifique se o usuário já tem conta em outros bancos.
Caso o usuário tenha conta em outros bancos verifique se ele quer fazer portabilidade
Verifique o nome do correntista.
Verifique qual o saldo que será depositado, zero ou um outro valor inicial.
Verifique se o usuário quer um empréstimo.
Ao final informe o nome do correntista, se ele quis um empréstimo e se ele fez portabilidade e o valor inicial da conta.</code></pre>
      <pre data-caption="Requisito de negócio 5 Universidade."><code>Desenvolver um chatbot para realização de matricula em duas disciplinas eletivas.
O chatbot apresenta as duas disciplinas eletivas (Inteligência artificial Avançado, Aprendizagem de Máquina)
Verificar se ele tem o pré-requisito introdução a programação para ambas as disciplinas.
Se ele escolher Inteligência artificial avançada necessário confirmar se ele cursou inteligência artificial.
Ao final informe qual o nome das disciplina em que ele se matriculou.</code></pre>
      <h4 class="unnumbered" id="solução">Solução</h4>
      <p>A seguir, mostramos como os fluxos de conversa do exercício
      anterior podem ser traduzidos em prompts para um LLM. Cada prompt
      instrui o modelo a agir como o chatbot específico e seguir as
      regras definidas.</p>
      <pre
      data-caption="Prompt para o LLM do Exemplo 1 Pizzaria."><code>Você é um chatbot de atendimento de uma pizzaria. Sua tarefa é anotar o pedido de pizza de um cliente. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao cliente qual o tipo de massa desejado. As únicas opções válidas são &quot;pan&quot; ou &quot;fina&quot;.
    * Exemplo de pergunta: &quot;Olá! Qual tipo de massa você prefere para sua pizza: pan ou fina?&quot;
2.  Depois que o cliente escolher a massa, pergunte qual o recheio desejado. As únicas opções válidas são &quot;queijo&quot;, &quot;calabresa&quot; ou &quot;bacon&quot;.
    * Exemplo de pergunta: &quot;Ótima escolha! E qual recheio você gostaria: queijo, calabresa ou bacon?&quot;
3.  APENAS SE o cliente escolheu a massa &quot;pan&quot; no passo 1, pergunte qual o recheio da borda. As únicas opções válidas são &quot;gorgonzola&quot; ou &quot;cheddar&quot;.
    * Exemplo de pergunta (apenas para massa pan): &quot;Para a massa pan, temos borda recheada! Você prefere com gorgonzola ou cheddar?&quot;
4.  Após coletar todas as informações necessárias (massa, recheio e recheio da borda, se aplicável), exiba um resumo claro do pedido com todas as opções escolhidas pelo cliente.
    * Exemplo de resumo: &quot;Perfeito! Seu pedido ficou assim: Pizza com massa [massa escolhida], recheio de [recheio escolhido] [se aplicável: e borda recheada com [recheio da borda escolhido]].&quot;
Inicie a conversa agora seguindo o passo 1.</code></pre>
      <pre
      data-caption="Prompt para o LLM do Exemplo 2 Loja de roupas."><code>Você é um chatbot de vendas de uma loja de roupas. Seu objetivo é ajudar o cliente a escolher uma calça ou uma camisa. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao cliente se ele está procurando por uma &quot;calça&quot; ou uma &quot;camisa&quot;.
    * Exemplo de pergunta: &quot;Bem-vindo(a) à nossa loja! Você está procurando por uma calça ou uma camisa hoje?&quot;
2.  SE o cliente responder &quot;calça&quot;:
    a.  Pergunte o tamanho da calça. As únicas opções válidas são &quot;34&quot;, &quot;35&quot; ou &quot;36&quot;.
        * Exemplo de pergunta: &quot;Para calças, qual tamanho você usa: 34, 35 ou 36?&quot;
    b.  Depois do tamanho, pergunte o tipo de fit da calça. As únicas opções válidas são &quot;slim fit&quot;, &quot;regular fit&quot; ou &quot;skinny fit&quot;.
        * Exemplo de pergunta: &quot;E qual tipo de fit você prefere: slim fit, regular fit ou skinny fit?&quot;
3.  SE o cliente responder &quot;camisa&quot;:
    a.  Pergunte o tamanho da camisa. As únicas opções válidas são &quot;P&quot;, &quot;M&quot; ou &quot;G&quot;.
        * Exemplo de pergunta: &quot;Para camisas, qual tamanho você prefere: P, M ou G?&quot;
    b.  Depois do tamanho, pergunte o tipo de gola. As únicas opções válidas são &quot;V&quot;, &quot;redonda&quot; ou &quot;polo&quot;.
        * Exemplo de pergunta: &quot;E qual tipo de gola você gostaria: V, redonda ou polo?&quot;
4.  Após coletar todas as informações (tipo de peça e suas especificações), apresente um resumo das opções escolhidas e informe que o pedido está sendo processado.
    * Exemplo de resumo (Cal\c{c}a): &quot;Entendido! Voc\^e escolheu uma cal\c{c}a tamanho [tamanho] com fit [fit]. Seu pedido est\&#39;a sendo processado.&quot;
    * Exemplo de resumo (Camisa): &quot;Entendido! Você escolheu uma camisa tamanho [tamanho] com gola [gola]. Seu pedido está sendo processado.&quot;
Inicie a conversa agora seguindo o passo 1.</code></pre>
      <pre
      data-caption="Prompt para o LLM do Exemplo 3: Empresa de Turismo."><code>Você é um agente de viagens virtual de uma empresa de turismo. Sua tarefa é ajudar um cliente a escolher e configurar um pacote turístico. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao cliente para qual das cidades disponíveis ele gostaria de viajar. As únicas opções são &quot;Maceió&quot;, &quot;Aracaju&quot; ou &quot;Fortaleza&quot;.
    * Exemplo de pergunta: &quot;Olá! Temos ótimos pacotes para Maceió, Aracaju e Fortaleza. Qual desses destinos te interessa mais?&quot;
2.  SE o cliente escolher &quot;Maceió&quot;:
    a.  Pergunte se ele já conhece as belezas naturais da cidade. (A resposta não altera o fluxo, é apenas conversacional).
        * Exemplo de pergunta: &quot;Maceió é linda! Você já conhece as belezas naturais de lá?&quot;
    b.  Sugira os dois pacotes disponíveis: &quot;Nove Ilhas&quot; e &quot;Orla de Alagoas&quot;. Pergunte qual ele prefere.
        * Exemplo de pergunta: &quot;Temos dois pacotes incríveis em Maceió: &#39;Nove Ilhas&#39; e &#39;Orla de Alagoas&#39;. Qual deles você prefere?&quot;
    c.  Vá para o passo 5.
3.  SE o cliente escolher &quot;Aracaju&quot;:
    a.  Pergunte qual dos dois passeios disponíveis ele prefere: &quot;Passarela do Caranguejo&quot; ou &quot;Orla de Aracaju&quot;.
        * Exemplo de pergunta: &quot;Em Aracaju, temos passeios pela &#39;Passarela do Caranguejo&#39; e pela &#39;Orla de Aracaju&#39;. Qual te atrai mais?&quot;
    b.  Informe ao cliente que para Aracaju, no momento, só temos transporte via ônibus. Pergunte se ele deseja continuar mesmo assim.
        * Exemplo de pergunta: &quot;Importante: para Aracaju, nosso transporte é apenas de ônibus. Podemos continuar com a reserva?&quot;
    c.  Se ele confirmar, vá para o passo 5. Se não, agradeça e encerre.
4.  SE o cliente escolher &quot;Fortaleza&quot;:
    a.  Informe que o pacote disponível é o &quot;Falésias Cearenses&quot;.
        * Exemplo de informação: &quot;Para Fortaleza, temos o pacote especial &#39;Falésias Cearenses&#39;.&quot;
    b.  Pergunte se ele prefere ir de &quot;ônibus&quot; ou &quot;avião&quot; para o Ceará.
        * Exemplo de pergunta: &quot;Como você prefere viajar para o Ceará: de ônibus ou avião?&quot;
    c.  Vá para o passo 5.
5.  Depois de definir o destino, pacote/passeio e transporte (se aplicável), pergunte qual a forma de pagamento preferida. As únicas opções são &quot;cartão&quot; ou &quot;débito&quot;.
    * Exemplo de pergunta: &quot;Para finalizar, como você prefere pagar: cartão ou débito?&quot;
6.  Ao final, apresente um resumo completo das opções escolhidas (destino, pacote/passeio, transporte se aplicável, forma de pagamento) e informe que o pedido está sendo processado.
    * Exemplo de resumo: &quot;Confirmado! Seu pacote para [Destino] inclui [Pacote/Passeio], transporte por [Ônibus/Avião, se aplicável], com pagamento via [Forma de Pagamento]. Seu pedido está sendo processado!&quot;

Inicie a conversa agora seguindo o passo 1.</code></pre>
      <pre
      data-caption="Prompt para o LLM do Exemplo 4: Banco Financeiro."><code>Você é um assistente virtual de um banco e sua função é auxiliar usuários na abertura de uma conta corrente. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Pergunte ao usuário se ele já possui conta em outros bancos. Respostas esperadas: &quot;sim&quot; ou &quot;não&quot;.
    * Exemplo de pergunta: &quot;Bem-vindo(a) ao nosso banco! Para começar, você já possui conta corrente em alguma outra instituição bancária?&quot;
2.  APENAS SE a resposta for &quot;sim&quot;, pergunte se ele gostaria de fazer a portabilidade da conta para o nosso banco. Respostas esperadas: &quot;sim&quot; ou &quot;não&quot;.
    * Exemplo de pergunta: &quot;Entendido. Você gostaria de solicitar a portabilidade da sua conta existente para o nosso banco?&quot;
3.  Pergunte o nome completo do futuro correntista.
    * Exemplo de pergunta: &quot;Por favor, informe o seu nome completo para o cadastro.&quot;
4.  Pergunte qual será o valor do depósito inicial na conta. Informe que pode ser &quot;zero&quot; ou qualquer outro valor.
    * Exemplo de pergunta: &quot;Qual valor você gostaria de depositar inicialmente? Pode ser R$ 0,00 ou outro valor à sua escolha.&quot;
5.  Pergunte se o usuário tem interesse em solicitar um empréstimo pré-aprovado junto com a abertura da conta. Respostas esperadas: &quot;sim&quot; ou &quot;não&quot;.
    * Exemplo de pergunta: &quot;Você teria interesse em verificar uma oferta de empréstimo pré-aprovado neste momento?&quot;
6.  Ao final, apresente um resumo com as informações coletadas: nome do correntista, se solicitou portabilidade (sim/não), se solicitou empréstimo (sim/não) e o valor do depósito inicial.
    * Exemplo de resumo: &quot;Perfeito! Finalizamos a solicitação. Resumo da abertura: Correntista: [Nome Completo], Portabilidade Solicitada: [Sim/Não], Empréstimo Solicitado: [Sim/Não], Depósito Inicial: R$ [Valor].&quot;

Inicie a conversa agora seguindo o passo 1.</code></pre>
      <pre
      data-caption="Prompt para o LLM do Exemplo 5: Universidade."><code>Você é um assistente de matrícula de uma universidade. Sua tarefa é ajudar um aluno a se matricular em até duas disciplinas eletivas. 
Não responda nada fora deste contexto. Diga que não sabe.
Siga EXATAMENTE estes passos:
1.  Apresente as duas disciplinas eletivas disponíveis: &quot;Inteligência Artificial Avançado&quot; e &quot;Aprendizagem de Máquina&quot;.
    * Exemplo de apresentação: &quot;Olá! Temos duas disciplinas eletivas disponíveis para matrícula: &#39;Inteligência Artificial Avançado&#39; e &#39;Aprendizagem de Máquina&#39;.&quot;
2.  Verifique se o aluno possui o pré-requisito obrigatório &quot;Introdução à Programação&quot;, que é necessário para AMBAS as disciplinas. Pergunte se ele já cursou e foi aprovado nesta disciplina. Respostas esperadas: &quot;sim&quot; ou &quot;não&quot;.
    * Exemplo de pergunta: &quot;Para cursar qualquer uma delas, é necessário ter sido aprovado em &#39;Introdução à Programação&#39;. Você já cumpriu esse pré-requisito?&quot;
3.  SE a resposta for &quot;não&quot;, informe que ele não pode se matricular nas eletivas no momento e encerre a conversa.
    * Exemplo de mensagem: &quot;Entendo. Infelizmente, sem o pré-requisito &#39;Introdução à Programação&#39;, não é possível se matricular nestas eletivas agora. Procure a coordenação para mais informações.&quot;
4.  SE a resposta for &quot;sim&quot; (possui o pré-requisito):
    a.  Pergunte em qual(is) das duas disciplinas ele deseja se matricular. Ele pode escolher uma ou ambas.
        * Exemplo de pergunta: &quot;Ótimo! Em qual(is) disciplina(s) você gostaria de se matricular: &#39;Inteligência Artificial Avançado&#39;, &#39;Aprendizagem de Máquina&#39; ou ambas?&quot;
    b.  APENAS SE o aluno escolher &quot;Inteligência Artificial Avançado&quot; (seja sozinha ou junto com a outra), pergunte se ele já cursou a disciplina &quot;Inteligência Artificial&quot;. Respostas esperadas: &quot;sim&quot; ou &quot;não&quot;.
        * Exemplo de pergunta (se escolheu IA Avançado): &quot;Para cursar &#39;Inteligência Artificial Avançado&#39;, é recomendado ter cursado &#39;Inteligência Artificial&#39; anteriormente. Você já cursou essa disciplina?&quot;
        * (Nota: O prompt original não especifica o que fazer se ele NÃO cursou IA. Vamos assumir que ele ainda pode se matricular, mas a pergunta serve como um aviso ou coleta de dados).
    c.  Após coletar as escolhas e a informação sobre IA (se aplicável), informe as disciplinas em que o aluno foi efetivamente matriculado. Liste apenas as disciplinas que ele escolheu E para as quais ele confirmou ter os pré-requisitos verificados neste fluxo (no caso, &#39;Introdução à Programação&#39;).
        * Exemplo de finalização (matriculado em ambas, confirmou IA): &quot;Matrícula realizada com sucesso! Você está matriculado em: Inteligência Artificial Avançado e Aprendizagem de Máquina.&quot;
        * Exemplo de finalização (matriculado apenas em Aprendizagem de Máquina): &quot;Matrícula realizada com sucesso! Você está matriculado em: Aprendizagem de Máquina.&quot;
        * Exemplo de finalização (matriculado em IA Avançado, mesmo sem ter cursado IA antes): &quot;Matrícula realizada com sucesso! Você está matriculado em: Inteligência Artificial Avançado.&quot;
Inicie a conversa agora seguindo o passo 1.</code></pre>
      <p>Veja na Figura <a href="#fig:chat_chatgpt_pizza"
      data-reference-type="ref"
      data-reference="fig:chat_chatgpt_pizza">5.11</a> um exemplo de
      implementação e diálogo quando utilizado o ChatGPT.</p>
      <figure id="fig:chat_chatgpt_pizza">
      <p><img src="./fig/chat_chatgpt_pizza.png" style="width:90.0%"
      alt="image" /> <span id="fig:chat_chatgpt_pizza"
      data-label="fig:chat_chatgpt_pizza"></span></p>
      <figcaption>Figure 29: Chatbot criado com LLM (via
      ChatGPT).</figcaption>
      </figure>
      <p>A qualidade da resposta de um LLM depende muito da clareza e do
      detalhamento do prompt. Quanto mais específicas forem as
      instruções, maior a probabilidade de o chatbot se comportar
      exatamente como desejado. Entretanto, no período em que este livro
      foi escrito, os LLMs eram bons em detectar intenções, mas ainda
      não tão eficientes em seguir instruções complexas. Por isso,
      frameworks de orquestração de agentes, como o mangaba.ia ou o
      crewAI, mostram-se úteis, pois agentes mais atômicos tendem a
      performar melhor. A utilização da orquestração — encadeando
      pequenos agentes com intenções bem definidas e transferindo parte
      das instruções para a comunicação entre eles, com o apoio desses
      frameworks — tem recebido boa aceitação pela comunidade.</p>
      <h2 id="integração-de-técnicas">Integração de Técnicas</h2>
      <p>Nesta seção, vamos explorar como integrar as técnicas
      discutidas nas seções anteriores para construir chatbots
      eficientes. Abordaremos a utilização de Modelos de Linguagem
      Grande (LLMs), <em>Fine-Tuning</em>, Retrieval-Augmented
      Generation (RAG) e LLaMA em um único sistema, visando criar
      experiências de diálogo sofisticadas e personalizadas.</p>
      <p>Desenhando um Chatbot: A construção de um chatbot avançado
      requer a combinação de várias técnicas para garantir que ele seja
      capaz de entender, processar e responder a uma ampla gama de
      consultas de usuários. Vamos revisar os principais
      componentes:</p>
      <ul>
      <li><p>Modelo de Linguagem Grande (LLM): A base para a compreensão
      e geração de linguagem natural.</p></li>
      <li><p><em>Fine-Tuning</em>: Adaptar o LLM a domínios ou tarefas
      específicas.</p></li>
      <li><p>Retrieval-Augmented Generation (RAG): Melhorar a relevância
      e precisão das respostas, combinando recuperação de informações
      com geração de texto.</p></li>
      <li><p>LLaMA: Utilizar um modelo mais eficiente para sistemas de
      produção que precisam balancear desempenho e custo.</p></li>
      </ul>
      <p>Arquitetura de um Chatbot: A arquitetura de um chatbot pode ser
      desenhada de forma modular, combinando diferentes técnicas de
      acordo com a necessidade da aplicação. Primeiro, a entrada do
      usuário é capturada e tokenizada; em seguida, a entrada é
      analisada para determinar a intenção e extrair entidades
      importantes. Se necessário, o chatbot recupera informações
      relevantes de uma base de dados externa. Depois, utiliza-se o LLM,
      potencialmente ajustado com <em>Fine-Tuning</em>, para gerar uma
      resposta baseada na entrada e nas informações recuperadas. Por
      fim, a resposta gerada é enviada de volta ao usuário. Veja na
      Figura <a href="#fig:fluxodadoschatbot" data-reference-type="ref"
      data-reference="fig:fluxodadoschatbot">5.12</a> um diagrama visual
      deste procedimento.</p>
      <figure id="fig:fluxodadoschatbot">
      <p><img src="fig/fluxo_llm_rag2.png" style="width:90.0%"
      alt="image" /> <span id="fig:fluxodadoschatbot"
      data-label="fig:fluxodadoschatbot"></span></p>
      <figcaption>Figure 30: Fluxo de dados em um chatbot.</figcaption>
      </figure>
      <p>Implementação de um Chatbot com LLaMA e RAG: Vamos agora
      implementar um chatbot que utiliza LLaMA como o modelo principal
      para geração de respostas e RAG para recuperar informações
      adicionais, se necessário.</p>
      <p>Configuração Inicial: Primeiro, configuramos os componentes
      principais, como o modelo LLaMA para geração de respostas e DPR
      (Dense Passage Retrieval) para recuperação de informações.
      Recomendamos utilizar o Google Colab com a configuração de
      Processador A100. Os 3 blocos de código a seguir devem ser
      executados sequencialmente, um após o outro.</p>
      <div class="sourceCode" id="cb124" data-language="Python"
      data-caption="Este código prepara dois tipos de modelos: um para geração de texto (LLaMA) e outro para busca de passagens relevantes em textos (DPR), ambos usando modelos pré-treinados da biblioteca Hugging Face Transformers."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM, DPRQuestionEncoder, DPRContextEncoder</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Carregar o tokenizer e o modelo LLaMA</span></span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>llama_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;meta-llama/LLaMA-7B&quot;</span>)</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>llama_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;meta-llama/LLaMA-7B&quot;</span>)</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Configurar DPR para recuperação de passagens</span></span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>question_encoder <span class="op">=</span> DPRQuestionEncoder.from_pretrained( <span class="st">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a>context_encoder <span class="op">=</span> DPRContextEncoder.from_pretrained( <span class="st">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)</span></code></pre></div>
      <pre><code>Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: [&#39;ctx_encoder.bert_model.pooler.dense.bias&#39;, &#39;ctx_encoder.bert_model.pooler.dense.weight&#39;]
- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre>
      <p>Processamento da Entrada e Recuperação de Informações: Em
      seguida, implementamos a lógica para processar a entrada do
      usuário e, se necessário, recuperar informações relevantes de uma
      base de dados. O bloco de código abaixo deve ser executado no
      Google Colab, em outra célula, depois do código anterior.</p>
      <div class="sourceCode" id="cb126" data-language="Python"
      data-caption="Implementa uma função de recuperação de passagens relevantes usando embeddings de linguagem."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Este código deve ser executado na sequencia, depois do código anterior.</span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> retrieve_relevant_passage(query, passages):</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>    query_input <span class="op">=</span> question_tokenizer(query, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a>    query_embedding <span class="op">=</span> question_encoder(</span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a>       <span class="op">**</span>query_input).pooler_output</span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a>    passage_inputs <span class="op">=</span> context_tokenizer(passages, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a>    passage_embeddings <span class="op">=</span> context_encoder(</span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a>      <span class="op">**</span>passage_inputs).pooler_output</span>
<span id="cb126-12"><a href="#cb126-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-13"><a href="#cb126-13" aria-hidden="true" tabindex="-1"></a>    similarity_scores <span class="op">=</span> torch.matmul(query_embedding, passage_embeddings.T)</span>
<span id="cb126-14"><a href="#cb126-14" aria-hidden="true" tabindex="-1"></a>    best_passage_index <span class="op">=</span> torch.argmax(similarity_scores, dim<span class="op">=</span><span class="dv">1</span>).item()</span>
<span id="cb126-15"><a href="#cb126-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-16"><a href="#cb126-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> passages[best_passage_index]</span>
<span id="cb126-17"><a href="#cb126-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-18"><a href="#cb126-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de passagens</span></span>
<span id="cb126-19"><a href="#cb126-19" aria-hidden="true" tabindex="-1"></a>passages <span class="op">=</span> [</span>
<span id="cb126-20"><a href="#cb126-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;LLaMA é um modelo de linguagem desenvolvido pela Meta AI.&quot;</span>,</span>
<span id="cb126-21"><a href="#cb126-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;RAG combina recuperação de informações com geração de texto.&quot;</span>,</span>
<span id="cb126-22"><a href="#cb126-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;GPT-3 é um dos maiores modelos de linguagem disponíveis.&quot;</span></span>
<span id="cb126-23"><a href="#cb126-23" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb126-24"><a href="#cb126-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-25"><a href="#cb126-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrada do usuário</span></span>
<span id="cb126-26"><a href="#cb126-26" aria-hidden="true" tabindex="-1"></a>user_input <span class="op">=</span> <span class="st">&quot;O que é LLaMA?&quot;</span></span>
<span id="cb126-27"><a href="#cb126-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-28"><a href="#cb126-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Recuperar a passagem mais relevante</span></span>
<span id="cb126-29"><a href="#cb126-29" aria-hidden="true" tabindex="-1"></a>relevant_passage <span class="op">=</span> retrieve_relevant_passage(user_input, passages)</span>
<span id="cb126-30"><a href="#cb126-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-31"><a href="#cb126-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Passagem mais relevante: </span><span class="sc">{</span>relevant_passage<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
      <pre><code>Passagem mais relevante: LLaMA é um modelo de linguagem desenvolvido pela Meta AI.</code></pre>
      <p>Geração de Resposta com LLaMA: Finalmente, usamos o modelo
      LLaMA para gerar uma resposta, utilizando tanto a entrada original
      do usuário quanto a passagem recuperada. O bloco de código abaixo
      deve ser executado no Google Colab, em outra célula, depois do
      código anterior.</p>
      <div class="sourceCode" id="cb128" data-language="Python"
      data-caption="Realiza a geração de uma resposta de chatbot usando um modelo LLM carregado."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenar a consulta com a passagem relevante</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> user_input <span class="op">+</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> relevant_passage</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Geração da resposta</span></span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> llama_tokenizer.encode(input_text, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> llama_model.generate(input_ids, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">5</span>, early_stopping<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> llama_tokenizer.decode(generated_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Resposta do chatbot:&quot;</span>, response)</span></code></pre></div>
      <pre><code>Resposta do chatbot: O que é LLaMA? LLaMA é um modelo de linguagem desenvolvido pela Meta AI. LLaMA é um acrônimo para Large Language Model, que significa modelo de linguagem grande. LLaMA é</code></pre>
      <p>Neste exemplo, o chatbot gera uma resposta baseada na
      combinação da entrada do usuário e na informação relevante
      recuperada, criando uma resposta informativa e
      contextualizada.</p>
      <p>Desafios na Implementação de Chatbots: Implementar chatbots com
      múltiplas técnicas apresenta vários desafios, tais como: combinar
      modelos como LLaMA e DPR pode ser computacionalmente intensivo,
      especialmente em aplicações em tempo real; o <em>Fine-Tuning</em>
      e a configuração de modelos precisam ser bem ajustados para
      garantir que o chatbot seja eficaz e relevante em suas respostas;
      além disso, garantir que os diferentes componentes (recuperação,
      geração, etc.) funcionem de maneira coesa pode ser desafiador.
      Mesmo assim, os chatbots podem ser aplicados em uma variedade de
      cenários, como: fornecer suporte automatizado, personalizado, em
      tempo real; ajudar na triagem de sintomas ou fornecer informações
      médicas básicas; ou mesmo criar tutores virtuais que podem
      responder a perguntas de estudantes de maneira precisa e
      contextualizada.</p>
      <h2 id="api-e-playground">API e Playground</h2>
      <p>Geralmente, a desenvolvedora de um modelo LLM fornece uma
      interface web para o usuário conversar com o seu modelo em forma
      de chatbot e fornece uma API (com um playground) para quem deseja
      integrar a solução em suas aplicações. Assim é o ChatGPT, da
      empresa OpenAI. Ela cobra um valor fixo para disponibilizar aos
      assinantes acesso completo aos recursos do seu chatbot, inclusive
      com o recurso de SearchGPT (uma busca na web), lousa, geração de
      vídeos, imagens etc. Já para acesso à API, a modalidade de
      cobrança é paga-pelo-uso. Veja na Figura <a href="#fig:chat1"
      data-reference-type="ref" data-reference="fig:chat1">5.13</a> uma
      imagem do chatbot da OpenAI e na Figura <a href="#fig:chat2"
      data-reference-type="ref" data-reference="fig:chat2">5.14</a> o
      playground da OpenAI.</p>
      <figure id="fig:chat1">
      <p><img src="fig/chatgpt1.png" style="width:90.0%" alt="image" />
      <span id="fig:chat1" data-label="fig:chat1"></span></p>
      <figcaption>Figure 31: Chatbot da OpenAI.</figcaption>
      </figure>
      <figure id="fig:chat2">
      <p><img src="fig/chat2.png" style="width:90.0%" alt="image" />
      <span id="fig:chat2" data-label="fig:chat2"></span></p>
      <figcaption>Figure 32: Playground da openai.</figcaption>
      </figure>
      <p>Existe uma provedora de inferência chamada GROQ <a
      href="https://groq.com/" class="uri">https://groq.com/</a>. O GROQ
      (com Q) é uma fornecedora de infraestrutura para inferência em
      modelos de linguagem de grande porte (LLM). Esta empresa vende um
      serviço de inferência mais rápido do que seus concorrentes. No
      GROQ, podemos utilizar vários modelos LLMs disponíveis
      abertamente, tais como o Llama da Meta, o Gemma do Google ou o
      Mixtral da Mixtral AI. O ChatGPT não está disponível no GROQ pois
      ele não é open source.</p>
      <p>Entre outros modelos, utilizam uma versão do LLaMA em seu
      Chatbot GROQ, porém seu foco é a API, que pode ser testada no
      playground GROQ. O Playground funciona no formato pague-pelo-uso.
      O Chatbot GROQ também consome os mesmos créditos pague-pelo-uso,
      não tendo uma assinatura à parte, como no caso do chatbot do
      ChatGPT. Veja na Figura <a href="#fig:chat3"
      data-reference-type="ref" data-reference="fig:chat3">5.15</a> o
      Chatbot do GROQ e na Figura <a href="#fig:chat4"
      data-reference-type="ref" data-reference="fig:chat4">5.16</a> o
      Playground do GROQ. Note que no Chatbot do GROQ não existe o
      recurso de histórico como no ChatGPT.</p>
      <figure id="fig:chat3">
      <p><img src="fig/groq3.png" style="width:90.0%" alt="image" />
      <span id="fig:chat3" data-label="fig:chat3"></span></p>
      <figcaption>Figure 33: Chatbot do GROQ.</figcaption>
      </figure>
      <figure id="fig:chat4">
      <p><img src="fig/gro4.png" style="width:90.0%" alt="image" />
      <span id="fig:chat4" data-label="fig:chat4"></span></p>
      <figcaption>Figure 34: Playground do GROQ.</figcaption>
      </figure>
      <p>Já o Grok (com K) é utilizado em referência ao chatbot da
      empresa xAI. Ele é um chatbot e está disponível para quem tem o
      acesso premium ao X (ex-twitter), enquanto a API Grok pode ser
      acessada na forma pague-pelo-uso. Inclusive, o Grok está
      oferecendo este ano 25 R$ para quem deseja testar o serviço, ou
      seja, é possível utilizar a API do Grok sem desembolsar nada neste
      momento. Porém, o Grok não tem um playground avançado, logo, para
      usar a API do Grok é necessário utilizar uma ferramenta gráfica ou
      código externo, por exemplo: LLM Studio, JAN AI, GPT4ALL ou o
      excelente Msty app. Veja na Figura <a href="#fig:chat5"
      data-reference-type="ref" data-reference="fig:chat5">5.17</a> o
      chatbot do Grok e na Figura <a href="#fig:chat6"
      data-reference-type="ref" data-reference="fig:chat6">5.18</a> o
      playground do Grok.</p>
      <figure id="fig:chat5">
      <p><img src="fig/chatgpt5.png" style="width:90.0%" alt="image" />
      <span id="fig:chat5" data-label="fig:chat5"></span></p>
      <figcaption>Figure 35: Interface do Grok.</figcaption>
      </figure>
      <figure id="fig:chat6">
      <p><img src="fig/chat7.png" style="width:90.0%" alt="image" />
      <span id="fig:chat6" data-label="fig:chat6"></span></p>
      <figcaption>Figure 36: Playground do Grok.</figcaption>
      </figure>
      <h2 id="resumo-2">Resumo</h2>
      <p>Os Modelos de Linguagem Grande (LLMs) têm desempenhado um papel
      central na revolução do Processamento de Linguagem Natural (PLN).
      Esses modelos, que incluem variantes como GPT, BERT, e seus
      sucessores, são capazes de realizar uma ampla gama de tarefas,
      desde geração de texto até compreensão profunda de linguagem,
      graças ao seu treinamento em grandes volumes de dados
      textuais.</p>
      <p>Esses modelos são alimentados por vastos conjuntos de dados
      textuais e utilizam técnicas de aprendizado profundo,
      particularmente redes neurais, para aprender padrões, contextos e
      nuances da linguagem. Eles são capazes de realizar uma variedade
      de tarefas linguísticas, incluindo tradução automática, geração de
      texto, resumo de informações, resposta a perguntas e até mesmo a
      criação de diálogos interativos. Funcionam com base em
      arquiteturas complexas, como a <em>Transformer</em>, que permite
      que o modelo preste atenção a diferentes partes de um texto
      simultaneamente, facilitando a compreensão do contexto e das
      relações semânticas entre palavras e frases.</p>
      <p>O treinamento desses modelos envolve a exposição a enormes
      quantidades de texto, o que lhes permite desenvolver uma
      compreensão profunda da gramática, do vocabulário e dos estilos de
      comunicação. No entanto, essa capacidade de gerar texto coerente e
      relevante também levanta questões éticas e de responsabilidade,
      especialmente em relação à desinformação, viés algorítmico e
      privacidade.</p>
      <p>Eles hoje representam um marco significativo na evolução da
      inteligência artificial, assim como foi o Eliza em sua época,
      porém com um impacto maior, oferecendo ferramentas para a
      interação humano-computador e abrindo novas possibilidades para
      aplicações em diversas áreas, como educação <span class="citation"
      data-cites="Neo2020Engajando">(N. G. da Silva, Pereira, and Silva
      Neo 2020)</span>, atendimento ao cliente ou criação de
      conteúdo.</p>
      <p>Neste capítulo, exploramos a arquitetura <em>Transformer</em>,
      que revolucionou o Processamento de Linguagem Natural ao
      introduzir o mecanismo de atenção. Discutimos como os
      <em>Transformers</em> funcionam e como eles são aplicados em
      modelos populares como BERT e GPT. Com exemplos em Python, vimos
      como utilizar esses modelos para tarefas de PLN.</p>
      <p>Também, exploramos a técnica de Retrieval-Augmented Generation
      (RAG), uma abordagem que combina a recuperação de informações com
      a geração de texto. Vimos como implementar RAG em Python usando
      modelos como DPR e BART, e discutimos as aplicações e desafios
      dessa técnica.</p>
      <p>Adicionalmente, exploramos os LLMs incluindo suas arquiteturas,
      técnicas de treinamento e principais aplicações. Modelos como GPT,
      BERT, T5 e XLNet exemplificam como os LLMs estão redefinindo o
      campo do PLN. Com exemplos práticos, demonstramos como esses
      modelos podem ser aplicados em uma variedade de tarefas.</p>
      <p>Outro tópico explorado neste capítulo foi o processo de
      <em>Fine-Tuning</em> de modelos pré-treinados, discutindo sua
      importância, desafios e aplicações práticas. O
      <em>Fine-Tuning</em> permite que Modelos de Linguagem Grande sejam
      adaptados para tarefas específicas com eficiência, tornando-os
      extremamente versáteis em diversas aplicações de PLN.</p>
      <p>Também, exploramos o LLaMA, um modelo de linguagem projetado
      para ser eficiente e acessível sem comprometer a capacidade de
      realizar tarefas complexas de PLN. Com uma arquitetura otimizada e
      foco em eficiência computacional, LLaMA oferece uma alternativa
      prática para modelos gigantescos como o GPT-3. Vimos também como
      implementar LLaMA em Python para tarefas de geração de texto e
      discutimos suas aplicações e limitações.</p>
      <p>Além disso, integramos várias técnicas discutidas em capítulos
      anteriores para criar um chatbot capaz de fornecer respostas
      precisas e contextualizadas utilizando LLaMA e RAG. A
      implementação dessas técnicas oferece uma pequena base para o
      desenvolvimento de sistemas de diálogo eficientes.</p>
      <h2 id="exercícios-3">Exercícios</h2>
      <ol>
      <li><p><strong>Qual é a principal inovação introduzida pelos
      Transformers em relação a modelos anteriores?</strong></p>
      <ol>
      <li><p>A) O uso de redes neurais convolucionais para processamento
      de texto.</p></li>
      <li><p>B) A capacidade de processar sequências em paralelo,
      utilizando mecanismos de atenção.</p></li>
      <li><p>C) A utilização de redes neurais recorrentes para manter o
      contexto ao longo das sequências.</p></li>
      <li><p>D) A introdução de embeddings de palavras em redes
      neurais.</p></li>
      </ol></li>
      <li><p><strong>O que é o "mecanismo de atenção" em
      Transformers?</strong></p>
      <ol>
      <li><p>A) Um mecanismo que aumenta a frequência das palavras mais
      comuns.</p></li>
      <li><p>B) Um algoritmo que distribui o foco igualmente entre todas
      as palavras em uma sequência.</p></li>
      <li><p>C) Um método que permite ao modelo focar em partes
      específicas da entrada ao gerar uma saída, ponderando a
      importância das diferentes partes da sequência.</p></li>
      <li><p>D) Uma técnica para ignorar palavras irrelevantes em um
      texto.</p></li>
      </ol></li>
      <li><p><strong>O que significa o termo "self-attention" no
      contexto dos Transformers?</strong></p>
      <ol>
      <li><p>A) O modelo ajusta automaticamente seu aprendizado com base
      no erro de previsão.</p></li>
      <li><p>B) O modelo atribui pesos a diferentes partes da sequência
      de entrada para determinar quais partes são mais relevantes ao
      processar cada palavra.</p></li>
      <li><p>C) O modelo decide qual sequência de palavras é mais
      provável com base em exemplos anteriores.</p></li>
      <li><p>D) O modelo ignora todas as palavras, exceto a palavra
      alvo.</p></li>
      </ol></li>
      <li><p><strong>Qual é o papel do "positional encoding" nos
      Transformers?</strong></p>
      <ol>
      <li><p>A) Ajudar o modelo a entender a ordem das palavras em uma
      sequência, já que o Transformer processa todas as palavras em
      paralelo.</p></li>
      <li><p>B) Substituir palavras desconhecidas por
      sinônimos.</p></li>
      <li><p>C) Compactar a representação de texto para economizar
      espaço de armazenamento.</p></li>
      <li><p>D) Permitir ao modelo focar em palavras específicas dentro
      de uma frase.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes afirmações é verdadeira sobre a
      arquitetura Transformer?</strong></p>
      <ol>
      <li><p>A) Os Transformers utilizam camadas convolucionais para
      processar texto.</p></li>
      <li><p>B) Os Transformers dependem exclusivamente de redes neurais
      recorrentes para manter o contexto.</p></li>
      <li><p>C) Os Transformers eliminam a necessidade de processar
      sequências em ordem, graças ao mecanismo de atenção e à
      paralelização.</p></li>
      <li><p>D) Os Transformers não são capazes de lidar com longas
      sequências de texto devido a limitações de memória.</p></li>
      </ol></li>
      <li><p><strong>Qual é a principal característica dos Modelos de
      Linguagem Grande (LLMs) como GPT e BERT?</strong></p>
      <ol>
      <li><p>A) Eles são baseados exclusivamente em redes neurais
      convolucionais.</p></li>
      <li><p>B) Eles são treinados em grandes volumes de dados textuais
      e são capazes de realizar tarefas de PLN sem a necessidade de
      re-treinamento específico para cada tarefa.</p></li>
      <li><p>C) Eles dependem exclusivamente de dicionários
      pré-definidos para gerar respostas.</p></li>
      <li><p>D) Eles utilizam redes neurais recorrentes para prever a
      próxima palavra em uma sequência.</p></li>
      </ol></li>
      <li><p><strong>Qual é a diferença principal entre os modelos GPT e
      BERT?</strong></p>
      <ol>
      <li><p>A) O GPT utiliza um mecanismo de atenção bidirecional,
      enquanto o BERT utiliza atenção unidirecional.</p></li>
      <li><p>B) O GPT é um modelo autoregressivo que gera texto palavra
      por palavra, enquanto o BERT é um modelo pré-treinado bidirecional
      para tarefas de preenchimento de máscara.</p></li>
      <li><p>C) O BERT é projetado apenas para geração de texto,
      enquanto o GPT é projetado apenas para compreensão de
      texto.</p></li>
      <li><p>D) O BERT utiliza aprendizado supervisionado, enquanto o
      GPT utiliza aprendizado não supervisionado.</p></li>
      </ol></li>
      <li><p><strong>Qual é uma das principais vantagens do ajuste fino
      de modelos pré-treinados?</strong></p>
      <ol>
      <li><p>A) Reduz a necessidade de dados de treinamento específicos
      para uma nova tarefa.</p></li>
      <li><p>B) Garante que o modelo não precisará ser treinado
      novamente.</p></li>
      <li><p>C) Remove completamente a necessidade de validação
      cruzada.</p></li>
      <li><p>D) Evita qualquer risco de overfitting.</p></li>
      </ol></li>
      <li><p><strong>Em que tipo de tarefa o ajuste fino é
      particularmente útil?</strong></p>
      <ol>
      <li><p>Tarefas que exigem uma grande quantidade de dados não
      rotulados.</p></li>
      <li><p>Tarefas específicas que requerem adaptação de um modelo
      geral para um domínio particular.</p></li>
      <li><p>Tarefas que não envolvem aprendizado de máquina.</p></li>
      <li><p>Tarefas de compressão de dados para armazenamento
      eficiente.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes abordagens é recomendada ao
      realizar ajuste fino para evitar overfitting?</strong></p>
      <ol>
      <li><p>Aumentar a taxa de aprendizado para forçar o modelo a
      aprender rapidamente.</p></li>
      <li><p>Congelar algumas camadas do modelo pré-treinado e treinar
      apenas as camadas superiores.</p></li>
      <li><p>Reduzir drasticamente o tamanho do conjunto de dados de
      treinamento.</p></li>
      <li><p>Utilizar apenas um pequeno subconjunto do modelo
      pré-treinado.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes técnicas pode ser usada para
      melhorar o ajuste fino em um modelo pré-treinado?</strong></p>
      <ol>
      <li><p>Aumentar o tamanho do lote para melhorar a estabilidade do
      treinamento.</p></li>
      <li><p>Implementar o decaimento da taxa de aprendizado ao longo do
      treinamento.</p></li>
      <li><p>Treinar o modelo apenas por uma época para evitar
      overfitting.</p></li>
      <li><p>Usar técnicas de normalização de batch para manter a média
      e variância constantes.</p></li>
      </ol></li>
      <li><p><strong>O que é Retrieval-Augmented Generation
      (RAG)?</strong></p>
      <ol>
      <li><p>Uma técnica que combina a recuperação de informações com a
      geração de texto para criar respostas mais informadas e
      contextuais.</p></li>
      <li><p>Um método de compressão de texto que reduz o tamanho dos
      dados sem perda de informação.</p></li>
      <li><p>Uma abordagem para treinar modelos de linguagem
      exclusivamente em dados não rotulados.</p></li>
      <li><p>Uma técnica para traduzir textos entre diferentes
      idiomas.</p></li>
      </ol></li>
      <li><p><strong>Qual é a principal vantagem de usar RAG em
      chatbots?</strong></p>
      <ol>
      <li><p>A capacidade de gerar respostas baseadas em dados estáticos
      sem necessidade de atualização.</p></li>
      <li><p>A habilidade de integrar informações externas e
      atualizadas, permitindo respostas mais precisas e
      relevantes.</p></li>
      <li><p>A eliminação da necessidade de modelos de linguagem
      grande.</p></li>
      <li><p>A redução dos custos de treinamento de modelos.</p></li>
      </ol></li>
      <li><p><strong>No processo de RAG, qual é o papel do componente de
      "recuperação"?</strong></p>
      <ol>
      <li><p>Gerar novas informações com base em uma sequência de texto
      fornecida.</p></li>
      <li><p>Recuperar documentos, passagens ou dados relevantes de uma
      base de conhecimento para serem usados na geração de uma
      resposta.</p></li>
      <li><p>Executar a tradução de texto de um idioma para
      outro.</p></li>
      <li><p>Classificar textos em diferentes categorias.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes estratégias é usada para
      melhorar a precisão das respostas em um sistema RAG?</strong></p>
      <ol>
      <li><p>Utilizar um modelo de linguagem unidirecional.</p></li>
      <li><p>Combinar a recuperação de informações com a geração de
      texto, onde a informação recuperada guia a resposta
      gerada.</p></li>
      <li><p>Implementar somente a geração de texto sem recuperação de
      informações.</p></li>
      <li><p>Usar exclusivamente redes neurais convolucionais para
      processamento de texto.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes afirmativas é verdadeira sobre a
      arquitetura RAG?</strong></p>
      <ol>
      <li><p>RAG utiliza apenas modelos de recuperação e não depende de
      modelos de geração de texto.</p></li>
      <li><p>RAG é eficiente para gerar respostas em tempo real com base
      em grandes volumes de dados externos.</p></li>
      <li><p>RAG não é capaz de integrar informações externas ao
      contexto de uma conversa.</p></li>
      <li><p>RAG é uma abordagem exclusiva para análise de sentimentos
      em texto.</p></li>
      </ol></li>
      <li><p><strong>Qual é o principal objetivo do LLaMA (Large
      Language Model Meta AI)?</strong></p>
      <ol>
      <li><p>Criar um modelo de linguagem massivo e pesado para tarefas
      específicas.</p></li>
      <li><p>Oferecer um modelo de linguagem grande e eficiente que pode
      ser treinado e implantado com menor custo computacional.</p></li>
      <li><p>Desenvolver um modelo de linguagem exclusivamente para
      tarefas de tradução automática.</p></li>
      <li><p>Implementar um modelo de linguagem focado apenas em
      reconhecimento de voz.</p></li>
      </ol></li>
      <li><p><strong>Como o LLaMA se diferencia de outros Modelos de
      Linguagem Grande (LLMs) como GPT-3?</strong></p>
      <ol>
      <li><p>LLaMA é um modelo maior e mais caro para treinar do que
      GPT-3.</p></li>
      <li><p>LLaMA é projetado para ser mais leve e eficiente, com
      diferentes tamanhos de modelo, enquanto ainda oferece alto
      desempenho em tarefas de PLN.</p></li>
      <li><p>LLaMA só pode ser utilizado para tarefas de visão
      computacional.</p></li>
      <li><p>LLaMA depende de dados estruturados enquanto GPT-3 usa
      dados não estruturados.</p></li>
      </ol></li>
      <li><p><strong>Em que contexto o LLaMA seria especialmente
      vantajoso para ser utilizado?</strong></p>
      <ol>
      <li><p>Em dispositivos com recursos computacionais limitados, onde
      modelos grandes como GPT-3 não podem ser executados
      eficientemente.</p></li>
      <li><p>Em servidores de alto desempenho que exigem modelos
      extremamente grandes.</p></li>
      <li><p>Em ambientes que não necessitam de processamento de
      linguagem natural.</p></li>
      <li><p>Para operações que requerem apenas reconhecimento de fala
      em tempo real.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes afirmações é verdadeira sobre a
      arquitetura do LLaMA?</strong></p>
      <ol>
      <li><p>LLaMA é baseado em uma arquitetura de redes neurais
      convolucionais.</p></li>
      <li><p>LLaMA utiliza a arquitetura Transformer, otimizada para
      eficiência em termos de parâmetros e recursos
      computacionais.</p></li>
      <li><p>LLaMA não é capaz de realizar tarefas de compreensão de
      texto.</p></li>
      <li><p>LLaMA foi projetado exclusivamente para tarefas de visão
      computacional.</p></li>
      </ol></li>
      <li><p><strong>Qual é uma das principais aplicações do LLaMA em
      chatbots?</strong></p>
      <ol>
      <li><p>Tradução automática de textos literários
      complexos.</p></li>
      <li><p>Implementação de assistentes virtuais que precisam operar
      em dispositivos móveis com recursos limitados.</p></li>
      <li><p>Análise de grandes volumes de imagens e vídeos.</p></li>
      <li><p>Classificação de sons em ambientes ruidosos.</p></li>
      </ol></li>
      <li><p><strong>Em que tarefa o BERT se destaca em comparação ao
      GPT?</strong></p>
      <ol>
      <li><p>Tradução automática</p></li>
      <li><p>Geração de texto criativo</p></li>
      <li><p>Preenchimento de lacunas em uma frase (masked language
      modeling)</p></li>
      <li><p>Criação de imagens a partir de descrições textuais</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes afirmações é verdadeira sobre o
      modelo GPT?</strong></p>
      <ol>
      <li><p>O GPT utiliza uma abordagem bidirecional para entender o
      contexto ao redor de uma palavra em uma frase.</p></li>
      <li><p>O GPT é um modelo autoregressivo que gera texto com base
      nas palavras anteriores da sequência.</p></li>
      <li><p>O GPT é incapaz de realizar tarefas de compreensão de
      texto.</p></li>
      <li><p>O GPT é treinado apenas em pequenas bases de dados
      altamente especializadas.</p></li>
      </ol></li>
      <li><p><strong>Qual é um dos principais usos de modelos como GPT e
      BERT em chatbots?</strong></p>
      <ol>
      <li><p>A tradução automática de grandes textos
      literários.</p></li>
      <li><p>A geração de respostas naturais e coerentes em conversas
      com os usuários, simulando uma interação humana.</p></li>
      <li><p>A análise de imagens e vídeos para identificar
      objetos.</p></li>
      <li><p>A classificação de sons e ruídos em diferentes
      ambientes.</p></li>
      </ol></li>
      <li><p><strong>Qual é o principal objetivo ao integrar técnicas
      avançadas como RAG, Fine-Tuning e LLaMA em chatbots?</strong></p>
      <ol>
      <li><p>Criar chatbots que operam exclusivamente em dispositivos
      móveis.</p></li>
      <li><p>Melhorar a precisão, relevância e capacidade de adaptação
      dos chatbots em diferentes cenários.</p></li>
      <li><p>Reduzir o tamanho do modelo ao mínimo possível.</p></li>
      <li><p>Evitar o uso de inteligência artificial em
      chatbots.</p></li>
      </ol></li>
      <li><p><strong>Como a técnica Retrieval-Augmented Generation (RAG)
      contribui para a eficiência de chatbots?</strong></p>
      <ol>
      <li><p>Reduzindo o tempo de treinamento dos modelos.</p></li>
      <li><p>Permitindo que o chatbot acesse e utilize informações
      externas para gerar respostas mais precisas e
      contextuais.</p></li>
      <li><p>Substituindo completamente o processo de fine-tuning em
      modelos grandes.</p></li>
      <li><p>Facilitando a compressão de dados de texto em formato
      binário.</p></li>
      </ol></li>
      <li><p><strong>Qual é a vantagem de utilizar Fine-Tuning em um
      modelo como o LLaMA antes de integrá-lo em um
      chatbot?</strong></p>
      <ol>
      <li><p>Reduzir o custo de desenvolvimento do chatbot.</p></li>
      <li><p>Adaptar o modelo para responder de maneira mais eficaz a
      consultas específicas de um domínio particular.</p></li>
      <li><p>Garantir que o modelo funcione apenas em idiomas
      específicos.</p></li>
      <li><p>Aumentar o número de parâmetros do modelo para melhorar a
      precisão.</p></li>
      </ol></li>
      <li><p><strong>Por que é importante considerar a escalabilidade ao
      integrar técnicas avançadas em chatbots?</strong></p>
      <ol>
      <li><p>Para garantir que o chatbot possa ser implantado em
      múltiplos idiomas sem qualquer modificação.</p></li>
      <li><p>ara assegurar que o chatbot possa lidar com um grande
      volume de interações simultâneas sem perda de desempenho.</p></li>
      <li><p>Para eliminar a necessidade de armazenamento de
      dados.</p></li>
      <li><p>Para garantir que o chatbot possa operar sem qualquer
      conexão à internet.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes estratégias pode ajudar a
      melhorar a personalização das respostas de um chatbot utilizando
      técnicas avançadas?</strong></p>
      <ol>
      <li><p>Implementar caching de respostas comuns.</p></li>
      <li><p>Utilizar dados históricos de interações para ajustar as
      respostas às preferências do usuário.</p></li>
      <li><p>Reduzir o número de camadas no modelo para melhorar a
      eficiência.</p></li>
      <li><p>Evitar o uso de técnicas de machine learning para gerar
      respostas.</p></li>
      </ol></li>
      </ol>
      <h1 id="conclusao">Conclusão</h1>
      <div class="epigraph">
      <p>“Uma vez que um programa é explicado em linguagem
      suficientemente simples o observador diz para si mesmo “Eu poderia
      ter escrito isso” ”.</p>
      <p>Joseph Weizembaum</p>
      </div>
      <div class="myboxobj">
      <p><span>Objetivo</span> Refletir sobre ética, privacidade, viés
      algorítmico e perspectivas futuras dos chatbots, incentivando um
      uso responsável e inovador dessas tecnologias.</p>
      </div>
      <h2 id="considerações-éticas">Considerações Éticas</h2>
      <p>O desenvolvimento de chatbots não envolve apenas desafios
      técnicos. À medida que esses sistemas se tornam cada vez mais
      sofisticados e amplamente utilizados, questões éticas ganham
      destaque. O uso de chatbots precisa ser entendido, experimentado e
      avaliado, para identificarmos os impactos de tal utilização,
      mediante experimentos, como por exemplo <span class="citation"
      data-cites="Neo2020Engajando">N. G. da Silva, Pereira, and Silva
      Neo (2020)</span>, que avaliaram seu uso em sala de aula com uma
      turma de alunos.</p>
      <p>Os impactos da Inteligência Artificial(IA) no bem-estar humano
      – positivo ou negativo – são mais complexos do que às vezes se
      supõe <span class="citation" data-cites="schiff2020ieee">(Schiff
      et al. 2020)</span>. No campo da ética, algumas abordagens
      específicas, tal como a aristotélica, preconizam a noção de
      felicidade como cerne da conduta ética. Um representante da
      modernidade é a filosofia kantiana, que estabelece a distinção
      entre moralidade e civilidade. Kant traz à tona a noção de que o
      moralmente correto não é apenas a ação externa do indivíduo, pois
      o indicativo externo só pode ser mensurado pelas leis e normas.
      Uma moralidade verdadeiramente ética seria aquela que parte de uma
      convicção; para ele, “nós somos civilizados até a saturação por
      toda espécie de boas maneiras e decoro social. Mas ainda falta
      muito para nos considerarmos moralizados” <span class="citation"
      data-cites="kant2023fundamentaccao">(Kant 2023)</span>. Nessa
      mesma perspectiva, Kant elabora seu imperativo categórico, uma
      máxima norteadora da ação ética: “age de tal modo que tua máxima
      possa ser universalizada” <span class="citation"
      data-cites="kant2017critica">(Kant 2017)</span>.</p>
      <p>Hans Jonas, a partir de suas reflexões éticas acerca do meio
      ambiente, se opõe claramente à ética kantiana, Jonas assevera:
      “Age de tal forma que os efeitos da tua ação sejam compatíveis com
      a permanência de uma vida autenticamente humana sobre a terra.”
      <span class="citation" data-cites="jonas2006principio">(Jonas
      2006)</span>. Jonas alarga a noção de ética, descentrando-a das
      ações temporais e projetando-as para o futuro; a ação do indivíduo
      é responsável pelos seus desdobramentos futuros.</p>
      <p>Ao longo dos anos, importantes esforços de pesquisa têm sido
      dedicados à IA, e em especial aos chatbots. Tradicionalmente, um
      dos aspectos que mais chamou a atenção foram suas capacidades de
      interação destes programas com os seres humanos. Por exemplo, o
      Eliza foi um dos primeiros e mais influentes chatbots que se
      baseou em correspondência de padrões para criar esta ilusão de
      inteligência <span class="citation"
      data-cites="Weizenbaum1996">(Weizenbaum 1966)</span>.</p>
      <p>É sempre desafiador pensar na perspectiva moral e ética na
      relação tecnologia e usuário. Faz-se necessário refletir em torno
      da dicotomia moral e ética, pois, embora muitas vezes associados
      como elementos idênticos, possuem especificidades. São justamente
      essas especificidades que tornam candente o ensino da ética em
      sala de aula. Ora, podemos asseverar que ética e moral se coadunam
      quando pensadas como elementos do agir humano. Juízos de valor,
      noções de bem e mal, normas de conduta, são elementos comuns à
      ética e à moral. Entretanto, ética e moral separam-se quando
      entendemos que esta última se concentra nos valores vigentes de
      uma sociedade. Assim, tempo e espaço são elementos que alteram a
      moral, ou melhor, as morais. Dito de outra forma, a conduta
      valorada como correta em um período pode não ser em outro, bem
      como o comportamento criminoso de uma região pode, ao mesmo tempo,
      ser correto em outra. Esse veio que particulariza as ações dos
      indivíduos, de acordo com a sociedade em que vivemos, chamamos de
      moral.</p>
      <p>Por ética, em linhas gerais, consideramos as reflexões
      filosóficas que se desdobram sobre os diversos sistemas morais. O
      processo de interação homem-tecnologia deve sempre partir, em um
      movimento dialético, dos valores já sedimentados pelos usuários em
      questão. Não há grau zero do conhecimento. Ao tratarmos pessoas,
      deparamo-nos com seus sistemas morais, cristalizados desde a
      infância. O papel da filosofia, assim, não é de ser a demolidora
      dos valores. Entendemos que as diversas perspectivas filosóficas
      constituem-se essencialmente radicais, posto que se debruçam sobre
      a raiz e universalidade dos problemas.</p>
      <p>A filosofia pode ser utilizada primordialmente como elemento de
      crítica, sendo esta, em termos kantianos, uma “condição de
      possibilidade”. Intentando problematizar a dificuldade desta
      interação, compreendemos como a formação social da moral nos
      indivíduos necessita ser pensada à luz da crítica. Os usos da
      tecnologia, sobremaneira, impuseram uma dinâmica até então não
      empreendida antes do seu surgimento: a velocidade das mudanças, a
      internet e, recentemente, os chatbots aceleram o acesso à
      informação, tornando descompassada sua relação com o conhecimento.
      Porém, a filosofia evidencia a necessidade de ajuste do compasso
      entre tecnologia, conhecimento e seres humanos.</p>
      <p>A tecnologia pode ser aliada aos processos humanos e pode
      minimizar o cenário das dificuldades vividas. Porém, não basta
      simplesmente concluir que o benefício surge única e exclusivamente
      a partir da simples adoção dos chatbots. A simples incorporação
      dos chatbots ou uso deles não gera processos de inovação e
      melhoria; estes são determinados a partir dos usos específicos que
      parecem ter a capacidade de desencadear esses processos.</p>
      <p>Temos diversos desafios pela frente, entre eles: Como garantir
      um uso eficiente dos chatbots e qual o impacto social desta
      interação? É possível asseverar que há um impacto social em
      refletir sobre a ética no uso dos chatbots. É justamente pela
      falta de reflexão e crítica que temos perpetuado em nossa
      sociedade sistemas de dominação e corrupção. A naturalização
      acrítica dos valores vigentes permite que tais valores sejam
      transmitidos como verdades, ocultando muitas vezes os interesses
      individuais em detrimento dos coletivos.</p>
      <p>Desenvolvedores de chatbots têm a responsabilidade de garantir
      que seus sistemas sejam projetados e operados de maneira ética.
      Algumas recomendações podem incluir auditorias regulares para
      implementar revisões regulares de privacidade, segurança e viés
      para garantir a conformidade com os princípios éticos; o envio de
      notificações para informar os usuários de forma clara quando estão
      interagindo com um chatbot, e não com um humano, para evitar
      confusões. Além de processos para garantir que as empresas possam
      ser responsabilizadas por suas ações.</p>
      <h2 id="privacidade-e-proteção-de-dados">Privacidade e Proteção de
      Dados</h2>
      <p>Os chatbots modernos, especialmente aqueles implantados em
      plataformas de atendimento ao cliente e assistentes pessoais,
      frequentemente processam informações pessoais e sensíveis. A
      coleta, armazenamento e uso de dados precisam estar em
      conformidade com regulamentos de proteção de dados, como o GDPR
      (Regulamento Geral de Proteção de Dados) na União Europeia.</p>
      <p>Os seguintes princípios são fundamentais para garantir a
      privacidade dos dados:</p>
      <ul>
      <li><p><strong>Minimização de Dados</strong>: Coletar apenas os
      dados estritamente necessários para a tarefa em questão.</p></li>
      <li><p><strong>Transparência</strong>: Informar os usuários sobre
      quais dados estão sendo coletados, como serão usados e por quanto
      tempo serão armazenados.</p></li>
      <li><p><strong>Consentimento</strong>: Garantir que os usuários
      concordem explicitamente com o uso de seus dados.</p></li>
      <li><p><strong>Anonimização</strong>: Sempre que possível,
      anonimizar os dados para evitar que informações pessoais possam
      ser associadas a um indivíduo específico.</p></li>
      </ul>
      <p>Este exemplo em python mostra como dados sensíveis, como
      informações pessoais, podem ser protegidos com criptografia em
      repouso e em trânsito, garantindo que apenas usuários autorizados
      possam acessá-los.</p>
      <div class="sourceCode" id="cb130" data-language="Python"
      data-caption="Este código Gera uma chave de criptografia; Criptografa dados sensíveis; Descriptografa os dados e Exibe o resultado descriptografado no console."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install cryptography  </span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cryptography.fernet <span class="im">import</span> Fernet</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Gerar uma chave de criptografia</span></span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> Fernet.generate_key()</span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>cipher_suite <span class="op">=</span> Fernet(key)</span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Criptografar dados sensíveis</span></span>
<span id="cb130-9"><a href="#cb130-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="st">&quot;Dados sensíveis do usuário&quot;</span></span>
<span id="cb130-10"><a href="#cb130-10" aria-hidden="true" tabindex="-1"></a>encrypted_data <span class="op">=</span> cipher_suite.encrypt(data.encode())</span>
<span id="cb130-11"><a href="#cb130-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-12"><a href="#cb130-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Descriptografar quando necessário</span></span>
<span id="cb130-13"><a href="#cb130-13" aria-hidden="true" tabindex="-1"></a>decrypted_data <span class="op">=</span> cipher_suite.decrypt(encrypted_data).decode()</span>
<span id="cb130-14"><a href="#cb130-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Dados descriptografados:&quot;</span>, decrypted_data)</span></code></pre></div>
      <pre><code>Dados descriptografados: Dados sensíveis do usuário</code></pre>
      <h2 id="viés-algorítmico">Viés Algorítmico</h2>
      <p>Viés algorítmico ocorre quando um modelo de chatbot aprende
      padrões indesejáveis ou preconceituosos a partir dos dados de
      treinamento, levando a resultados injustos ou discriminatórios.
      Este problema pode se manifestar de várias formas, desde o uso de
      linguagem tendenciosa até a recomendação de serviços que favoreçam
      certos grupos em detrimento de outros.</p>
      <p>Os vieses nos chatbots podem surgir dos dados de treinamento,
      pois os dados usados para treinar o chatbot contêm preconceitos
      históricos; esses preconceitos podem ser perpetuados pelo modelo.
      Também pode existir viés nas iterações do Usuário, pois, caso o
      chatbot aprenda com as interações de usuários, pode absorver e
      amplificar vieses existentes nas entradas dos usuários. Além
      disso, alguns algoritmos podem inadvertidamente favorecer certos
      tipos de dados em detrimento de outros.</p>
      <p>Mitigar o viés algorítmico requer abordagens proativas, tais
      como: a diversificação de dados de treinamento para garantir que o
      chatbot seja treinado em um conjunto de dados diversificado que
      represente várias culturas, gêneros, idades e contextos sociais;
      outra forma é regularmente auditar os modelos de chatbot para
      identificar e corrigir viés indesejado; e por fim, utilizar
      algoritmos que considerem equidade e justiça ao treinar o
      chatbot.</p>
      <div class="sourceCode" id="cb132" data-language="Python"
      data-caption="O código verifica possíveis vieses nas respostas do chatbot analisando os acertos e erros das predições."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install scikit-learn</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Verificar o viés nas predições do chatbot</span></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_bias(true_labels, predictions):</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(true_labels, predictions)</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Matriz de confusão:</span><span class="ch">\n</span><span class="st">&quot;</span>, cm)</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Exemplo de auditoria simples de viés</span></span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># Representa as classificações corretas</span></span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># Predições do chatbot</span></span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>evaluate_bias(true_labels, predictions)</span></code></pre></div>
      <pre><code>Matriz de confusão:
[[2 0]
[1 2]]</code></pre>
      <p>Este código simula uma auditoria básica para verificar a
      presença de viés em um modelo de chatbot, comparando predições com
      rótulos verdadeiros para identificar possíveis discrepâncias.</p>
      <h2 id="segurança-de-chatbots">Segurança de Chatbots</h2>
      <p>Os chatbots, especialmente aqueles integrados em sistemas
      críticos, são alvos potenciais para ataques cibernéticos. Garantir
      a segurança desses sistemas é essencial para proteger tanto os
      usuários quanto as empresas.</p>
      <p>Algumas das principais vulnerabilidades de chatbots
      incluem:</p>
      <ul>
      <li><p><strong>Ataques de Injeção de Código</strong>: Um atacante
      pode tentar injetar código malicioso nas entradas do chatbot para
      comprometer o sistema.</p></li>
      <li><p><strong>Ataques de Engenharia Social</strong>: Os usuários
      podem ser enganados para compartilhar informações confidenciais,
      acreditando que estão falando com um agente confiável.</p></li>
      <li><p><strong>Abuso de API</strong>: Se a API usada pelo chatbot
      não estiver adequadamente protegida, um atacante pode explorá-la
      para acessar dados ou realizar ações não autorizadas.</p></li>
      </ul>
      <p>Medidas de segurança para proteger chatbots incluem:</p>
      <ul>
      <li><p><strong>Validação de Entrada</strong>: Implementar uma
      validação rigorosa das entradas do usuário para evitar ataques de
      injeção de código.</p></li>
      <li><p><strong>Autenticação de Usuário</strong>: Exigir
      autenticação para acessar funcionalidades críticas do chatbot,
      como alteração de dados pessoais.</p></li>
      <li><p><strong>Limitação de Taxa (Rate Limiting)</strong>: Impedir
      o abuso da API limitando o número de solicitações que podem ser
      feitas por um usuário em um determinado período de tempo.</p></li>
      </ul>
      <div class="sourceCode" id="cb134" data-language="Python"
      data-caption="um exemplo de como aplicar limitação de taxa (rate limiting) em uma rota de API usando Flask."><pre
      class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install flask_limiter</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pip install flask</span></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flask_limiter <span class="im">import</span> Limiter</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flask <span class="im">import</span> Flask</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>app <span class="op">=</span> Flask(<span class="va">__name__</span>)</span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>limiter <span class="op">=</span> Limiter(key_func<span class="op">=</span><span class="kw">lambda</span>: <span class="st">&quot;user_ip&quot;</span>)</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a>limiter.init_app(app)</span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a><span class="at">@app.route</span>(<span class="st">&quot;/secure-endpoint&quot;</span>)</span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a><span class="at">@limiter.limit</span>(<span class="st">&quot;5 per minute&quot;</span>)</span>
<span id="cb134-13"><a href="#cb134-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> secure_endpoint():</span>
<span id="cb134-14"><a href="#cb134-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&quot;Acesso seguro garantido!&quot;</span></span></code></pre></div>
      <pre><code>UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. [...] documentation about configuring the storage backend. [...]</code></pre>
      <p>Neste exemplo, utilizamos a limitação de taxa para proteger um
      endpoint sensível, garantindo que os usuários não possam abusar da
      API.</p>
      <p>Impacto Social dos Chatbots: Além das questões técnicas e de
      segurança, os chatbots podem ter um impacto social significativo.
      Eles podem influenciar o comportamento dos usuários, moldar
      interações sociais e até substituir trabalhos humanos em
      determinadas indústrias.</p>
      <p>Substituição de Trabalho Humano: Embora os chatbots possam
      aumentar a eficiência e reduzir custos, sua implementação pode
      resultar na substituição de empregos, especialmente em setores de
      atendimento ao cliente. É importante considerar como a automação
      pode ser introduzida de forma ética, proporcionando requalificação
      e apoio a trabalhadores impactados.</p>
      <p>Manipulação e Desinformação: Os chatbots também podem ser
      usados para manipulação e disseminação de desinformação. Modelos
      de Linguagem Grande podem ser explorados para criar bots
      maliciosos que disseminam fake news, discursos de ódio ou
      influenciam eleições. Uma possibilidade de mitigação seria
      implementar filtros de moderação e ferramentas de verificação de
      fatos em chatbots para impedir a disseminação de informações
      enganadoras ou prejudiciais.</p>
      <h2 id="manutenção-e-atualização">Manutenção e Atualização</h2>
      <p>Implantar um chatbot é apenas o início. Para garantir que ele
      continue a atender às necessidades dos usuários de forma eficaz e
      segura, é essencial implementar um processo contínuo de manutenção
      e atualização. Nesta seção, vamos explorar as melhores práticas
      para manter um chatbot em operação, incluindo monitoramento,
      atualização de modelos, coleta e análise de feedback dos usuários,
      e práticas para prevenir a deterioração do desempenho.</p>
      <p>Um chatbot que não é mantido adequadamente pode rapidamente se
      tornar obsoleto, irrelevante ou até prejudicial para a experiência
      do usuário. A manutenção contínua é necessária para:</p>
      <ul>
      <li><p><strong>Atualizar Conhecimento</strong>: Adaptar o chatbot
      a novas informações, gírias ou mudanças de contexto.</p></li>
      <li><p><strong>Melhorar o Desempenho</strong>: Ajustar o chatbot
      com base no feedback do usuário para aprimorar a precisão e a
      relevância das respostas.</p></li>
      <li><p><strong>Garantir a Segurança</strong>: Aplicar patches de
      segurança e atualizações para proteger o chatbot de
      vulnerabilidades.</p></li>
      <li><p><strong>Adaptar-se às Mudanças no Ambiente</strong>:
      Ajustar o chatbot às mudanças na infraestrutura técnica, como
      atualizações em APIs de integração.</p></li>
      </ul>
      <p>Já o monitoramento contínuo do chatbot em produção é
      fundamental para identificar problemas de desempenho, compreender
      como os usuários estão interagindo com o sistema e detectar
      comportamentos inesperados ou indesejados. Algumas métricas
      importantes para monitorar incluem:</p>
      <p>Atualização de Modelos: Com o tempo, os modelos de linguagem
      usados pelos chatbots podem se tornar desatualizados,
      especialmente se o domínio de aplicação estiver em constante
      evolução. A atualização dos modelos pode envolver realizar
      re-treinamento com dados novos, ou incorporar novos dados para que
      o modelo se adapte a mudanças de linguagem, gírias ou tópicos
      emergentes. Outra técnica é realizar <em>Fine-Tuning</em>
      periódico com base em novos exemplos de interações reais dos
      usuários.</p>
      <p>Feedback do Usuário e Melhorias Contínuas: O feedback dos
      usuários é uma fonte valiosa de informações para melhorar o
      chatbot. Processos automatizados e manuais de coleta e análise de
      feedback ajudam a identificar áreas para aprimoramento. É possível
      realizar a coleta de feedback via solicitação direta, perguntando
      diretamente aos usuários sobre sua satisfação com as respostas
      fornecidas, ou usar algoritmos para analisar o sentimento geral
      das interações dos usuários com o chatbot. De posse do feedback do
      chatbot, é possível refinar respostas problemáticas identificadas
      através do feedback para incluir novos dados ou ajustar o modelo
      para melhor atender às necessidades dos usuários.</p>
      <p>Prevenção de Deterioração de Desempenho: Com o tempo, chatbots
      podem sofrer deterioração de desempenho devido a mudanças no
      ambiente ou desgaste do modelo. Estratégias para prevenir isso
      incluem detectar e corrigir quando o desempenho do modelo começa a
      se degradar em relação a novas entradas de dados; além de
      avaliações regulares, realizando testes regulares para garantir
      que o chatbot continue operando conforme esperado; por fim, manter
      versões anteriores do chatbot para comparação e possível rollback
      em caso de problemas com novas versões.</p>
      <p>Segurança e Privacidade: Garantir a segurança contínua e a
      privacidade dos dados do usuário é uma prioridade. Isso envolve
      manter o ambiente e as dependências do chatbot atualizados com os
      patches de segurança mais recentes, o que chamamos de Aplicação de
      Patches de Segurança; além de implementar medidas para proteger
      dados sensíveis, incluindo criptografia de dados em repouso e em
      trânsito, e conformidade com regulamentos como o GDPR.</p>
      <h2 id="perspectivas-futuras">Perspectivas Futuras</h2>
      <p>À medida que a tecnologia avança, os chatbots estão se tornando
      cada vez mais sofisticados e capazes de realizar tarefas complexas
      que antes eram consideradas impossíveis. A seguir, destacamos
      algumas das principais tendências e áreas de pesquisa que
      provavelmente moldarão o futuro dos chatbots:</p>
      <p>Modelos de Linguagem Multimodais: Chatbots PLN não se limitam
      apenas ao texto. Modelos multimodais, que combinam texto com
      imagens, áudio e até mesmo vídeo, já são uma nova fronteira no
      desenvolvimento de chatbots. Esses modelos permitem que chatbots
      compreendam e respondam a entradas que combinam diferentes tipos
      de dados, oferecendo experiências mais ricas e interativas. Hoje,
      chatbots multimodais já analisam imagens enviadas por um usuário e
      fornecem diagnósticos ou recomendações, além de responderem a
      perguntas textuais relacionadas.</p>
      <p>Personalização Avançada: À medida que a capacidade de coleta e
      análise de dados se expande, a personalização de chatbots se
      tornará mais sofisticada. Chatbots serão capazes de adaptar suas
      respostas com base no histórico de interações, preferências do
      usuário e até mesmo no contexto emocional, criando interações
      verdadeiramente personalizadas. Hoje, assistentes virtuais ajustam
      as recomendações de compras ou serviços com base nos hábitos de
      consumo e humor do usuário.</p>
      <p>Chatbots Colaborativos e de Aprendizado Contínuo: Chatbots que
      podem aprender de forma contínua e colaborativa com outros
      sistemas e com os próprios usuários serão uma área chave de
      desenvolvimento. Isso permitirá que chatbots se adaptem
      rapidamente a novos domínios e que integrem informações em tempo
      real, melhorando sua eficácia e utilidade. Um exemplo de aplicação
      seria um chatbot que aprende novos termos e conceitos diretamente
      das interações com usuários e os compartilha com outros chatbots,
      criando uma rede de conhecimento distribuída. Outro exemplo é um
      chatbot que discute Ecologia com estudantes do ensino médio <span
      class="citation" data-cites="Neo2023Ecowe">(A. V. B. S. Neo et al.
      2023)</span>.</p>
      <p>Segurança e Privacidade Melhoradas: Com a crescente
      sofisticação dos chatbots, as questões de segurança e privacidade
      se tornarão ainda mais críticas. Avanços em criptografia,
      anonimização de dados e conformidade com regulamentos serão
      essenciais para garantir que chatbots possam ser usados com
      confiança em setores sensíveis, como saúde e finanças. Um exemplo
      seriam chatbots médicos que garantem a privacidade total dos dados
      do paciente enquanto oferecem diagnósticos e recomendações de
      tratamento, por exemplo <span class="citation"
      data-cites="pires2024conversational">Pires (2024)</span>.</p>
      <p>Explicabilidade e Transparência dos Modelos: Com o uso
      crescente de Modelos de Linguagem Grande e complexos, a
      explicabilidade e a transparência se tornarão áreas críticas de
      pesquisa. Ferramentas e técnicas para explicar como um chatbot
      chegou a uma determinada resposta serão cada vez mais demandadas,
      especialmente em setores regulamentados.Uma solução pode ser
      fornecer uma explicação detalhada de como chegaram a uma decisão
      ou recomendação, aumentando a confiança dos usuários.</p>
      <h2 id="oportunidades-de-inovação">Oportunidades de Inovação</h2>
      <p>À medida que o campo dos chatbots continua a evoluir, novas
      oportunidades para inovação e pesquisa emergem constantemente.
      Nesta seção, vamos explorar as áreas emergentes. Também
      discutiremos os desafios abertos que podem ser abordados e como os
      desenvolvedores e pesquisadores podem contribuir para o avanço
      deste campo, explorando as tendências tecnológicas e áreas
      promissoras de estudo.</p>
      <h3 class="unnumbered"
      id="tendências-tecnológicas-emergentes">Tendências Tecnológicas
      Emergentes</h3>
      <p>As tecnologias de Processamento de Linguagem Natural (PLN) e
      Inteligência Artificial (IA) estão em rápida evolução. Algumas das
      principais tendências que moldarão o futuro dos chatbots
      incluem:</p>
      <h4 class="unnumbered" id="vibe-coding">Vibe Coding</h4>
      <p>Em sua essência, vibe coding é uma abordagem ao desenvolvimento
      de software que se apoia fortemente na IA para gerar código. Em
      vez de escrever meticulosamente cada linha de código você mesmo,
      você descreve o que quer alcançar em linguagem natural, e a IA faz
      o trabalho pesado. É como ter um par de programação de IA que
      compreende suas vibrações (daí o nome) e as traduz em código
      funcional.</p>
      <p>Este conceito ganhou força com os avanços recentes em Modelos
      de Linguagem Grande (LLMs) como o GPT-4. Essas ferramentas de IA
      se tornaram cada vez mais hábeis em compreender a intenção humana
      e gerar código correspondente, tornando o processo de
      desenvolvimento mais rápido e acessível.</p>
      <p>Características do Vibe Coding:</p>
      <ul>
      <li><p>Abstração: Vibe coding abstrai as complexidades da escrita
      de código tradicional, permitindo que os programadores se
      concentrem na lógica de nível superior e na criatividade.</p></li>
      <li><p>Eficiência: Ao automatizar tarefas repetitivas e gerar
      código boilerplate, o vibe coding pode acelerar significativamente
      o processo de desenvolvimento.</p></li>
      <li><p>Acessibilidade: O vibe coding pode potencialmente
      democratizar o desenvolvimento de software, tornando-o mais
      acessível a pessoas com menos experiência em programação.</p></li>
      <li><p>Colaboração: O vibe coding pode facilitar novas formas de
      colaboração entre humanos e IA, com os programadores trabalhando
      em conjunto com as ferramentas de IA para construir
      software.</p></li>
      </ul>
      <p>Embora o vibe coding ainda esteja em seus estágios iniciais,
      tem o potencial de revolucionar o desenvolvimento de software.
      Imagine você poder construir um aplicativo simplesmente
      descrevendo-o em uma conversa com um assistente de IA. Este
      cenário pode estar mais perto do que pensamos.</p>
      <p>No entanto, o vibe coding também levanta questões importantes.
      Irá levar a uma perda de controle sobre o processo de
      desenvolvimento? Quais são as implicações para a qualidade do
      código, segurança e manutenção a longo prazo? À medida que o vibe
      coding continua a evoluir, os programadores terão de navegar por
      estas questões e encontrar um equilíbrio entre aproveitar o poder
      da IA.</p>
      <h4 class="unnumbered" id="model-contex-protocol">Model Contex
      Protocol</h4>
      <p>O <em>Model Context Protocol</em> (MCP) é um padrão aberto e um
      <em>framework</em> de código aberto introduzido pela Anthropic em
      2024 para padronizar a forma como sistemas de IA, especialmente
      grandes modelos de linguagem (LLMs), se conectam a ferramentas e
      fontes de dados externas <span class="citation"
      data-cites="anthropic2024mcp">(Anthropic 2024)</span>. Seu
      objetivo principal é unificar o modo como modelos acessam
      arquivos, executam funções externas e utilizam <em>prompts</em>
      contextuais, substituindo integrações ad hoc por uma interface
      consistente. Dessa forma, o MCP resolve o problema de integração
      <span class="math inline"><em>N</em> × <em>M</em></span>, em que
      cada agente precisava de conectores específicos para cada
      ferramenta, tornando o desenvolvimento mais complexo e difícil de
      escalar <span class="citation"
      data-cites="anthropic2024mcp">(Anthropic 2024)</span>.</p>
      <p>Do ponto de vista conceitual, o MCP utiliza uma arquitetura
      cliente-servidor em que o agente de IA atua como cliente MCP e se
      comunica com um ou mais servidores MCP que expõem ferramentas,
      recursos e <em>prompts</em> <span class="citation"
      data-cites="jsonrpcSpec">(Group 2010)</span>. Toda a comunicação é
      realizada usando o protocolo JSON-RPC 2.0, que padroniza as
      mensagens de requisição e resposta em formato JSON e é
      independente de linguagem de programação. O MCP suporta transporte
      por <em>stdio</em> para execução local com baixa latência e
      transporte via HTTP com suporte a <em>Server-Sent Events</em> para
      cenários distribuídos ou remotos <span class="citation"
      data-cites="jsonrpcSpec">(Group 2010)</span>. Essa padronização
      garante interoperabilidade e permite que novos servidores ou
      ferramentas possam ser descobertos dinamicamente pelo cliente, que
      pode então invocá-los com parâmetros estruturados e receber
      resultados também em formato JSON.</p>
      <p>A adoção do MCP traz vantagens significativas para o
      ecossistema de agentes de IA, pois elimina a dependência de
      soluções proprietárias e facilita a criação de ambientes híbridos,
      onde modelos locais ou em nuvem podem acessar os mesmos recursos
      de maneira uniforme. Em aplicações corporativas ou privadas, o MCP
      permite que servidores sejam executados localmente, garantindo que
      dados sensíveis não precisem sair do ambiente da organização,
      atendendo a requisitos de segurança e conformidade. Além disso, ao
      ser um protocolo aberto, incentiva a colaboração e a criação de um
      ecossistema de conectores reutilizáveis, reduzindo o custo de
      desenvolvimento e favorecendo a interoperabilidade entre
      diferentes plataformas de IA <span class="citation"
      data-cites="anthropic2024mcp">(Anthropic 2024)</span>.</p>
      <h4 class="unnumbered" id="outras-tendências">Outras
      tendências</h4>
      <p>Inteligência Artificial Explicável (XAI): Com o aumento da
      complexidade dos modelos de linguagem, a necessidade de
      Inteligência Artificial Explicável (XAI) se torna mais premente.
      XAI visa tornar os sistemas de IA mais transparentes e
      compreensíveis para os seres humanos, o que é importante para
      garantir a confiança do usuário em decisões automatizadas. Um
      exemplo de aplicação é desenvolver chatbots que possam explicar
      como chegaram a uma determinada resposta, oferecendo aos usuários
      uma visão dos processos de tomada de decisão do modelo.</p>
      <p>Modelos de Linguagem Contínuos e Atualizáveis: À medida que o
      conhecimento e a linguagem evoluem, há uma crescente necessidade
      de modelos de linguagem que possam ser continuamente atualizados
      sem a necessidade de re-treinamento completo. A pesquisa em
      aprendizado contínuo e incremental está ganhando força, com o
      objetivo de criar chatbots que possam se adaptar a novas
      informações em tempo real. Um exemplo de aplicação é implementar
      chatbots em ambientes corporativos que possam incorporar novas
      políticas ou informações à medida que são introduzidas, mantendo a
      precisão e relevância das respostas.</p>
      <p>Integração Multimodal Avançada: Os chatbots do futuro não se
      limitarão apenas ao texto. A integração de dados multimodais –
      combinando texto, áudio, vídeo e outras formas de dados sensoriais
      – permitirá que os chatbots ofereçam interações muito mais ricas e
      naturais. Um exemplo de aplicação seria desenvolver assistentes
      virtuais que possam interpretar e responder a comandos de voz,
      reconhecer expressões faciais em vídeo, e até mesmo responder a
      estímulos táteis em interfaces especializadas.</p>
      <h3 class="unnumbered" id="desafios-em-abertos">Desafios em
      abertos</h3>
      <p>Apesar dos avanços significativos, vários desafios permanecem
      no desenvolvimento de chatbots. Esses desafios representam
      oportunidades para pesquisa e inovação.</p>
      <p>Compreensão de Contexto Profundo: Embora os modelos atuais
      sejam capazes de manter o contexto em conversas curtas, a
      compreensão de contexto em conversas longas e complexas ainda é um
      desafio. Isso inclui a capacidade de lembrar detalhes ao longo de
      várias interações e responder de maneira coerente em tópicos que
      evoluem com o tempo. Uma área de pesquisa seria investigar novas
      arquiteturas de memória e mecanismos de atenção que possam
      melhorar a retenção e o uso de contexto em conversas
      prolongadas.</p>
      <p>Interação Emocional e Comportamental: Os chatbots ainda carecem
      de habilidades avançadas para interpretar e responder a sinais
      emocionais e comportamentais dos usuários. A capacidade de um
      chatbot de ajustar seu tom, estilo de resposta e sugestões com
      base no estado emocional do usuário pode melhorar
      significativamente a qualidade da interação. Uma área de pesquisa
      seria desenvolver modelos de linguagem que incorporem
      reconhecimento e resposta emocional, utilizando técnicas de
      aprendizado profundo e processamento de sinais.</p>
      <p>Redução de Viés e Aumento da Inclusividade: Como discutido na
      seção sobre considerações éticas, a mitigação de viés é um desafio
      contínuo. Garantir que os chatbots sejam inclusivos e justos em
      suas interações é essencial para evitar a perpetuação de
      preconceitos e discriminações. Uma área de pesquisa é criar
      métodos para detectar e corrigir viés em modelos de linguagem e
      explorar novos conjuntos de dados que representem uma diversidade
      maior de culturas e contextos.</p>
      <p>Automação da Criação e Treinamento de Chatbots: O
      desenvolvimento e o treinamento de chatbots ainda são processos
      intensivos em tempo e recursos. Automação avançada nesses
      processos pode acelerar o desenvolvimento e permitir a criação de
      chatbots mais personalizados e especializados. Uma área de
      pesquisa é investigar o uso de técnicas de AutoML (Machine
      Learning Automático) para automatizar a seleção de modelos, ajuste
      de hiperparâmetros e treinamento de chatbots para diferentes
      domínios.</p>
      <h3 class="unnumbered" id="oportunidades-de-pd">Oportunidades de
      P&amp;D</h3>
      <p>Para desenvolvedores e pesquisadores interessados em contribuir
      para a pesquisa e desenvolvimento dos chatbots, as seguintes áreas
      podem representar algumas oportunidades:</p>
      <p>Colaboração entre Humanos e Chatbots: A pesquisa em sistemas
      colaborativos, onde humanos e chatbots trabalham juntos para
      resolver problemas complexos, está crescendo. Explorar como
      chatbots podem complementar e aprimorar as capacidades humanas em
      ambientes colaborativos é uma área rica para inovação. Um exemplo
      de aplicação seriam chatbots assistentes que ajudam equipes a
      coordenar projetos, fornecendo sugestões baseadas em análises de
      dados em tempo real e facilitando a comunicação entre os membros
      da equipe.</p>
      <p>Chatbots para a Inclusão Digital: Com bilhões de pessoas ainda
      desconectadas do mundo digital, chatbots podem realizar inclusão
      digital. Desenvolver chatbots que funcionem em ambientes de baixa
      conectividade e que sejam acessíveis para pessoas com baixa
      alfabetização digital é uma área importante de pesquisa. Um
      exemplo seriam chatbots que operam em redes de baixa largura de
      banda e que utilizam interfaces de voz para alcançar comunidades
      rurais ou sub-representadas.</p>
      <p>Segurança e Privacidade em Chatbots Autônomos: À medida que os
      chatbots se tornam mais autônomos, a segurança e a privacidade se
      tornam preocupações ainda maiores. A pesquisa em criptografia,
      autenticação e anonimização de dados em chatbots autônomos é
      essencial para proteger os usuários. Um exemplo de aplicação seria
      um chatbot financeiro que realiza transações automaticamente,
      protegendo as informações do usuário com criptografia avançada e
      garantindo a conformidade com regulamentos como o GDPR.</p>
      <p>Colaboração Interdisciplinar: O futuro do desenvolvimento de
      chatbots será moldado pela colaboração interdisciplinar. As áreas
      de IA, psicologia, linguística, direito e ética precisarão
      trabalhar juntas para criar sistemas que sejam não apenas
      tecnicamente avançados, mas também socialmente responsáveis. Uma
      proposta pode ser desenvolver chatbots educacionais que não apenas
      ensinem, mas também sejam capazes de adaptar seus métodos
      pedagógicos com base em princípios psicológicos e educativos,
      colaborando com educadores e psicólogos.</p>
      <h3 class="unnumbered"
      id="recomendações-para-futuros-desenvolvedores">Recomendações para
      Futuros Desenvolvedores</h3>
      <p>Para aqueles que desejam continuar explorando e inovando no
      campo dos chatbots, algumas recomendações:</p>
      <ul>
      <li><p>Explore Novas Ferramentas e Tecnologias: Mantenha-se
      atualizado sobre os últimos desenvolvimentos em PLN e IA, e não
      tenha medo de experimentar novas ferramentas e
      frameworks.</p></li>
      <li><p>Concentre-se na Experiência do Usuário: Ao desenvolver
      chatbots, sempre coloque a experiência do usuário em primeiro
      lugar. Um chatbot útil e intuitivo será muito mais
      bem-sucedido.</p></li>
      <li><p>Mantenha a Ética em Mente: Com o poder dos chatbots vem a
      responsabilidade. Certifique-se de que seu chatbot é ético,
      respeita a privacidade do usuário e está em conformidade com as
      regulamentações.</p></li>
      <li><p>Participe da Comunidade: Envolva-se com a comunidade de PLN
      e IA. Contribua com projetos de código aberto, participe de
      conferências e workshops, e colabore com outros pesquisadores e
      desenvolvedores.</p></li>
      </ul>
      <h2 id="considerações-finais">Considerações Finais</h2>
      <p>As considerações éticas discutidas nesta seção são essenciais
      para o desenvolvimento de chatbots que respeitem os direitos e a
      dignidade dos usuários. Ao abordar questões de privacidade, viés
      algorítmico, segurança e impacto social, os desenvolvedores podem
      garantir que seus chatbots não apenas funcionem de maneira eficaz,
      mas também contribuam positivamente para a sociedade. Com um foco
      em práticas éticas, os chatbots têm o potencial de transformar
      interações humanas de forma significativa e benéfica.</p>
      <p>Também revisamos as técnicas essenciais discutidas ao longo do
      livro e exploramos as tendências emergentes que estão moldando o
      futuro dos chatbots. À medida que continuamos a avançar no campo
      do Processamento de Linguagem Natural e Inteligência Artificial,
      as possibilidades para chatbots se expandem exponencialmente. Com
      as ferramentas e conhecimentos adquiridos neste livro, você está
      preparado para enfrentar esses desafios e contribuir para a
      próxima geração de sistemas de diálogo inteligentes. Além disso,
      destacamos as oportunidades de inovação e pesquisa futura no campo
      dos chatbots, apontando as tendências emergentes e os desafios que
      ainda precisam ser abordados.</p>
      <p>Este livro explorou uma ampla gama de técnicas e conceitos
      essenciais para o desenvolvimento de chatbots modernos e eficazes.
      À medida que avançamos, a responsabilidade recai sobre os
      desenvolvedores, pesquisadores e inovadores para continuar
      explorando, questionando e expandindo os limites do que os
      chatbots podem alcançar.</p>
      <p>O campo está repleto de oportunidades para aqueles que estão
      dispostos a enfrentar os desafios técnicos e éticos na construção
      dos chatbots. Com a combinação de técnica e responsabilidade
      social, o seu uso pode beneficiar a sociedade.</p>
      <p>Convidamos os pesquisadores a continuar sua jornada no
      desenvolvimento de chatbots, contribuindo para a criação de
      sistemas de diálogo que não apenas resolvam problemas técnicos,
      mas também ajudem a construir um mundo melhor, explorando novas
      fronteiras, garantindo que os chatbots do futuro sejam mais
      capazes, inclusivos e responsáveis.</p>
      <h2 id="exercícios-4">Exercícios</h2>
      <ol>
      <li><p><strong>Por que é importante considerar a ética no
      desenvolvimento de chatbots?</strong></p>
      <ol>
      <li><p>Para garantir que o chatbot opere de forma eficiente em
      todos os idiomas.</p></li>
      <li><p>Para evitar que o chatbot cause danos sociais, respeitando
      a privacidade e evitando vieses.</p></li>
      <li><p>Para aumentar a taxa de resposta do chatbot.</p></li>
      <li><p>Para garantir que o chatbot possa operar em ambientes sem
      conexão com a internet.</p></li>
      </ol></li>
      <li><p><strong>Qual é uma prática recomendada para proteger a
      privacidade dos usuários ao interagir com um chatbot?</strong></p>
      <ol>
      <li><p>Coletar todos os dados possíveis do usuário,
      independentemente de seu consentimento.</p></li>
      <li><p>Implementar criptografia de ponta a ponta e obter
      consentimento explícito antes de coletar informações
      pessoais.</p></li>
      <li><p>Evitar qualquer tipo de monitoramento de interações do
      usuário.</p></li>
      <li><p>Limitar o uso do chatbot a conversas triviais para evitar
      questões de privacidade.</p></li>
      </ol></li>
      <li><p><strong>Como o viés algorítmico pode afetar a interação de
      um chatbot com os usuários?</strong></p>
      <ol>
      <li><p>O viés pode levar a respostas imprecisas, discriminatórias
      ou injustas, prejudicando a experiência do usuário.</p></li>
      <li><p>O viés melhora a capacidade do chatbot de aprender
      rapidamente.</p></li>
      <li><p>O viés reduz o tempo de resposta do chatbot, tornando-o
      mais eficiente.</p></li>
      <li><p>O viés ajuda a personalizar as respostas do chatbot para
      cada usuário.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes práticas pode ajudar a mitigar o
      viés em modelos de chatbot?</strong></p>
      <ol>
      <li><p>Treinar o modelo exclusivamente com dados provenientes de
      uma única fonte.</p></li>
      <li><p>Diversificar os dados de treinamento e realizar auditorias
      regulares para identificar e corrigir vieses.</p></li>
      <li><p>Aumentar o tamanho do modelo para incluir mais
      parâmetros.</p></li>
      <li><p>Manter o modelo desconectado de qualquer base de dados
      externa.</p></li>
      </ol></li>
      <li><p><strong>Por que é essencial garantir a transparência e a
      explicabilidade em chatbots avançados?</strong></p>
      <ol>
      <li><p>Para permitir que os desenvolvedores controlem todas as
      interações do chatbot manualmente.</p></li>
      <li><p>Para que os usuários compreendam como o chatbot toma
      decisões, aumentando a confiança e a aceitação.</p></li>
      <li><p>Para reduzir o custo de desenvolvimento do
      chatbot.</p></li>
      <li><p>Para eliminar a necessidade de manutenção contínua do
      chatbot.</p></li>
      </ol></li>
      <li><p><strong>Qual é o principal objetivo de analisar casos de
      estudo em chatbots avançados?</strong></p>
      <ol>
      <li><p>Demonstrar como as técnicas teóricas discutidas podem ser
      aplicadas na prática para resolver problemas reais.</p></li>
      <li><p>Descrever os processos de instalação de chatbots em
      dispositivos móveis.</p></li>
      <li><p>Explicar como criar um chatbot sem conhecimento técnico
      prévio.</p></li>
      <li><p>Comparar o desempenho de diferentes assistentes
      virtuais.</p></li>
      </ol></li>
      <li><p><strong>Em um caso de estudo, qual foi a principal vantagem
      observada ao integrar um chatbot em uma plataforma de e-learning
      para o aprendizado de idiomas?</strong></p>
      <ol>
      <li><p>Redução dos custos operacionais da plataforma.</p></li>
      <li><p>elhoria na fluência e confiança dos alunos ao praticar
      conversação em tempo real com correções imediatas.</p></li>
      <li><p>Substituição completa dos professores humanos por
      chatbots.</p></li>
      <li><p>Aumento do tempo de carga das páginas da
      plataforma.</p></li>
      </ol></li>
      <li><p><strong>Qual foi um dos desafios enfrentados ao implementar
      um chatbot de suporte ao cliente para uma empresa de
      telecomunicações?</strong></p>
      <ol>
      <li><p>Manter o contexto em longas conversas para fornecer
      respostas coerentes.</p></li>
      <li><p>Reduzir o tempo de resposta para menos de 1
      milissegundo.</p></li>
      <li><p>Treinar o chatbot para realizar diagnósticos
      médicos.</p></li>
      <li><p>Implementar o chatbot em dispositivos sem conexão à
      internet.</p></li>
      </ol></li>
      <li><p><strong>Por que a empatia é uma característica importante
      em chatbots voltados para a saúde mental, conforme discutido em um
      dos casos de estudo?</strong></p>
      <ol>
      <li><p>Porque ela substitui completamente a necessidade de
      intervenção humana.</p></li>
      <li><p>Porque ela ajuda a criar uma conexão mais forte com os
      usuários, oferecendo suporte emocional adequado.</p></li>
      <li><p>Porque ela permite que o chatbot responda mais rapidamente
      às consultas dos usuários.</p></li>
      <li><p>Porque ela elimina a necessidade de coleta de dados dos
      usuários.</p></li>
      </ol></li>
      <li><p><strong>Qual foi uma das lições aprendidas ao implementar
      chatbots avançados em diferentes indústrias, conforme os casos de
      estudo discutidos?</strong></p>
      <ol>
      <li><p>A importância de personalizar o chatbot para o contexto
      específico da aplicação, garantindo relevância e
      eficácia.</p></li>
      <li><p>A necessidade de substituir completamente os sistemas de
      suporte tradicionais por chatbots.</p></li>
      <li><p>A impossibilidade de usar chatbots em plataformas online
      devido à falta de tecnologia.</p></li>
      <li><p>A dificuldade de integrar chatbots em plataformas de redes
      sociais.</p></li>
      </ol></li>
      <li><p><strong>Por que é importante realizar a manutenção contínua
      de um chatbot em produção?</strong></p>
      <ol>
      <li><p>Para garantir que o chatbot permaneça atualizado, relevante
      e capaz de lidar com mudanças no ambiente ou nas necessidades dos
      usuários.</p></li>
      <li><p>Para reduzir o número de interações que o chatbot pode
      realizar por dia.</p></li>
      <li><p>Para garantir que o chatbot funcione apenas em horários
      comerciais.</p></li>
      <li><p>Para desativar temporariamente o chatbot e evitar o
      desgaste de hardware.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes métricas é essencial para
      monitorar o desempenho de um chatbot em produção?</strong></p>
      <ol>
      <li><p>Taxa de compressão de dados.</p></li>
      <li><p>Tempo de resposta do chatbot.</p></li>
      <li><p>Tamanho do código-fonte do chatbot.</p></li>
      <li><p>Número de desenvolvedores envolvidos no projeto.</p></li>
      </ol></li>
      <li><p><strong>Como o feedback dos usuários pode ser utilizado
      para melhorar um chatbot?</strong></p>
      <ol>
      <li><p>Ignorando o feedback dos usuários para manter a
      consistência das respostas.</p></li>
      <li><p>Ajustando as respostas do chatbot e atualizando o modelo
      com base nas sugestões e críticas dos usuários.</p></li>
      <li><p>Reduzindo o tempo de atividade do chatbot para evitar
      problemas relatados.</p></li>
      <li><p>Mantendo as respostas do chatbot sem alterações,
      independentemente do feedback.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes práticas ajuda a prevenir a
      deterioração do desempenho de um chatbot ao longo do
      tempo?</strong></p>
      <ol>
      <li><p>Realizar avaliações regulares do modelo e implementar
      monitoramento de drift para detectar quando o desempenho começa a
      decair.</p></li>
      <li><p>Aumentar a complexidade do modelo constantemente sem
      revisões periódicas.</p></li>
      <li><p>Desativar o chatbot durante horários de pico para evitar
      sobrecarga.</p></li>
      <li><p>Evitar atualizações para manter a estabilidade do
      sistema.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes abordagens é crucial para
      garantir a segurança contínua dos chatbots em
      produção?</strong></p>
      <ol>
      <li><p>Aplicar patches de segurança regularmente e garantir a
      conformidade com regulamentos de proteção de dados.</p></li>
      <li><p>Reduzir o uso de autenticação para facilitar o acesso dos
      usuários.</p></li>
      <li><p>Manter o código do chatbot desatualizado para evitar
      incompatibilidades.</p></li>
      <li><p>Evitar o uso de criptografia para melhorar o
      desempenho.</p></li>
      </ol></li>
      <li><p><strong>Por que é importante considerar as implicações
      éticas ao desenvolver chatbots?</strong></p>
      <ol>
      <li><p>Para garantir que o chatbot funcione corretamente em
      diferentes dispositivos.</p></li>
      <li><p>Para evitar consequências negativas, como viés,
      discriminação e invasão de privacidade.</p></li>
      <li><p>Para reduzir o custo de desenvolvimento do
      chatbot.</p></li>
      <li><p>Para garantir que o chatbot responda mais rapidamente às
      consultas dos usuários.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes práticas ajuda a garantir a
      transparência em um chatbot?</strong></p>
      <ol>
      <li><p>Manter o código do chatbot fechado e inacessível ao
      público.</p></li>
      <li><p>Fornecer explicações claras sobre como o chatbot toma
      decisões e responde às consultas dos usuários.</p></li>
      <li><p>Reduzir o número de interações com o usuário para evitar
      mal-entendidos.</p></li>
      <li><p>Implementar o chatbot apenas em plataformas
      limitadas.</p></li>
      </ol></li>
      <li><p><strong>Como o viés algorítmico pode afetar negativamente
      as interações com um chatbot?</strong></p>
      <ol>
      <li><p>O viés algorítmico pode levar o chatbot a fornecer
      respostas que são injustas ou discriminatórias, afetando
      negativamente a experiência do usuário.</p></li>
      <li><p>O viés algorítmico ajuda a personalizar a experiência do
      usuário, garantindo respostas mais precisas.</p></li>
      <li><p>O viés algorítmico melhora a velocidade do chatbot em
      responder a perguntas.</p></li>
      <li><p>O viés algorítmico reduz a necessidade de treinamento
      contínuo do chatbot.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes abordagens pode ajudar a mitigar
      o viés nos modelos de chatbot?</strong></p>
      <ol>
      <li><p>Usar apenas dados históricos, sem realizar atualizações nos
      modelos.</p></li>
      <li><p>Diversificar os dados de treinamento e implementar
      auditorias de viés regularmente.</p></li>
      <li><p>Reduzir a quantidade de dados usados no treinamento para
      evitar sobrecarga de informações.</p></li>
      <li><p>Confiar em uma única fonte de dados para garantir
      consistência.</p></li>
      </ol></li>
      <li><p><strong>Por que a proteção de dados é crucial ao
      desenvolver e implantar chatbots?</strong></p>
      <ol>
      <li><p>Para garantir que o chatbot funcione corretamente em
      diferentes idiomas.</p></li>
      <li><p>Para proteger a privacidade dos usuários e garantir a
      conformidade com regulamentações como o GDPR.</p></li>
      <li><p>Para reduzir o custo de desenvolvimento e manutenção do
      chatbot.</p></li>
      <li><p>Para aumentar a velocidade das respostas do
      chatbot.</p></li>
      </ol></li>
      <li><p><strong>Qual é uma das principais áreas de pesquisa futura
      mencionada no Capítulo 17 para o avanço dos chatbots?</strong></p>
      <ol>
      <li><p>A substituição completa dos assistentes virtuais por
      agentes humanos.</p></li>
      <li><p>O desenvolvimento de chatbots capazes de integrar
      multimodalidade, como texto, áudio e vídeo em interações.</p></li>
      <li><p>foco exclusivo em interações de texto simples e baseadas em
      regras.</p></li>
      <li><p>A eliminação de machine learning em favor de processamento
      de linguagem baseado em regras.</p></li>
      </ol></li>
      <li><p><strong>Por que a personalização avançada é uma tendência
      importante para o futuro dos chatbots?</strong></p>
      <ol>
      <li><p>Porque elimina a necessidade de treinamento adicional dos
      modelos.</p></li>
      <li><p>Porque permite que os chatbots adaptem suas respostas com
      base no histórico e preferências dos usuários, melhorando a
      experiência de interação.</p></li>
      <li><p>Porque acelera o tempo de resposta do chatbot para qualquer
      consulta.</p></li>
      <li><p>Porque substitui completamente os modelos de linguagem por
      modelos baseados em lógica.</p></li>
      </ol></li>
      <li><p><strong>Como os modelos de linguagem contínuos podem
      contribuir para a inovação no desenvolvimento de
      chatbots?</strong></p>
      <ol>
      <li><p>Permitindo que os chatbots realizem tarefas sem a
      necessidade de dados de entrada.</p></li>
      <li><p>Permitindo que os chatbots se adaptem em tempo real a novas
      informações, sem a necessidade de re-treinamento
      completo.</p></li>
      <li><p>Reduzindo a necessidade de machine learning em
      chatbots.</p></li>
      <li><p>Substituindo a compreensão de linguagem natural por regras
      fixas.</p></li>
      </ol></li>
      <li><p><strong>Por que a integração de ética na pesquisa e
      inovação de chatbots é essencial?</strong></p>
      <ol>
      <li><p>Para garantir que os chatbots sejam eficientes no uso de
      dados de treinamento.</p></li>
      <li><p>Para garantir que os chatbots operem de forma justa e
      responsável, respeitando a privacidade e evitando
      discriminação.</p></li>
      <li><p>Para garantir que os chatbots possam ser usados em todos os
      idiomas.</p></li>
      <li><p>Para melhorar a velocidade de resposta dos
      chatbots.</p></li>
      </ol></li>
      <li><p><strong>Qual é o papel da colaboração interdisciplinar no
      futuro dos chatbots?</strong></p>
      <ol>
      <li><p>Garantir que os chatbots possam operar sem a necessidade de
      manutenção contínua.</p></li>
      <li><p>Facilitar a integração de diferentes áreas de conhecimento,
      como IA, psicologia, linguística e ética, para criar chatbots mais
      eficazes e responsáveis.</p></li>
      <li><p>Reduzir o custo de desenvolvimento de chatbots em larga
      escala.</p></li>
      <li><p>Eliminar a necessidade de monitoramento e atualização de
      chatbots.</p></li>
      </ol></li>
      <li><p><strong>Qual é a principal consideração ao escolher uma
      plataforma de implantação para um chatbot?</strong></p>
      <ol>
      <li><p>O tamanho do modelo de linguagem usado.</p></li>
      <li><p>O público-alvo e os requisitos específicos de interação do
      chatbot.</p></li>
      <li><p>A capacidade do chatbot de realizar tarefas em tempo
      real.</p></li>
      <li><p>O idioma principal em que o chatbot foi treinado.</p></li>
      </ol></li>
      <li><p><strong>Por que é importante realizar a otimização do
      modelo antes da implantação de um chatbot em
      produção?</strong></p>
      <ol>
      <li><p>Para garantir que o chatbot funcione sem a necessidade de
      atualizações futuras.</p></li>
      <li><p>Para reduzir o uso de recursos computacionais e melhorar o
      tempo de resposta em produção.</p></li>
      <li><p>Para eliminar a necessidade de monitoramento
      contínuo.</p></li>
      <li><p>Para permitir que o chatbot funcione apenas em um único
      idioma.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes práticas pode ajudar a garantir
      a segurança de um chatbot implantado?</strong></p>
      <ol>
      <li><p>Desativar todos os logs de interação do usuário.</p></li>
      <li><p>Implementar criptografia de dados e controles de acesso
      rigorosos.</p></li>
      <li><p>Evitar o uso de autenticação para acelerar as
      interações.</p></li>
      <li><p>Manter o código-fonte do chatbot em plataformas
      abertas.</p></li>
      </ol></li>
      <li><p><strong>Qual é a vantagem de usar containers e orquestração
      na implantação de chatbots?</strong></p>
      <ol>
      <li><p>Permitir que o chatbot seja executado exclusivamente em
      dispositivos móveis.</p></li>
      <li><p>Facilitar o gerenciamento, escalabilidade e atualização do
      chatbot em ambientes de produção.</p></li>
      <li><p>Aumentar a complexidade do processo de
      implantação.</p></li>
      <li><p>arantir que o chatbot opere apenas em redes
      locais.</p></li>
      </ol></li>
      <li><p><strong>Como a integração com aplicativos de mensagens,
      como WhatsApp ou Slack, pode beneficiar um chatbot?</strong></p>
      <ol>
      <li><p>Permite que o chatbot interaja diretamente com os usuários
      em plataformas onde eles já estão ativos, melhorando o alcance e a
      conveniência.</p></li>
      <li><p>Garante que o chatbot funcione apenas em horários
      comerciais.</p></li>
      <li><p>Elimina a necessidade de monitoramento e manutenção do
      chatbot.</p></li>
      <li><p>Limita as funcionalidades do chatbot para apenas responder
      a perguntas básicas.</p></li>
      </ol></li>
      <li><p><strong>Por que é importante realizar a manutenção contínua
      de um chatbot em produção?</strong></p>
      <ol>
      <li><p>ara garantir que o chatbot permaneça atualizado, relevante
      e capaz de lidar com mudanças no ambiente ou nas necessidades dos
      usuários.</p></li>
      <li><p>Para reduzir o número de interações que o chatbot pode
      realizar por dia.</p></li>
      <li><p>Para garantir que o chatbot funcione apenas em horários
      comerciais.</p></li>
      <li><p>Para desativar temporariamente o chatbot e evitar o
      desgaste de hardware.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes métricas é essencial para
      monitorar o desempenho de um chatbot em produção?</strong></p>
      <ol>
      <li><p>Taxa de compressão de dados.</p></li>
      <li><p>Tempo de resposta do chatbot.</p></li>
      <li><p>Tamanho do código-fonte do chatbot.</p></li>
      <li><p>Número de desenvolvedores envolvidos no projeto.</p></li>
      </ol></li>
      <li><p><strong>Como o feedback dos usuários pode ser utilizado
      para melhorar um chatbot?</strong></p>
      <ol>
      <li><p>Ignorando o feedback dos usuários para manter a
      consistência das respostas.</p></li>
      <li><p>Ajustando as respostas do chatbot e atualizando o modelo
      com base nas sugestões e críticas dos usuários.</p></li>
      <li><p>Reduzindo o tempo de atividade do chatbot para evitar
      problemas relatados.</p></li>
      <li><p>Mantendo as respostas do chatbot sem alterações,
      independentemente do feedback.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes práticas ajuda a prevenir a
      deterioração do desempenho de um chatbot ao longo do
      tempo?</strong></p>
      <ol>
      <li><p>Realizar avaliações regulares do modelo e implementar
      monitoramento de drift para detectar quando o desempenho começa a
      decair.</p></li>
      <li><p>Aumentar a complexidade do modelo constantemente sem
      revisões periódicas.</p></li>
      <li><p>Desativar o chatbot durante horários de pico para evitar
      sobrecarga.</p></li>
      <li><p>Evitar atualizações para manter a estabilidade do
      sistema.</p></li>
      </ol></li>
      <li><p><strong>Qual das seguintes abordagens é crucial para
      garantir a segurança contínua dos chatbots em
      produção?</strong></p>
      <ol>
      <li><p>Aplicar patches de segurança regularmente e garantir a
      conformidade com regulamentos de proteção de dados.</p></li>
      <li><p>Reduzir o uso de autenticação para facilitar o acesso dos
      usuários.</p></li>
      <li><p>Manter o código do chatbot desatualizado para evitar
      incompatibilidades.</p></li>
      <li><p>Evitar o uso de criptografia para melhorar o
      desempenho.</p></li>
      </ol></li>
      </ol>
      <h1 class="unnumbered" id="sobre-os-autores">Sobre os autores</h1>
      <h2 class="unnumbered" id="giseldo-da-silva-neo">Giseldo da Silva
      Neo</h2>
      <figure>
      <img src="fig/autores/giseldo.jpg" style="width:50.0%" />
      <figcaption>Giseldo Neo</figcaption>
      </figure>
      <p>GISELDO DA SILVA NEO é Professor de Informática no Instituto
      Federal de Alagoas (IFAL) e desenvolve pesquisas na área de IA.
      Doutorando em Ciência da Computação na Universidade Federal de
      Campina Grande (UFCG). Possui Mestrado em Modelagem Computacional
      do Conhecimento (UFAL) e Mestrado em Contabilidade (FUCAPE).
      Possui MBA em Gestão e Estratégia Empresarial (ESTÁCIO),
      Especialização em Arquitetura e Engenharia de Software (ESTÁCIO),
      MBA em Gestão de Projetos (UNINTER). Graduação em Análise e
      Desenvolvimento de Sistemas (ESTÁCIO) e Graduação em Processos
      Gerenciais (UNINTER) e possui nível Técnico em Informática (Escola
      Técnica Federal de Sergipe).</p>
      <h2 class="unnumbered" id="alana-viana-borges-da-silva-neo">Alana
      Viana Borges da Silva Neo</h2>
      <figure>
      <img src="fig/autores/alana.jpg" style="width:50.0%" />
      <figcaption>Alana Neo</figcaption>
      </figure>
      <p>ALANA VIANA BORGES DA SILVA NEO é Professora de Informática no
      Instituto Federal do Mato Grosso do Sul (IFMS) e desenvolve
      pesquisas na área de Informática na Educação. Doutoranda em
      Ciência da Computação na Universidade Federal de Campina Grande
      (UFCG), Mestra em Modelagem Computacional do Conhecimento na
      Universidade Federal de Alagoas (UFAL), Especialista em
      Estratégias Didáticas para a Educação Básica com Uso de TIC na
      Universidade Federal de Alagoas (UFAL), Especialista em
      Desenvolvimento de Software, Especialista em Segurança da
      Informação, Graduada em Análise e Desenvolvimento de Sistemas e
      Bacharel em Sistemas de Informação pela Universidade Estácio de Sá
      (ESTÁCIO) e Licenciatura em Computação pelo Claretiano Centro
      Universitário.</p>
      <h2 class="unnumbered" id="contato">Contato</h2>
      <p>Caso deseje entrar em contato com os autores para reportar
      algum erro, crítica ou sugestão, envie e-mail para
      giseldo@gmail.com ou acesse o site <a
      href="https://giseldo.github.io/"
      class="uri">https://giseldo.github.io/</a></p>
      <h2 class="unnumbered" id="aviso-legal">Aviso Legal</h2>
      <p>As informações fornecidas neste trabalho são apenas para fins
      educacionais e informativos. Embora todos os esforços tenham sido
      feitos para garantir a precisão e a integridade, o autor e o
      editor não fazem declarações ou garantias de qualquer tipo,
      expressas ou implícitas, em relação à precisão, confiabilidade ou
      completude do conteúdo.</p>
      <p>O autor e o editor não poderão ser responsabilizados por
      quaisquer danos ou perdas decorrentes do uso deste material. As
      opiniões expressas são de responsabilidade exclusiva do autor e
      não refletem necessariamente as opiniões de qualquer instituição
      ou organização com a qual o autor possa estar vinculado.</p>
      <h2 class="unnumbered" id="uso-da-ia-generativa">Uso da IA
      Generativa</h2>
      <p>Algumas partes textuais e algumas imagens foram criadas ou
      alteradas com várias das IAs Generativas disponíveis no momento da
      escrita. Porém, todo o texto foi revisado pelos autores e
      revisores.</p>
      <div id="refs" class="references csl-bib-body hanging-indent"
      data-entry-spacing="0" role="list">
      <div id="ref-Abdul-Kader2015" class="csl-entry" role="listitem">
      Abdul-Kader, Sameera A., and John Woods. 2015. <span>“<span
      class="nocase">Survey on Chatbot Design Techniques in Speech
      Conversation Systems</span>.”</span> <em>International Journal of
      Advanced Computer Science and Applications</em> 6 (7): 72–80. <a
      href="https://doi.org/10.14569/ijacsa.2015.060712">https://doi.org/10.14569/ijacsa.2015.060712</a>.
      </div>
      <div id="ref-akhila2023comparative" class="csl-entry"
      role="listitem">
      Akhila, N et al. 2023. <span>“Comparative Study of Bert Models and
      Roberta in Transformer Based Question Answering.”</span> In
      <em>2023 3rd International Conference on Intelligent Technologies
      (CONIT)</em>, 1–5. IEEE.
      </div>
      <div id="ref-anthropic2024mcp" class="csl-entry" role="listitem">
      Anthropic. 2024. <span>“Model Context Protocol: Open Standard for
      AI Tooling Integration.”</span> <a
      href="https://modelcontextprotocol.io/"
      class="uri">https://modelcontextprotocol.io/</a>.
      </div>
      <div id="ref-AZEVEDO2015" class="csl-entry" role="listitem">
      Azevedo, Ryan Ribeiro de. 2015. <span>“<span class="nocase">Um
      sistema de dialogo inteligente baseado em logica de
      descricoes</span>.”</span> PhD thesis, UFCG.
      </div>
      <div id="ref-Bada2012" class="csl-entry" role="listitem">
      Bada, Everton Moschen. 2012. <span>“<span class="nocase">Uma
      proposta para extracao de perguntas e respostas de
      textos</span>.”</span> <em>XVII Congreso Internacional de
      Informatica Educativa, TISE</em>, 44–49.
      </div>
      <div id="ref-blipblog" class="csl-entry" role="listitem">
      blipblog. 2024. <span>“Inteligência Artificial.”</span>
      </div>
      <div id="ref-borah2018survey" class="csl-entry" role="listitem">
      Borah, Bhriguraj, Dhrubajyoti Pathak, Priyankoo Sarmah, Bidisha
      Som, and Sukumar Nandi. 2018. <span>“Survey of Textbased Chatbot
      in Perspective of Recent Technologies.”</span> In
      <em>International Conference on Computational Intelligence,
      Communications, and Business Analytics</em>, 84–96. Springer.
      </div>
      <div id="ref-rcelebrone2025" class="csl-entry" role="listitem">
      Celebrone, R. 2025. <span>“Processamento de Linguagem Natural: Um
      Caminho Do Zero Ao Herói.”</span> <a
      href="https://www.rcelebrone.com/2025/06/deep-research-pln.html"
      class="uri">https://www.rcelebrone.com/2025/06/deep-research-pln.html</a>.
      </div>
      <div id="ref-chomsky2002syntactic" class="csl-entry"
      role="listitem">
      Chomsky, Noam, and David W Lightfoot. 2002. <em><span
      class="nocase">Syntactic structures</span></em>. Walter de
      Gruyter.
      </div>
      <div id="ref-crewai2025" class="csl-entry" role="listitem">
      CrewAI. 2025. <span>“CrewAI — the Leading Multi-Agent
      Platform.”</span> <a href="https://www.crewai.com/"
      class="uri">https://www.crewai.com/</a>.
      </div>
      <div id="ref-DeGasperis2013" class="csl-entry" role="listitem">
      De Gasperis, Giovanni, Isabella Chiari, and Niva Florio. 2013.
      <em><span class="nocase">AIML knowledge base construction from
      text corpora</span></em>. Vol. 427. June 2014. <a
      href="https://doi.org/10.1007/978-3-642-29694-9_12">https://doi.org/10.1007/978-3-642-29694-9_12</a>.
      </div>
      <div id="ref-devlin2019bert" class="csl-entry" role="listitem">
      Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
      2019. <span>“Bert: Pre-Training of Deep Bidirectional Transformers
      for Language Understanding.”</span> In <em>Proceedings of the 2019
      Conference of the North American Chapter of the Association for
      Computational Linguistics: Human Language Technologies, Volume 1
      (Long and Short Papers)</em>, 4171–86.
      </div>
      <div id="ref-Ferrucci2012" class="csl-entry" role="listitem">
      Ferrucci, David. 2012. <span>“<span>This Is Watson</span>.”</span>
      <em>Journal of Research and Development</em> 56 (3): 88.
      </div>
      <div id="ref-jsonrpcSpec" class="csl-entry" role="listitem">
      Group, JSON-RPC Working. 2010. <span>“JSON-RPC 2.0
      Specification.”</span> <a
      href="https://www.jsonrpc.org/specification"
      class="uri">https://www.jsonrpc.org/specification</a>.
      </div>
      <div id="ref-Hohn2019" class="csl-entry" role="listitem">
      Höhn, Sviatlana. 2019. <em><span class="nocase">Artificial
      Companion for Second Language Conversation</span></em>.
      </div>
      <div id="ref-Jacobstein1998" class="csl-entry" role="listitem">
      Jacobstein, Murray, Sams, and Sincoff. 1998. <span>“A Multi-Agent
      Associate System Guide for a Virtual Collaboration Center.”</span>
      <em>Proceedings of the International Conference on Virtual Worlds
      and Simulation Conference</em>, 215–20.
      </div>
      <div id="ref-jonas2006principio" class="csl-entry"
      role="listitem">
      Jonas, Hans. 2006. <em>O Princ<span>ı́</span>pio Responsabilidade:
      Ensaio de Uma <span class="nocase">é</span>tica Para a
      Civiliza<span>ç</span><span>ã</span>o
      Tecnol<span>ó</span>gica</em>. Digitaliza Conteudo.
      </div>
      <div id="ref-Junior2008" class="csl-entry" role="listitem">
      Junior, Antonio Fernando Lavareda Jacob. 2008. <span>“<span
      class="nocase">Buti: um Companheiro Virtual baseado em Computacao
      Afetiva para Auxiliar na Manutencao da Saude
      Cardiovascular</span>.”</span> PhD thesis.
      </div>
      <div id="ref-jurafskyspeech" class="csl-entry" role="listitem">
      Jurafsky, Daniel, and James H Martin. 2023. <span>“Speech and
      Language Processing: An Introduction to Natural Language
      Processing, Computational Linguistics, and Speech
      Recognition.”</span>
      </div>
      <div id="ref-kant2017critica" class="csl-entry" role="listitem">
      Kant, Immanuel. 2017. <em>Cr<span>ı́</span>tica Da
      Raz<span>ã</span>o Pr<span>á</span>tica</em>. Editora Vozes
      Limitada.
      </div>
      <div id="ref-kant2023fundamentaccao" class="csl-entry"
      role="listitem">
      ———. 2023. <em>Fundamenta<span>ç</span><span>ã</span>o Da
      Metaf<span>ı́</span>sica Dos Costumes</em>. Leya.
      </div>
      <div id="ref-Khan2018" class="csl-entry" role="listitem">
      Khan, Rashid, and Anik Das. 2018. <em><span>Build Better
      Chatbots</span></em>. <a
      href="https://doi.org/10.1007/978-1-4842-3111-1">https://doi.org/10.1007/978-1-4842-3111-1</a>.
      </div>
      <div id="ref-Klopfenstein2017" class="csl-entry" role="listitem">
      Klopfenstein, Lorenz Cuno, Saverio Delpriori, Silvia Malatini, and
      Alessandro Bogliolo. 2017. <span>“<span class="nocase">The Rise of
      Bots: A Survey of Conversational Interfaces, Patterns, and
      Paradigms.</span>”</span> <em>Proceedings of the 2017 Conference
      on Designing Interactive Systems</em>, 555–65. <a
      href="https://doi.org/10.1145/3064663.3064672">https://doi.org/10.1145/3064663.3064672</a>.
      </div>
      <div id="ref-Krassmann2017" class="csl-entry" role="listitem">
      Krassmann, Aliane Loureiro, Fabrício Herpich, Álvaro Souza Pereira
      da Silva, Anita Raquel da Silva, Cristiane de Souza Abreu, Marcelo
      Augusto Rauh Schmitt, Magda Bercht, and Liane Margarida Rockenbach
      Tarouco. 2017. <span>“<span class="nocase">FastAIML: uma
      ferramenta para apoiar a geracao de base de conhecimento para
      chatbots educacionais</span>.”</span> <em>Revista Novas
      Tecnologias Na Educacao (RENOTE)</em> 15 (2): 1–10. <a
      href="https://doi.org/10.22456/1679-1916.79256">https://doi.org/10.22456/1679-1916.79256</a>.
      </div>
      <div id="ref-Kraus2008" class="csl-entry" role="listitem">
      Kraus, Helton, and Anita Fernandes. 2008. <span>“<span
      class="nocase">Ivetebyte: Um Chatterbot para Area Imobiliaria
      Integrando Raciocinio Baseado em Casos</span>.”</span> <em>Revista
      Iberica de Sistemas e Tecnologias de Informacao</em>, no. 1:
      40–51.
      </div>
      <div id="ref-Lane2025" class="csl-entry" role="listitem">
      Lane, Rupert, Anthony Hay, Arthur Schwarz, David M. Berry, and
      Jeff Shrager. 2025. <span>“<span class="nocase">ELIZA Reanimated:
      The world’s first chatbot restored on the world’s first time
      sharing system</span>,”</span> 1–21. <a
      href="http://arxiv.org/abs/2501.06707">http://arxiv.org/abs/2501.06707</a>.
      </div>
      <div id="ref-Leonhardt2003" class="csl-entry" role="listitem">
      Leonhardt, Michelle Denise, Ricardo Neisse, and Liane Margarida
      Rockenbach Tarouco. 2003. <span>“<span class="nocase">MEARA: Um
      Chatterbot Tem<span class="nocase">á</span>tico para Uso em
      Ambiente Educacional</span>.”</span> <a
      href="https://doi.org/10.5753/CBIE.SBIE.2003.81-88">https://doi.org/10.5753/CBIE.SBIE.2003.81-88</a>.
      </div>
      <div id="ref-Li2018" class="csl-entry" role="listitem">
      Li, Jiwei, Will Monroe, Tianlin Shi, Sėbastien Jean, Alan Ritter,
      and Dan Jurafsky. 2018. <span>“<span class="nocase">Adversarial
      Learning for Neural Dialogue Generation</span>,”</span> 2157–69.
      <a
      href="https://doi.org/10.18653/v1/d17-1230">https://doi.org/10.18653/v1/d17-1230</a>.
      </div>
      <div id="ref-Lima2017" class="csl-entry" role="listitem">
      Lima, Carlos Eduardo Teixeira Lima. 2017. <span>“<span
      class="nocase">Um Chatterbot para Criacao e Desenvolvimento de
      Ontologias com Logica de Descricao</span>.”</span> PhD thesis.
      </div>
      <div id="ref-Macedo2014" class="csl-entry" role="listitem">
      Macedo, Rafael Luiz De, and Elvis Fusco. 2014. <span>“<span
      class="nocase">An Intelligent Robotic Engine Using Digital
      Repository of the DSpace Platform</span>,”</span> no. c: 17–23.
      </div>
      <div id="ref-Madhumitha2015" class="csl-entry" role="listitem">
      Madhumitha, S., B. Keerthana, and B. Hemalatha. 2015.
      <span>“<span>Interactive Chatbot Using AIML</span>.”</span>
      <em>International Journal Of Advanced Networking And
      Applications</em>, 217–23.
      </div>
      <div id="ref-mangabaAI2025" class="csl-entry" role="listitem">
      Mangaba AI. 2025. <span>“Mangaba AI — Framework de Automação Com
      Agentes Inteligentes.”</span> <a
      href="https://mangaba-ai.vercel.app/"
      class="uri">https://mangaba-ai.vercel.app/</a>.
      </div>
      <div id="ref-B2020" class="csl-entry" role="listitem">
      Marcondes, Francisco Supino, Jose Joao Almeida, and Paulo Novais.
      2020. <span>“<span class="nocase">A Short Survey on Chatbot
      Technology : Failure in Raising the State of the
      Art</span>,”</span> 28–36. <a
      href="https://doi.org/10.1007/978-3-030-23887-2">https://doi.org/10.1007/978-3-030-23887-2</a>.
      </div>
      <div id="ref-Marchuschi1986" class="csl-entry" role="listitem">
      Marcuschi, Luís Antonio. 1986. <em><span class="nocase">An<span
      class="nocase">á</span>lise da Conversa<span
      class="nocase">ç</span><span class="nocase">ã</span>o</span></em>.
      </div>
      <div id="ref-Maria2010" class="csl-entry" role="listitem">
      Maria, Filomena, Gonçalves Da, Silva Cordeiro, and Rodrigo Lins
      Rodrigues. 2010. <span>“<span class="nocase">Um ambiente virtual
      de aprendizagem apoiado por um agente virtual:
      chatterbot</span>.”</span> <em>Encontro Internacional TIC e
      Educacao 2010 Lisboa Portugal</em>.
      </div>
      <div id="ref-Mauldin1994" class="csl-entry" role="listitem">
      Mauldin, Michael L. 1994. <span>“<span class="nocase">CHATTERBOTS,
      TINYMUDS, and the Turing Test Entering the Loebner Prize
      Competition</span>.”</span> <em>AAAI</em> 94: 16–21. <a
      href="http://www.aaai.org/Papers/AAAI/1994/AAAI94-003.pdf">http://www.aaai.org/Papers/AAAI/1994/AAAI94-003.pdf</a>.
      </div>
      <div id="ref-mikolov2013efficient" class="csl-entry"
      role="listitem">
      Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
      <span>“Efficient Estimation of Word Representations in Vector
      Space.”</span> <em>arXiv Preprint arXiv:1301.3781</em>.
      </div>
      <div id="ref-Minaee2025" class="csl-entry" role="listitem">
      Minaee, Shervin, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
      Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2025.
      <span>“<span>Large Language Models: A Survey</span>.”</span>
      <em>arXiv</em>. <a
      href="http://arxiv.org/abs/2503.23037">http://arxiv.org/abs/2503.23037</a>.
      </div>
      <div id="ref-mittmann2021ElizaOZ" class="csl-entry"
      role="listitem">
      Mittmann, Adiel. 2021.
      <span>“Implementa<span>ç</span><span>ã</span>o Do Chatterbot ELIZA
      Na Linguagem Multiparadigma Oz.”</span> PhD thesis, Universidade
      Federal de Santa Catarina.
      </div>
      <div id="ref-Neo2023Ecowe" class="csl-entry" role="listitem">
      Neo, Alana Viana Borges S., Giseldo da Silva Neo, Ítalo Rodrigo S.
      Arruda, Jônatas Duarte Vital Leite, Ezequiel Santana S. Oliveira,
      Thallis Henrique F. da Silva, Olival de Gusmão F. Júnior, Evandro
      de Barros Costa, and Wanderson R. M. Rodrigues. 2023. <span>“<span
      class="nocase">ECOW<span>Ê</span>: Um chatbot que ensina
      ecologia</span>.”</span> <em>XXVI Confer<span>ê</span>ncia
      Internacional Sobre Inform<span>á</span>tica Na
      Educa<span>ç</span><span>ã</span>o (TISE)</em> 17 (December):
      518–23.
      </div>
      <div id="ref-neo2023BPMN" class="csl-entry" role="listitem">
      Neo, Giseldo S, Evandro C Barros, Alana VBS Neo, J Antão B Moura,
      Arthur C Correia, and Wanderson RM Rodrigues. 2023.
      <span>“Integrando BPMN e AIML Para
      Constru<span>ç</span><span>ã</span>o de Fluxos de
      Di<span>á</span>logo Para Chatbots.”</span> In
      <em>Semin<span>á</span>rio Integrado de Software e Hardware
      (SEMISH)</em>, 356–67. SBC.
      </div>
      <div id="ref-Neves2005" class="csl-entry" role="listitem">
      Neves, André M. M., and Flávia de Almeida Barros. 2005.
      <span>“<span class="nocase">iAIML: Um mecanismo para tratamento de
      intencao em chatterbots</span>.”</span> <em>Congresso Da Sociedade
      Brasileira de Computacao</em>, 1032–41.
      </div>
      <div id="ref-OpenAI2023" class="csl-entry" role="listitem">
      OpenAI. 2023. <span>“<span>GPT-4 Technical Report</span>”</span>
      4: 1–100. <a
      href="http://arxiv.org/abs/2303.08774">http://arxiv.org/abs/2303.08774</a>.
      </div>
      <div id="ref-pennington2014glove" class="csl-entry"
      role="listitem">
      Pennington, Jeffrey, Richard Socher, and Christopher D Manning.
      2014. <span>“Glove: Global Vectors for Word
      Representation.”</span> In <em>Proceedings of the 2014 Conference
      on Empirical Methods in Natural Language Processing (EMNLP)</em>,
      1532–43.
      </div>
      <div id="ref-pickover2021artificial" class="csl-entry"
      role="listitem">
      Pickover, A. C. 2021. <em>Artificial Intelligence: An Illustrated
      History: From Medieval Robots to Neural Networks</em>. Sterling
      Publishing Co.
      </div>
      <div id="ref-pires2024conversational" class="csl-entry"
      role="listitem">
      Pires, Jorge Guerra. 2024. <span>“A Conversational Artificial
      Intelligence Based Web Application for Medical Conversations: A
      Prototype for a Chatbot.”</span> <em>medRxiv</em>, 2023–12.
      </div>
      <div id="ref-primo2001chatterbot" class="csl-entry"
      role="listitem">
      PRIMO, Alex Fernando Teixeira, and Luciano Roth COELHO. 2001.
      <span>“<span class="nocase">A chatterbot Cybelle: experi<span
      class="nocase">ê</span>ncia pioneira no Brasil</span>.”</span>
      <em>M<span>í</span>dia, Textos e Contextos. Porto Alegre:
      EDIPUCRS</em>, 259–76.
      </div>
      <div id="ref-radford2019language" class="csl-entry"
      role="listitem">
      Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
      and Ilya Sutskever. 2019. <span>“Language Models Are Unsupervised
      Multitask Learners.”</span> <em>OpenAI</em>.
      </div>
      <div id="ref-raffel2020exploring" class="csl-entry"
      role="listitem">
      Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
      Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.
      <span>“Exploring the Limits of Transfer Learning with a Unified
      Text-to-Text Transformer.”</span> <em>arXiv Preprint
      arXiv:1910.10683</em>.
      </div>
      <div id="ref-Raj2019" class="csl-entry" role="listitem">
      Raj, Sumit. 2019. <em><span class="nocase">Building Chatbots with
      Python</span></em>. <a
      href="https://doi.org/10.1007/978-1-4842-4096-0">https://doi.org/10.1007/978-1-4842-4096-0</a>.
      </div>
      <div id="ref-Ramesh2019" class="csl-entry" role="listitem">
      Ramesh, Kiran, Surya Ravishankaran, Abhishek Joshi, and K.
      Chandrasekaran. 2017. <span>“<span class="nocase">A Survey of
      Design Techniques for Conversational Agents</span>.”</span> In
      <em>International Conference on Information, Communication and
      Computing Technology</em>, 835:336–50. <a
      href="https://doi.org/10.1007/978-981-13-5992-7">https://doi.org/10.1007/978-981-13-5992-7</a>.
      </div>
      <div id="ref-raschka2024build" class="csl-entry" role="listitem">
      Raschka, Sebastian. 2024. <em>Build a Large Language Model (from
      Scratch)</em>. Simon; Schuster.
      </div>
      <div id="ref-ribeiro2025eraIA" class="csl-entry" role="listitem">
      Ribeiro, Eliéser. 2025. <em>A Era Da Inteligência Artificial e a
      Nova Economia</em>. Edição padrão. Brasil: Impressão sob demanda.
      </div>
      <div id="ref-Russel2013" class="csl-entry" role="listitem">
      Russel, Stuart, and Peter Norving. 2013. <em><span
      class="nocase">Intelig<span class="nocase">ê</span>ncia
      Artificial</span></em>.
      </div>
      <div id="ref-Sanh2019" class="csl-entry" role="listitem">
      Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
      2019. <span>“<span class="nocase">DistilBERT, a distilled version
      of BERT: smaller, faster, cheaper and lighter</span>,”</span> 2–6.
      <a
      href="http://arxiv.org/abs/1910.01108">http://arxiv.org/abs/1910.01108</a>.
      </div>
      <div id="ref-schiff2020ieee" class="csl-entry" role="listitem">
      Schiff, Daniel, Aladdin Ayesh, Laura Musikanski, and John C
      Havens. 2020. <span>“IEEE 7010: A New Standard for Assessing the
      Well-Being Implications of Artificial Intelligence.”</span> In
      <em>2020 IEEE International Conference on Systems, Man, and
      Cybernetics (SMC)</em>, 2746–53. IEEE.
      </div>
      <div id="ref-Shaikh2016" class="csl-entry" role="listitem">
      Shaikh, Ayesha, Geetanjali Phalke, Pranita Pat, Sangita Bhosale,
      and Jyoti Raghatwan. 2016. <span>“<span>A Survey On Chatbot
      Conversational Systems</span>.”</span> <em>International Journal
      of Engineering Science and Computing</em> 6 (11): 3117–19. <a
      href="http://ijesc.org/upload/464758c5f7d1a1cd13085e8a584ec5f3.A Survey On Chatbot Conversational Systems.pdf">http://ijesc.org/upload/464758c5f7d1a1cd13085e8a584ec5f3.A
      Survey On Chatbot Conversational Systems.pdf</a>.
      </div>
      <div id="ref-Sharma2020" class="csl-entry" role="listitem">
      Sharma, Moolchand, Shivang Verma, and Lakshay Sahni. 2020.
      <span>“<span class="nocase">Comparative Analysis of
      Chatbots</span>.”</span> <em>SSRN Electronic Journal</em>. <a
      href="https://doi.org/10.2139/ssrn.3563674">https://doi.org/10.2139/ssrn.3563674</a>.
      </div>
      <div id="ref-Shum2018" class="csl-entry" role="listitem">
      Shum, Heung-yeung, Xiao-dong He, and Di Li. 2018. <span>“<span
      class="nocase">From Eliza to XiaoIce: challenges and opportunities
      with social chatbots</span>.”</span> <em>Frontiers of Information
      Technology e Electronic Engineering</em> 19 (1): 10–26. <a
      href="https://doi.org/10.1631/fitee.1700826">https://doi.org/10.1631/fitee.1700826</a>.
      </div>
      <div id="ref-Silva2007" class="csl-entry" role="listitem">
      Silva, Arandir Cordeiro da, and Evandro De Barros Costa. 2007.
      <span>“<span class="nocase">Um Chatterbot aplicado ao Turismo: Um
      estudo de caso no Litoral Alagoano e na Cidade de Macei<span
      class="nocase">ó</span></span>,”</span> 160–67.
      </div>
      <div id="ref-Neo2023Chatcreator" class="csl-entry"
      role="listitem">
      Silva Neo, Alana Viana Borges da, Giseldo da Silva Neo, Olival de
      Gusmão F., Wanderon R. M. Rodrigues, and José Antão Beltrão Moura.
      2023. <span>“<a
      href="https://www.ncbi.nlm.nih.gov/pubmed/36709552"><span
      class="nocase">CHATCreator: Um construtor de chatbots de forma
      visual</span></a>.”</span> <em>S<span>á</span>nchez, J. (2023)
      Editor. Nuevas Ideas En Inform<span>á</span>tica Educativa,
      Volumen 17, p. 418 - 423. Santiago de Chile</em> 17: 418–523.
      </div>
      <div id="ref-Neo2020Engajando" class="csl-entry" role="listitem">
      Silva, Neo Giseldo da, Otávio Monteiro Pereira, and Alana Viana
      Borges da Silva Neo. 2020. <span>“<span class="nocase">Engajando
      alunos na aula de filosofia com intelig<span
      class="nocase">ê</span>ncia artificial</span>.”</span> In <em>VII
      Semana Internacional de Pedagogia</em>, 1–12. 1.
      Macei<span>ó</span> - Alagoas - Brasil.
      </div>
      <div id="ref-singh2019building" class="csl-entry" role="listitem">
      Singh, Abhishek, Karthik Ramasubramanian, and Shrey Shivam. 2019.
      <em>Building an Enterprise Chatbot: Work with Protected Enterprise
      Data Using Open Source Frameworks</em>. Apress.
      </div>
      <div id="ref-Torreao2005" class="csl-entry" role="listitem">
      Torreao, Paula Geralda Barbosa Coelho. 2005. <span>“<span
      class="nocase">Project Management Knowledge Learning Environment:
      Ambiente Inteligente de Aprendizado para Educacao em Gerenciamento
      de Projetos</span>.”</span> PhD thesis.
      </div>
      <div id="ref-Turing1950" class="csl-entry" role="listitem">
      Turing, A M. 1950. <span>“<span class="nocase">Computing Machinery
      and intelligence</span>.”</span> <em>Mind</em> 49 (8): 433–60. <a
      href="https://doi.org/10.1016/B978-0-12-386980-7.50023-X">https://doi.org/10.1016/B978-0-12-386980-7.50023-X</a>.
      </div>
      <div id="ref-vaswani2017attention" class="csl-entry"
      role="listitem">
      Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
      Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
      <span>“Attention Is All You Need.”</span> In <em>Advances in
      Neural Information Processing Systems</em>. Vol. 30. <a
      href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
      </div>
      <div id="ref-Wallace2003" class="csl-entry" role="listitem">
      Wallace, R. 2003. <span>“<span class="nocase">The Elements of AIML
      Style</span>.”</span> <em>Alice AI Foundation, Inc</em>, 12–77. <a
      href="https://files.ifi.uzh.ch/cl/hess/classes/seminare/chatbots/style.pdf">https://files.ifi.uzh.ch/cl/hess/classes/seminare/chatbots/style.pdf</a>.
      </div>
      <div id="ref-Wallace2000" class="csl-entry" role="listitem">
      Wallace, Richard S. 2000. <span>“<span class="nocase">Don’ t Read
      Me: A.L.I.C.E. and AIML Documentation</span>,”</span> 1–72.
      </div>
      <div id="ref-Wallace2009" class="csl-entry" role="listitem">
      Wallace, Richard S. 2009. <span>“<span class="nocase">The anatomy
      of A.L.I.C.E.</span>”</span> <em>Parsing the Turing Test.
      Springer, Dordrecht</em>.
      </div>
      <div id="ref-Weizenbaum1996" class="csl-entry" role="listitem">
      Weizenbaum, Joseph. 1966. <span>“<span class="nocase">ELIZA - A
      Computer Program For the Study of Natural Language Communication
      Between Man And Machine</span>.”</span> <em>Communications of the
      ACM</em> 9 (1).
      </div>
      <div id="ref-wikipediahumano" class="csl-entry" role="listitem">
      Wikipedia. 2024a. <span>“Humano.”</span>
      </div>
      <div id="ref-wikipediaabelhas" class="csl-entry" role="listitem">
      ———. 2024b. <span>“Inteligência Em Abelhas.”</span>
      </div>
      <div id="ref-wikipediafourier" class="csl-entry" role="listitem">
      ———. 2024c. <span>“Transformada de Fourier.”</span>
      </div>
      <div id="ref-Yamaguchi2018" class="csl-entry" role="listitem">
      Yamaguchi, Hiroshi, Maxim Mozgovoy, and Anna Danielewicz-Betz.
      2018. <span>“<span>A Chatbot Based On AIML Rules Extracted From
      Twitter Dialogues</span>.”</span> <em>Communication Papers of the
      2018 Federated Conference on Computer Science and Information
      Systems</em> 17: 37–42. <a
      href="https://doi.org/10.15439/2018f297">https://doi.org/10.15439/2018f297</a>.
      </div>
      <div id="ref-Zhao2020" class="csl-entry" role="listitem">
      Zhao, Liping, Waad Alhoshan, Alessio Ferrari, Keletso J. Letsholo,
      Muideen A. Ajagbe, Erol Valeriu Chioasca, and Riza T.
      Batista-Navarro. 2020. <span>“<span class="nocase">Natural
      Language Processing (NLP) for requirements engineering: A
      systematic mapping study</span>.”</span> <em>arXiv</em>, no. v. <a
      href="https://arxiv.org/abs/2004.01099">https://arxiv.org/abs/2004.01099</a>.
      </div>
      </div>
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p><a
      href="https://github.com/keiffster/program-y/wiki/RDF"
      class="uri">https://github.com/keiffster/program-y/wiki/RDF</a><a
      href="#fnref1" class="footnote-back"
      role="doc-backlink">↩︎</a></p></li>
      <li id="fn2"><p><a
      href="https://medium.com/pandorabots-blog/new-feature-visualize-your-aiml-26e33a590da1"
      class="uri">https://medium.com/pandorabots-blog/new-feature-visualize-your-aiml-26e33a590da1</a><a
      href="#fnref2" class="footnote-back"
      role="doc-backlink">↩︎</a></p></li>
      <li id="fn3"><p><a href="https://www.pandorabots.com/mitsuku/"
      class="uri">https://www.pandorabots.com/mitsuku/</a><a
      href="#fnref3" class="footnote-back"
      role="doc-backlink">↩︎</a></p></li>
      <li id="fn4"><p><a
      href="https://aisb.org.uk/category/loebner-prize/"
      class="uri">https://aisb.org.uk/category/loebner-prize/</a><a
      href="#fnref4" class="footnote-back"
      role="doc-backlink">↩︎</a></p></li>
      </ol>
      </section>
    </main>

  </div>

  

  <script>
document.addEventListener("DOMContentLoaded", () => {
  // Controle do toggle do menu lateral (desktop)
  const sidebarToggle = document.querySelector(".sidebar-toggle");
  const layout = document.querySelector(".layout");

  if (sidebarToggle && layout) {
    sidebarToggle.addEventListener("click", () => {
      layout.classList.toggle("menu-hidden");
      const isHidden = layout.classList.contains("menu-hidden");
      sidebarToggle.setAttribute("aria-expanded", String(!isHidden));
      sidebarToggle.setAttribute("aria-label", isHidden ? "Exibir menu" : "Ocultar menu");
      sidebarToggle.setAttribute("title", isHidden ? "Exibir menu lateral" : "Ocultar menu lateral");
    });
  }

  // Controle do menu hambúrguer
  const menuToggle = document.querySelector(".menu-toggle");
  const menu = document.querySelector("nav.menu");

  if (menuToggle && menu) {
    menuToggle.addEventListener("click", () => {
      menu.classList.toggle("active");
      const isOpen = menu.classList.contains("active");
      menuToggle.setAttribute("aria-expanded", isOpen);
    });

    // Fechar menu ao clicar em um link
    menu.querySelectorAll("a").forEach(link => {
      link.addEventListener("click", () => {
        menu.classList.remove("active");
        menuToggle.setAttribute("aria-expanded", "false");
      });
    });
  }

  // Menu colapsável
  const rootUl = menu?.querySelector("ul");
  if (!rootUl) return;

  const toggleButtons = [];

  rootUl.querySelectorAll("li").forEach(li => {
    const childUl = Array.from(li.children).find(el => el.tagName === "UL");
    const link = Array.from(li.children).find(el => el.tagName === "A");
    if (!childUl || !link) return;

    li.classList.add("has-children", "collapsed");

    const btn = document.createElement("button");
    btn.type = "button";
    btn.className = "toc-toggle";
    btn.setAttribute("aria-label", "Abrir/fechar submenu");
    btn.setAttribute("aria-expanded", "false");

    btn.addEventListener("click", (e) => {
      e.preventDefault();
      e.stopPropagation();
      const collapsed = li.classList.toggle("collapsed");
      btn.setAttribute("aria-expanded", String(!collapsed));
    });

    toggleButtons.push({ button: btn, li: li });
    link.insertAdjacentElement('beforebegin', btn);
  });

  // Botões para expandir/colapsar tudo
  const expandAllBtn = document.querySelector(".toc-expand-all");
  const collapseAllBtn = document.querySelector(".toc-collapse-all");

  if (expandAllBtn) {
    expandAllBtn.addEventListener("click", () => {
      toggleButtons.forEach(({ button, li }) => {
        li.classList.remove("collapsed");
        button.setAttribute("aria-expanded", "true");
      });
    });
  }

  if (collapseAllBtn) {
    collapseAllBtn.addEventListener("click", () => {
      toggleButtons.forEach(({ button, li }) => {
        li.classList.add("collapsed");
        button.setAttribute("aria-expanded", "false");
      });
    });
  }
});
</script>



</body>
</html>
